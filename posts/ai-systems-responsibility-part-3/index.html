<!doctype html><html lang=en><head><title>AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why :: Gareth's Engineering Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="3:47am. Phone rings. On-call engineer sounds panicked.
OnCall: The model&rsquo;s broken.
Sysadmin: What&rsquo;s it doing?
OnCall: Giving wrong answers.
Sysadmin: How wrong?
OnCall: Very wrong. Accuracy dropped from 92% to 54% in the last hour.
Sysadmin: Any deployment changes?
OnCall: No.
Sysadmin: Input data look different?
OnCall: Don&rsquo;t know. How do I check?
Sysadmin: Can you roll back?
OnCall: To what? The model hasn&rsquo;t changed.
Welcome to AI incident response.
&#34;Normal incident: 'The database crashed.' AI incident: 'The model's giving wrong answers and nobody knows why because machine learning is basically a magic box that we've convinced ourselves we understand.'&#34; Part 3 of the series. This one&rsquo;s about what to do when your AI system breaks at 3am and you&rsquo;re expected to have answers you don&rsquo;t have.
"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/><link rel=stylesheet href=https://gazsecops.github.io/css/extended.min.c658c723e006469d82f697e19c5338967fad12c57650bdd915bacf9cfbe2cc38.css><link rel=stylesheet href=https://gazsecops.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://gazsecops.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://gazsecops.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://gazsecops.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://gazsecops.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://gazsecops.github.io/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://gazsecops.github.io/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://gazsecops.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://gazsecops.github.io/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://gazsecops.github.io/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://gazsecops.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://gazsecops.github.io/favicon.png><link rel=apple-touch-icon href=https://gazsecops.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why"><meta property="og:description" content="3:47am. Phone rings. On-call engineer sounds panicked.
OnCall: The model&rsquo;s broken.
Sysadmin: What&rsquo;s it doing?
OnCall: Giving wrong answers.
Sysadmin: How wrong?
OnCall: Very wrong. Accuracy dropped from 92% to 54% in the last hour.
Sysadmin: Any deployment changes?
OnCall: No.
Sysadmin: Input data look different?
OnCall: Don&rsquo;t know. How do I check?
Sysadmin: Can you roll back?
OnCall: To what? The model hasn&rsquo;t changed.
Welcome to AI incident response.
&#34;Normal incident: 'The database crashed.' AI incident: 'The model's giving wrong answers and nobody knows why because machine learning is basically a magic box that we've convinced ourselves we understand.'&#34; Part 3 of the series. This one&rsquo;s about what to do when your AI system breaks at 3am and you&rsquo;re expected to have answers you don&rsquo;t have.
"><meta property="og:url" content="https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/"><meta property="og:site_name" content="Gareth's Engineering Blog"><meta property="og:image" content="https://gazsecops.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2025-12-19 10:00:00 +0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>gareth@blog:~$</div></a></div><div class=header__search><input type=text id=header-search placeholder="Quick search..." style="padding:5px 10px;background:#222;color:#eee;border:1px solid #444;border-radius:3px;font-size:14px;width:200px"></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></nav></header><script>(function(){const e=document.getElementById("header-search");if(!e)return;e.addEventListener("keydown",function(e){if(e.key==="Enter"){const e=this.value.trim();e&&(window.location.href="/search?q="+encodeURIComponent(e))}}),document.addEventListener("keydown",function(t){t.key==="/"&&!["INPUT","TEXTAREA"].includes(t.target.tagName)&&(t.preventDefault(),e.focus()),t.key==="Escape"&&t.target===e&&e.blur()})})()</script><style>.header__inner{display:flex;align-items:center;gap:20px}.header__search{flex:none}#header-search::placeholder{color:#666;font-style:italic}#header-search:focus{outline:none;border-color:#888}@media(max-width:684px){.header__search{display:none}}</style><div class=content><article class=post><h1 class=post-title><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/>AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why</a></h1><div class=post-meta><time class=post-date>2025-12-19</time><span class=post-author>Gareth</span><span class=post-reading-time>8 min read (1540 words)</span></div><span class=post-tags>#<a href=https://gazsecops.github.io/tags/ai/>ai</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/incident-response/>incident-response</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/operations/>operations</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/series/>series</a>&nbsp;</span><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#why-ai-incidents-are-worse>Why AI Incidents Are Worse</a></li><li><a href=#the-incident-response-playbook-that-probably-wont-work>The Incident Response Playbook (That Probably Won&rsquo;t Work)</a></li><li><a href=#the-post-mortem-nobody-wants-to-write>The Post-Mortem Nobody Wants to Write</a></li><li><a href=#what-you-can-actually-do>What You Can Actually Do</a></li><li><a href=#the-scenarios-youll-encounter>The Scenarios You&rsquo;ll Encounter</a></li><li><a href=#what-management-expects>What Management Expects</a></li><li><a href=#what-management-gets>What Management Gets</a></li><li><a href=#part-4-preview>Part 4 Preview</a></li></ul></nav></div><div class=post-content><div><p>3:47am. Phone rings. On-call engineer sounds panicked.</p><div class=dialogue><p><span class="speaker speaker-oncall">OnCall:</span> The model&rsquo;s broken.</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> What&rsquo;s it doing?</p><p><span class="speaker speaker-oncall">OnCall:</span> Giving wrong answers.</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> How wrong?</p><p><span class="speaker speaker-oncall">OnCall:</span> Very wrong. Accuracy dropped from 92% to 54% in the last hour.</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> Any deployment changes?</p><p><span class="speaker speaker-oncall">OnCall:</span> No.</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> Input data look different?</p><p><span class="speaker speaker-oncall">OnCall:</span> Don&rsquo;t know. How do I check?</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> Can you roll back?</p><p><span class="speaker speaker-oncall">OnCall:</span> To what? The model hasn&rsquo;t changed.</p></div><p>Welcome to AI incident response.</p><aside class=pullquote>"Normal incident: 'The database crashed.' AI incident: 'The model's giving wrong answers and nobody knows why because machine learning is basically a magic box that we've convinced ourselves we understand.'"</aside><p>Part 3 of the series. This one&rsquo;s about what to do when your AI system breaks at 3am and you&rsquo;re expected to have answers you don&rsquo;t have.</p><h2 id=why-ai-incidents-are-worse>Why AI Incidents Are Worse<a href=#why-ai-incidents-are-worse class=hanchor arialabel=Anchor>#</a></h2><p>Normal software incident: Something breaks. You get logs. Stack traces. Error messages. You follow the trail. &ldquo;This function threw an exception because this input was null because this service was down.&rdquo; Root cause identified. Fix deployed. Done.</p><p>AI incidents: Something breaks. No error. No exception. No stack trace. Just wrong predictions.</p><div class=dialogue><p><span class="speaker speaker-management">Management:</span> Why is it wrong?</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> I don&rsquo;t know.</p><p><span class="speaker speaker-management">Management:</span> What changed?</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> Nothing we can see.</p><p><span class="speaker speaker-management">Management:</span> Can you fix it?</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> I don&rsquo;t know what&rsquo;s broken.</p></div><p>This is the conversation you&rsquo;ll have with management at 4am. They won&rsquo;t like it. You won&rsquo;t like it. But it&rsquo;s the truth.</p><h2 id=the-incident-response-playbook-that-probably-wont-work>The Incident Response Playbook (That Probably Won&rsquo;t Work)<a href=#the-incident-response-playbook-that-probably-wont-work class=hanchor arialabel=Anchor>#</a></h2><p><strong>Step 1: Confirm it&rsquo;s actually broken</strong></p><p>Is accuracy really down? Or is someone panicking over normal variance?</p><p>Check your monitoring dashboards (you have those, right?). Compare current metrics to baseline. If accuracy dropped 2%, that might be noise. If it dropped 40%, something&rsquo;s wrong.</p><p>Quick sanity check:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>curl -s http://model-service:8080/metrics | grep -E <span style=color:#e6db74>&#34;model_accuracy|model_prediction_total&#34;</span>
</span></span></code></pre></div><p>Or query Prometheus directly:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>curl -s <span style=color:#e6db74>&#39;http://prometheus:9090/api/v1/query?query=model_accuracy_ratio&#39;</span> | jq .
</span></span><span style=display:flex><span>curl -s <span style=color:#e6db74>&#39;http://prometheus:9090/api/v1/query?query=rate(model_prediction_total[5m])&#39;</span> | jq .
</span></span></code></pre></div><p>Compare to an hour ago:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span>model_accuracy_ratio
</span></span><span style=display:flex><span><span style=color:#f92672>(</span>model_accuracy_ratio <span style=color:#66d9ef>offset</span> <span style=color:#e6db74>1h</span><span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>model_accuracy_ratio <span style=color:#f92672>-</span> <span style=color:#f92672>(</span>model_accuracy_ratio <span style=color:#66d9ef>offset</span> <span style=color:#e6db74>1h</span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p><strong>Step 2: Roll back if you can</strong></p><p>Can you revert to the previous model? Do it. Don&rsquo;t investigate. Don&rsquo;t debug. Just roll back. Restore service first, investigate later.</p><p>&ldquo;But we need to understand why it broke!&rdquo;</p><p>Later. Right now we need it to stop being broken.</p><p>Of course, this assumes:</p><ul><li>You have a previous model to roll back to</li><li>You have a tested rollback procedure</li><li>The problem is with the model, not the data</li></ul><p>Good luck with all that.</p><p><strong>Step 3: Check the obvious things</strong></p><ul><li>Did the input data change? Compare current input distribution to historical. If it shifted, that&rsquo;s probably your problem.</li><li>Is the model actually running? Or is it failing and returning cached/default predictions?</li><li>Did upstream services change? API contract changed? Data format different?</li><li>Infrastructure issues? GPU running hot? Memory pressure? Disk full?</li></ul><p>Quick checks:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>curl -s http://model-service:8080/health
</span></span><span style=display:flex><span>curl -s http://model-service:8080/metrics | grep model_prediction_latency_seconds
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nvidia-smi
</span></span><span style=display:flex><span>free -h
</span></span><span style=display:flex><span>df -h /var/lib/model-data
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>journalctl -u model-server --since <span style=color:#e6db74>&#34;30 minutes ago&#34;</span> | tail -50
</span></span></code></pre></div><p>Check input distribution in Prometheus:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span>model_input_feature_value_bucket{feature<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>age</span>&#34;}
</span></span><span style=display:flex><span><span style=color:#66d9ef>topk</span><span style=color:#f92672>(</span><span style=color:#ae81ff>10</span>, model_input_feature_value_bucket<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Most of the time, it&rsquo;s something boring. Input format changed. Upstream service is returning nulls. Someone deployed a &ldquo;minor update&rdquo; that broke everything.</p><p><strong>Step 4: Check the non-obvious things</strong></p><p>Input data looks fine? Model&rsquo;s running? Infrastructure&rsquo;s healthy? Now you&rsquo;re in the hard part.</p><ul><li>Feature drift: Are the features the model uses still meaningful? Did someone change how a feature is calculated upstream?</li><li>Adversarial inputs: Is someone gaming your model? Spam filters and fraud detection are particularly vulnerable.</li><li>Data quality issues: Upstream data pipeline broke. Feeding garbage into your model. Model&rsquo;s doing its best with garbage inputs.</li><li>Model degradation: Your model trained on old data. World changed. Model&rsquo;s now making predictions based on outdated patterns.</li></ul><p>Good luck debugging any of this at 4am.</p><p><strong>Step 5: Implement a workaround</strong></p><p>Can&rsquo;t roll back? Can&rsquo;t fix the root cause? Implement a workaround:</p><ul><li>Route traffic to a simpler, more reliable model</li><li>Fall back to rule-based logic</li><li>Send high-risk predictions to human review</li><li>Disable the AI entirely and go full manual</li></ul><p>Management will hate this. &ldquo;We can&rsquo;t just turn off the AI!&rdquo;</p><p>Watch me.</p><p>Concrete workaround patterns:</p><p><strong>Route to fallback model:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>curl -X POST http://loadbalancer/admin/routes <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>  -d <span style=color:#e6db74>&#39;{&#34;model&#34;: &#34;fallback-v1&#34;, &#34;weight&#34;: 100}&#39;</span>
</span></span></code></pre></div><p><strong>Enable human review queue:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>curl -X POST http://model-service/admin/config <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>  -d <span style=color:#e6db74>&#39;{&#34;human_review_enabled&#34;: true, &#34;confidence_threshold&#34;: 0.7}&#39;</span>
</span></span></code></pre></div><p><strong>Disable automatic decisions:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>curl -X POST http://model-service/admin/config <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span>  -d <span style=color:#e6db74>&#39;{&#34;auto_approve&#34;: false, &#34;auto_deny&#34;: false}&#39;</span>
</span></span></code></pre></div><h2 id=the-post-mortem-nobody-wants-to-write>The Post-Mortem Nobody Wants to Write<a href=#the-post-mortem-nobody-wants-to-write class=hanchor arialabel=Anchor>#</a></h2><p>Normal incident post-mortem:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-fallback data-lang=fallback><span style=display:flex><span>Root cause: Database deadlock due to concurrent writes
</span></span><span style=display:flex><span>Fix: Implemented proper locking
</span></span><span style=display:flex><span>Prevention: Added monitoring for lock contention
</span></span></code></pre></div><p>AI incident post-mortem:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-fallback data-lang=fallback><span style=display:flex><span>Root cause: Unknown. Accuracy dropped 45% suddenly.
</span></span><span style=display:flex><span>Fix: Rolled back to previous model
</span></span><span style=display:flex><span>Prevention: ¯\_(ツ)_/¯
</span></span></code></pre></div><p>This won&rsquo;t satisfy management. They want root cause. They want corrective actions. They want assurance it won&rsquo;t happen again.</p><p>You don&rsquo;t have any of that. Because the honest answer is: &ldquo;Machine learning models are probabilistic systems trained on historical data. When the real world diverges from the training data in ways we didn&rsquo;t anticipate, the model breaks. We can&rsquo;t predict every way the world might change.&rdquo;</p><p>Management: &ldquo;That&rsquo;s not acceptable.&rdquo;</p><p>Me: &ldquo;Then don&rsquo;t use AI.&rdquo;</p><p>Management: &ldquo;We need AI for competitive advantage.&rdquo;</p><p>Me: &ldquo;Then accept that it will occasionally break in ways we can&rsquo;t explain.&rdquo;</p><p>Management doesn&rsquo;t like this answer. Tough.</p><h2 id=what-you-can-actually-do>What You Can Actually Do<a href=#what-you-can-actually-do class=hanchor arialabel=Anchor>#</a></h2><p><strong>1. Build better monitoring (Part 2)</strong></p><p>You can&rsquo;t debug what you can&rsquo;t see. Track accuracy. Track input distributions. Track everything. When something breaks, at least you&rsquo;ll know what changed.</p><p><strong>2. Have a rollback plan (Part 1)</strong></p><p>Can&rsquo;t fix it? Roll back. You need a tested, automated rollback procedure. Not &ldquo;we think we can roll back.&rdquo; You need &ldquo;I can run this script and we&rsquo;re back to the old model in 90 seconds.&rdquo;</p><p><strong>3. Have fallback logic</strong></p><p>When the AI breaks, what&rsquo;s your plan B? Rule-based logic? Simpler model? Human review? You need something. &ldquo;Hope the AI doesn&rsquo;t break&rdquo; is not a plan.</p><p><strong>4. Document known failure modes</strong></p><p>Your model will break in predictable ways. Document them. &ldquo;Model fails when input contains emojis.&rdquo; &ldquo;Model fails on data from time zones we didn&rsquo;t train on.&rdquo; &ldquo;Model fails during holiday shopping season when traffic patterns change.&rdquo;</p><p>Won&rsquo;t prevent incidents, but at least you&rsquo;ll know where to look.</p><p><strong>5. Have access to people who understand the model</strong></p><p>At 4am, you need to reach someone who knows how the model works. Not &ldquo;vaguely understands machine learning.&rdquo; Actually knows this specific model. Good luck if they quit three months ago and took all the knowledge with them.</p><p><strong>6. Practice incident response</strong></p><p>Run drills. Simulate an accuracy drop. See how fast you can respond. See if your rollback procedure works. See if you can actually access the logs you need.</p><p>Most teams don&rsquo;t do this. Most teams find out their incident response doesn&rsquo;t work when they&rsquo;re in an actual incident.</p><p><strong>Sample drill script:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;=== AI Incident Response Drill ===&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;1. Check model health:&#34;</span>
</span></span><span style=display:flex><span>curl -s http://model-service:8080/health
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;2. Check accuracy metrics:&#34;</span>
</span></span><span style=display:flex><span>curl -s http://model-service:8080/metrics | grep model_accuracy
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;3. Check recent errors:&#34;</span>
</span></span><span style=display:flex><span>journalctl -u model-server --since <span style=color:#e6db74>&#34;5 minutes ago&#34;</span> | grep -i error
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;4. Test rollback command (dry-run):&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Would run: kubectl rollout undo deployment/model-server&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;5. Time how long this took:&#34;</span>
</span></span></code></pre></div><p>Run it monthly. Track your response time. If it&rsquo;s getting slower, you&rsquo;ve got a problem.</p><h2 id=the-scenarios-youll-encounter>The Scenarios You&rsquo;ll Encounter<a href=#the-scenarios-youll-encounter class=hanchor arialabel=Anchor>#</a></h2><p><strong>Scenario 1: Silent degradation</strong></p><p>Accuracy slowly drops over weeks. Nobody notices until it hits a threshold. By the time you investigate, so much has changed you can&rsquo;t pinpoint the cause. Was it data drift? Model decay? Something else? Who knows.</p><p><strong>Scenario 2: Sudden catastrophic failure</strong></p><p>Accuracy drops 50% overnight. Turns out upstream service changed their API. Your parser broke. Been feeding garbage into your model for 6 hours. Rollback doesn&rsquo;t help because the problem isn&rsquo;t the model.</p><p><strong>Scenario 3: The model is fine, everything else is broken</strong></p><p>Model&rsquo;s working perfectly. Accuracy is good. But the system integrating with your model is broken. Passing wrong inputs. Ignoring outputs. Using predictions incorrectly. Your model gets blamed anyway.</p><p><strong>Scenario 4: The model is doing exactly what it was trained to do (which is wrong)</strong></p><p>Model learned from biased training data. It&rsquo;s making biased predictions. Accurately! According to the training data, it&rsquo;s 95% correct. According to basic fairness, it&rsquo;s completely wrong. This is not an ops problem. This is an &ldquo;oh shit&rdquo; problem.</p><h2 id=what-management-expects>What Management Expects<a href=#what-management-expects class=hanchor arialabel=Anchor>#</a></h2><ul><li>Immediate root cause analysis</li><li>Clear corrective actions</li><li>Assurance it won&rsquo;t happen again</li><li>Detailed timeline of events</li><li>Explanation in non-technical terms</li></ul><h2 id=what-management-gets>What Management Gets<a href=#what-management-gets class=hanchor arialabel=Anchor>#</a></h2><ul><li>&ldquo;We think it might be data drift but we&rsquo;re not sure&rdquo;</li><li>&ldquo;We rolled back and it&rsquo;s working now&rdquo;</li><li>&ldquo;It might happen again&rdquo;</li><li>&ldquo;Accuracy dropped at 3:47am, we rolled back at 4:15am&rdquo;</li><li>&ldquo;The magic box stopped working so we turned it off and on again&rdquo;</li></ul><p>They&rsquo;re not going to be happy. But that&rsquo;s the reality of running AI systems.</p><h2 id=part-4-preview>Part 4 Preview<a href=#part-4-preview class=hanchor arialabel=Anchor>#</a></h2><p>Next time: Testing AI before deployment. Or as I like to call it: &ldquo;How to avoid discovering your model is broken after you&rsquo;ve already shipped it to production.&rdquo;</p><p>Spoiler: Most teams don&rsquo;t do this. Most teams test that the model loads and returns predictions. That&rsquo;s not testing. That&rsquo;s hoping.</p><hr><p><em>Part 3 of N in a series on running AI systems. <a href=/posts/ai-systems-responsibility-part-1/>Part 1: Who Carries the Can?</a> | <a href=/posts/ai-systems-responsibility-part-2/>Part 2: Monitoring</a></em></p></div></div><div class=related-posts style="margin-top:3rem;padding-top:2rem;border-top:1px solid #333"><h3>Related Posts</h3><ul style=list-style:none;padding:0><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-2/>AI Systems Responsibility: Part 2 - Monitoring That Actually Works</a>
<span style=color:#999;font-size:.9rem>- 12 Dec 2025</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#monitoring</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-1/>AI Systems Responsibility: Part 1 - Who Carries the Can?</a>
<span style=color:#999;font-size:.9rem>- 5 Dec 2025</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#sysadmin</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/>AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode)</a>
<span style=color:#999;font-size:.9rem>- 30 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#tools</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/>AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions</a>
<span style=color:#999;font-size:.9rem>- 23 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#management</span>
<span style=margin-right:.5rem>#culture</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/>AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong)</a>
<span style=color:#999;font-size:.9rem>- 16 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#case-studies</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li></ul></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-4/ class="button inline prev">&lt; [<span class=button__text>AI Systems Responsibility: Part 4 - Testing Before You Ship (Or: Stop Discovering Bugs in Production)</span>]
</a>::
<a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-2/ class="button inline next">[<span class=button__text>AI Systems Responsibility: Part 2 - Monitoring That Actually Works</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2026</span></div><div class=build-info><span class=build-commit>e733073</span><span class=build-date>2026-02-12 12:33 UTC</span></div></div></footer></div></body></html>