<!doctype html><html lang=en><head><title>AI Systems Responsibility: Part 1 - Who Carries the Can? :: Gareth's Engineering Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content=' Management: We need to deploy the AI model to production by Friday.
Sysadmin: Have you tested it?
Management: The data scientists say it&rsquo;s good.
Sysadmin: That&rsquo;s not what I asked.
Management: Well, no, but marketing promised the client-
Sysadmin: Then no.
"When your AI system denies someone&#39;s mortgage application because they have a Welsh surname, guess who gets the phone call? Not the data scientist. Not the VP of Innovation. You." This is part one of a series about running AI systems when you&rsquo;re the poor sod responsible for keeping them alive. Not the hand-wavy ethics stuff. The practical bits: what breaks, how to know it&rsquo;s broken, and how to not get blamed when it inevitably goes sideways.
'><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-1/><link rel=stylesheet href=https://gazsecops.github.io/css/extended.min.c658c723e006469d82f697e19c5338967fad12c57650bdd915bacf9cfbe2cc38.css><link rel=stylesheet href=https://gazsecops.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://gazsecops.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://gazsecops.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://gazsecops.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://gazsecops.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://gazsecops.github.io/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://gazsecops.github.io/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://gazsecops.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://gazsecops.github.io/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://gazsecops.github.io/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://gazsecops.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://gazsecops.github.io/favicon.png><link rel=apple-touch-icon href=https://gazsecops.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="AI Systems Responsibility: Part 1 - Who Carries the Can?"><meta property="og:description" content=' Management: We need to deploy the AI model to production by Friday.
Sysadmin: Have you tested it?
Management: The data scientists say it&rsquo;s good.
Sysadmin: That&rsquo;s not what I asked.
Management: Well, no, but marketing promised the client-
Sysadmin: Then no.
"When your AI system denies someone&#39;s mortgage application because they have a Welsh surname, guess who gets the phone call? Not the data scientist. Not the VP of Innovation. You." This is part one of a series about running AI systems when you&rsquo;re the poor sod responsible for keeping them alive. Not the hand-wavy ethics stuff. The practical bits: what breaks, how to know it&rsquo;s broken, and how to not get blamed when it inevitably goes sideways.
'><meta property="og:url" content="https://gazsecops.github.io/posts/ai-systems-responsibility-part-1/"><meta property="og:site_name" content="Gareth's Engineering Blog"><meta property="og:image" content="https://gazsecops.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2025-12-05 10:00:00 +0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>gareth@blog:~$</div></a></div><div class=header__search><input type=text id=header-search placeholder="Quick search..." style="padding:5px 10px;background:#222;color:#eee;border:1px solid #444;border-radius:3px;font-size:14px;width:200px"></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></nav></header><script>(function(){const e=document.getElementById("header-search");if(!e)return;e.addEventListener("keydown",function(e){if(e.key==="Enter"){const e=this.value.trim();e&&(window.location.href="/search?q="+encodeURIComponent(e))}}),document.addEventListener("keydown",function(t){t.key==="/"&&!["INPUT","TEXTAREA"].includes(t.target.tagName)&&(t.preventDefault(),e.focus()),t.key==="Escape"&&t.target===e&&e.blur()})})()</script><style>.header__inner{display:flex;align-items:center;gap:20px}.header__search{flex:none}#header-search::placeholder{color:#666;font-style:italic}#header-search:focus{outline:none;border-color:#888}@media(max-width:684px){.header__search{display:none}}</style><div class=content><article class=post><h1 class=post-title><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-1/>AI Systems Responsibility: Part 1 - Who Carries the Can?</a></h1><div class=post-meta><time class=post-date>2025-12-05</time><span class=post-author>Gareth</span><span class=post-reading-time>8 min read (1499 words)</span></div><span class=post-tags>#<a href=https://gazsecops.github.io/tags/ai/>ai</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/sysadmin/>sysadmin</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/operations/>operations</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/series/>series</a>&nbsp;</span><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#the-fundamental-problem>The Fundamental Problem</a></li><li><a href=#what-we-actually-control-spoiler-not-much>What We Actually Control (Spoiler: Not Much)</a></li><li><a href=#what-actually-breaks-everything>What Actually Breaks (Everything)</a></li><li><a href=#what-we-should-do-but-probably-wont>What We Should Do (But Probably Won&rsquo;t)</a><ul><li><a href=#the-minimum-viable-metrics>The Minimum Viable Metrics</a></li><li><a href=#quick-cli-triage>Quick CLI Triage</a></li></ul></li><li><a href=#the-tools-that-actually-work>The Tools That Actually Work</a></li><li><a href=#whats-coming>What&rsquo;s Coming</a></li></ul></nav></div><div class=post-content><div><div class=dialogue><p><span class="speaker speaker-management">Management:</span> We need to deploy the AI model to production by Friday.</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> Have you tested it?</p><p><span class="speaker speaker-management">Management:</span> The data scientists say it&rsquo;s good.</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> That&rsquo;s not what I asked.</p><p><span class="speaker speaker-management">Management:</span> Well, no, but marketing promised the client-</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> Then no.</p></div><aside class=pullquote>"When your AI system denies someone's mortgage application because they have a Welsh surname, guess who gets the phone call? Not the data scientist. Not the VP of Innovation. You."</aside><p>This is part one of a series about running AI systems when you&rsquo;re the poor sod responsible for keeping them alive. Not the hand-wavy ethics stuff. The practical bits: what breaks, how to know it&rsquo;s broken, and how to not get blamed when it inevitably goes sideways.</p><h2 id=the-fundamental-problem>The Fundamental Problem<a href=#the-fundamental-problem class=hanchor arialabel=Anchor>#</a></h2><p>Traditional software fails loudly. Database goes down? Error. API times out? Error. Memory leak? Eventually, error. You get logs, stack traces, alerts. Something to work with.</p><p>AI systems fail silently. They return answers. Confident, plausible, completely wrong answers. No error. No log entry saying &ldquo;WARNING: I&rsquo;m talking complete rubbish here.&rdquo; Just wrong.</p><p>The medical AI that misdiagnoses patients? No error thrown. Just bad outcomes.</p><p>The credit scoring model that denies legitimate applications? No exception logged. Just people mysteriously getting rejections.</p><p>The content moderation system that removes perfectly fine posts? No stack trace. Just angry users and eroding trust.</p><p>And when someone finally notices and escalates, who do they call? The sysadmin. Because we&rsquo;re the ones keeping the bloody thing running.</p><h2 id=what-we-actually-control-spoiler-not-much>What We Actually Control (Spoiler: Not Much)<a href=#what-we-actually-control-spoiler-not-much class=hanchor arialabel=Anchor>#</a></h2><p>We don&rsquo;t build the models. We don&rsquo;t choose the training data. We don&rsquo;t decide what problems to solve with AI. But we do control:</p><p><strong>What gets deployed.</strong> If your model hasn&rsquo;t been tested properly, I can refuse to ship it. Management hates this, but tough.</p><p><strong>Monitoring.</strong> Uptime, latency, accuracy drift, input distributions. If I don&rsquo;t know it&rsquo;s broken, I can&rsquo;t fix it. More importantly, if I don&rsquo;t know it&rsquo;s broken, I can&rsquo;t prove it wasn&rsquo;t my fault.</p><p><strong>Rollback capability.</strong> Can I revert to the old model when the new one tanks? If the answer is &ldquo;well, technically, but it takes four hours and requires coordination across three teams,&rdquo; then the answer is no.</p><p><strong>Human oversight.</strong> For critical decisions - medical, financial, legal - there should be a human in the loop. Not because humans are infallible, but because when things go wrong, you need someone who can explain why. &ldquo;The algorithm decided&rdquo; doesn&rsquo;t cut it.</p><p><strong>Incident response.</strong> When it breaks at 3am, how fast can I respond? Do I even have the tools to respond? Or am I waiting for the data science team to wake up?</p><p>This is where being properly awkward helps. Management wants to ship fast? Great. Show me the test results. Show me the rollback plan. Show me the monitoring. No? Then we&rsquo;re not shipping.</p><p>&ldquo;But we promised the client!&rdquo;</p><p>Not my problem. I&rsquo;m not putting my name on something that&rsquo;s going to blow up in production.</p><h2 id=what-actually-breaks-everything>What Actually Breaks (Everything)<a href=#what-actually-breaks-everything class=hanchor arialabel=Anchor>#</a></h2><p><strong>Data drift:</strong> Model trained on 2023 data. It&rsquo;s 2025 now. World changed. Model didn&rsquo;t. Accuracy plummets. No error logged. Just gradually worse predictions until someone notices.</p><p><strong>Infrastructure failure:</strong> GPU OOMs mid-inference. Your fallback logic? Wasn&rsquo;t tested. System returns garbage. Users make decisions based on garbage. Hilarity ensues.</p><p><strong>Integration bugs:</strong> Your model expects JSON with fields A, B, C. Upstream service changes format, now sends A, B, D. Parser fails silently. Model gets malformed input. Outputs nonsense. Nobody notices for three days.</p><p><strong>Performance degradation:</strong> Model takes 10 seconds per request instead of 100ms. Timeouts cascade. Everything falls over. Users complain. Management asks why we didn&rsquo;t see this coming. Because you wouldn&rsquo;t pay for load testing, Dave.</p><p><strong>Adversarial inputs:</strong> Someone figures out how to game your spam filter. Suddenly it&rsquo;s passing spam through. You don&rsquo;t notice until your inbox is full of cryptocurrency scams. Fraud detection? Same problem. One clever attacker, model&rsquo;s useless.</p><p>None of this throws a stack trace. None of this pages you at 3am. It just quietly breaks things until someone important notices.</p><h2 id=what-we-should-do-but-probably-wont>What We Should Do (But Probably Won&rsquo;t)<a href=#what-we-should-do-but-probably-wont class=hanchor arialabel=Anchor>#</a></h2><p><strong>1. Monitor everything</strong></p><p>Not just &ldquo;is it up?&rdquo; Monitor accuracy. Monitor latency distributions. Monitor input distributions. If inputs suddenly look different, that&rsquo;s a problem. If accuracy drops 5%, that&rsquo;s a problem. I want alerts before the CEO gets a phone call.</p><h3 id=the-minimum-viable-metrics>The Minimum Viable Metrics<a href=#the-minimum-viable-metrics class=hanchor arialabel=Anchor>#</a></h3><p>If you&rsquo;re using Prometheus, start with these:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span>model_prediction_latency_seconds{quantile<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>0.95</span>&#34;}
</span></span><span style=display:flex><span>model_accuracy_ratio
</span></span><span style=display:flex><span>model_prediction_total{status<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>error</span>&#34;}
</span></span><span style=display:flex><span>model_input_feature_distribution
</span></span></code></pre></div><p>Concrete alerts that actually catch problems:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>groups</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>ai-model</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>alert</span>: <span style=color:#ae81ff>ModelAccuracyDropped</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>expr</span>: |<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>          (model_accuracy_ratio offset 1h) - model_accuracy_ratio &gt; 0.05</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>for</span>: <span style=color:#ae81ff>5m</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>severity</span>: <span style=color:#ae81ff>critical</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>annotations</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>summary</span>: <span style=color:#e6db74>&#34;Model accuracy dropped more than 5% in the last hour&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>alert</span>: <span style=color:#ae81ff>ModelLatencyHigh</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>expr</span>: <span style=color:#ae81ff>histogram_quantile(0.95, rate(model_prediction_latency_seconds_bucket[5m])) &gt; 2</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>for</span>: <span style=color:#ae81ff>5m</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>severity</span>: <span style=color:#ae81ff>warning</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>annotations</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>summary</span>: <span style=color:#e6db74>&#34;Model P95 latency above 2 seconds&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>alert</span>: <span style=color:#ae81ff>ModelErrorRateHigh</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>expr</span>: <span style=color:#ae81ff>rate(model_prediction_total{status=&#34;error&#34;}[5m]) / rate(model_prediction_total[5m]) &gt; 0.01</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>for</span>: <span style=color:#ae81ff>5m</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>severity</span>: <span style=color:#ae81ff>critical</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>annotations</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>summary</span>: <span style=color:#e6db74>&#34;Model error rate above 1%&#34;</span>
</span></span></code></pre></div><h3 id=quick-cli-triage>Quick CLI Triage<a href=#quick-cli-triage class=hanchor arialabel=Anchor>#</a></h3><p>When someone says &ldquo;the model&rsquo;s broken&rdquo;, start here:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>curl -s http://model-service:8080/metrics | grep model_accuracy
</span></span><span style=display:flex><span>curl -s http://model-service:8080/metrics | grep model_prediction_total
</span></span><span style=display:flex><span>curl -s http://model-service:8080/health
</span></span></code></pre></div><p>Platform-specific logs:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>journalctl -u model-server --since <span style=color:#e6db74>&#34;10 minutes ago&#34;</span> | grep -i error
</span></span><span style=display:flex><span>docker logs model-server --tail<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span> 2&gt;&amp;<span style=color:#ae81ff>1</span> | grep -i error
</span></span><span style=display:flex><span>kubectl logs -l app<span style=color:#f92672>=</span>model-server --tail<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span> | grep -i error
</span></span></code></pre></div><p>If you don&rsquo;t have a <code>/metrics</code> endpoint, you&rsquo;re flying blind. Add it before you need it.</p><p><strong>2. Have an actual rollback plan</strong></p><p>&ldquo;We can roll back&rdquo; is not a plan. &ldquo;We can roll back in under 5 minutes by running this script&rdquo; is a plan. If your rollback requires a change management meeting, you don&rsquo;t have a rollback plan.</p><p>Concrete rollback patterns depend on your stack:</p><p><strong>Systemd service:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>sudo systemctl restart model-server@v1.2.3
</span></span><span style=display:flex><span>sudo systemctl status model-server
</span></span></code></pre></div><p><strong>Docker Compose:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>docker compose down
</span></span><span style=display:flex><span>docker compose up -d --build model-server:v1.2.3
</span></span><span style=display:flex><span>docker compose logs -f --tail<span style=color:#f92672>=</span><span style=color:#ae81ff>50</span>
</span></span></code></pre></div><p><strong>Kubernetes:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl rollout undo deployment/model-server
</span></span><span style=display:flex><span>kubectl rollout status deployment/model-server --timeout<span style=color:#f92672>=</span>60s
</span></span></code></pre></div><p><strong>Cloud container service (ECS/ACI/Cloud Run):</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>aws ecs update-service --cluster prod --service model-server --task-definition model-server:42
</span></span><span style=display:flex><span>gcloud run services update model-server --image gcr.io/prod/model-server:v1.2.3
</span></span><span style=display:flex><span>az container update --resource-group prod --name model-server --image acrprod.azurecr.io/model-server:v1.2.3
</span></span></code></pre></div><p>Test your rollback in staging. Time it. If it takes more than 5 minutes, you have a problem.</p><p><strong>3. Test on real data</strong></p><p>Not the clean synthetic data that makes your accuracy metrics look good. Real data. Messy data. Edge cases. Weird inputs. Data from demographics your training set ignored. If it works in prod but not in test, your tests are wrong.</p><p><strong>4. Keep humans in the loop for critical decisions</strong></p><p>AI can suggest. Humans decide. Medical diagnoses? Human reviews. Credit decisions? Human reviews. Legal judgments? Human reviews. I don&rsquo;t care if the model is 99% accurate. That 1% is someone&rsquo;s life.</p><p><strong>5. Document what it can&rsquo;t do</strong></p><p>Every model has failure modes. Write them down. Make sure everyone using the system knows what not to trust it with. Will management read this documentation? Probably not. But when it breaks, you can point to page 3, section 2, where you explicitly said not to do that.</p><p><strong>6. Plan for failure</strong></p><p>What happens when the model is unavailable? Graceful degradation? Manual fallback? Or does everything just stop? Because if everything just stops, you&rsquo;re getting a phone call at 3am.</p><h2 id=the-tools-that-actually-work>The Tools That Actually Work<a href=#the-tools-that-actually-work class=hanchor arialabel=Anchor>#</a></h2><p>You don&rsquo;t need AI-specific platforms. You need boring tools that do the job:</p><p><strong>Monitoring:</strong> Prometheus + Grafana. Export model metrics via a <code>/metrics</code> endpoint. Alert on accuracy drops, latency spikes, error rates. That&rsquo;s it.</p><p><strong>Logging:</strong> ELK stack, Loki, or just structured JSON logs to stdout. The important bit: log inputs, outputs, and confidence scores. You&rsquo;ll need them when debugging.</p><p><strong>Experiment tracking:</strong> MLflow for model versioning and experiment comparison. Or just git tags and S3 if you&rsquo;re keeping it simple.</p><p><strong>Deployment:</strong> Docker images with versioned tags. Blue-green or canary deployments. Feature flags for gradual rollout. Whether you&rsquo;re on systemd, Docker Compose, Kubernetes, or a managed container service, the pattern is the same: versioned artifacts, health checks, and a way to switch traffic.</p><p><strong>Testing:</strong> pytest for unit tests. Great Expectations for data validation. Locust or k6 for load testing.</p><p>The pattern: boring tools + discipline > fancy tools + chaos. Every AI platform vendor will tell you their thing solves monitoring. It doesn&rsquo;t. You still need to know what to monitor.</p><h2 id=whats-coming>What&rsquo;s Coming<a href=#whats-coming class=hanchor arialabel=Anchor>#</a></h2><p>This is part one. Future installments:</p><ul><li><strong>Part 2:</strong> Monitoring strategies that actually work (not the ones vendors sell you)</li><li><strong>Part 3:</strong> Incident response for AI systems (spoiler: it&rsquo;s worse than normal ops)</li><li><strong>Part 4:</strong> Testing before deployment (or: how to avoid discovering problems in prod)</li><li><strong>Part 5:</strong> When to say no (and how to make it stick)</li><li><strong>Part 6:</strong> Case studies of things going wrong (names changed to protect the guilty)</li></ul><p>The goal isn&rsquo;t to stop using AI. It&rsquo;s to stop using it stupidly. Run it properly, monitor it properly, and when it inevitably breaks, catch it before it makes the news.</p><p>Because when it does make the news, you&rsquo;re the one explaining it to management.</p><hr><p><em>Part 1 of N in a series on running AI systems without getting blamed for their failures.</em></p></div></div><div class=related-posts style="margin-top:3rem;padding-top:2rem;border-top:1px solid #333"><h3>Related Posts</h3><ul style=list-style:none;padding:0><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/>AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why</a>
<span style=color:#999;font-size:.9rem>- 19 Dec 2025</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#incident-response</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-2/>AI Systems Responsibility: Part 2 - Monitoring That Actually Works</a>
<span style=color:#999;font-size:.9rem>- 12 Dec 2025</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#monitoring</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/>AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode)</a>
<span style=color:#999;font-size:.9rem>- 30 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#tools</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/>AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions</a>
<span style=color:#999;font-size:.9rem>- 23 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#management</span>
<span style=margin-right:.5rem>#culture</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/>AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong)</a>
<span style=color:#999;font-size:.9rem>- 16 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#case-studies</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li></ul></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-2/ class="button inline prev">&lt; [<span class=button__text>AI Systems Responsibility: Part 2 - Monitoring That Actually Works</span>]
</a>::
<a href=https://gazsecops.github.io/posts/security-as-code-beyond-scanning/ class="button inline next">[<span class=button__text>Beyond Scanning: What Security as Code Really Means</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2026</span></div><div class=build-info><span class=build-commit>e733073</span><span class=build-date>2026-02-12 12:33 UTC</span></div></div></footer></div></body></html>