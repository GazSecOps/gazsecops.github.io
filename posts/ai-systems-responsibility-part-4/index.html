<!doctype html><html lang=en><head><title>AI Systems Responsibility: Part 4 - Testing Before You Ship (Or: Stop Discovering Bugs in Production) :: Gareth's Engineering Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content=' Engineer: We&rsquo;ve tested the model. It works.
Sysadmin: What did you test?
Engineer: We ran it on the validation set. 94% accuracy.
Sysadmin: What about edge cases?
Engineer: What edge cases?
Sysadmin: The ones that will break it in production.
Engineer: We&rsquo;ll handle those when we see them.
And that&rsquo;s how you end up debugging at 3am.
"Testing that your model loads and returns predictions is not testing. That&#39;s checking if Python still works. Testing is finding out all the ways your model breaks before your users do." Part 4 of the series. This one&rsquo;s about testing AI systems before deployment. Not the &ldquo;it runs on my laptop&rdquo; kind of testing. The &ldquo;I&rsquo;ve actively tried to break this and couldn&rsquo;t&rdquo; kind of testing.
'><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-4/><link rel=stylesheet href=https://gazsecops.github.io/css/extended.min.c658c723e006469d82f697e19c5338967fad12c57650bdd915bacf9cfbe2cc38.css><link rel=stylesheet href=https://gazsecops.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://gazsecops.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://gazsecops.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://gazsecops.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://gazsecops.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://gazsecops.github.io/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://gazsecops.github.io/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://gazsecops.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://gazsecops.github.io/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://gazsecops.github.io/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://gazsecops.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://gazsecops.github.io/favicon.png><link rel=apple-touch-icon href=https://gazsecops.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="AI Systems Responsibility: Part 4 - Testing Before You Ship (Or: Stop Discovering Bugs in Production)"><meta property="og:description" content=' Engineer: We&rsquo;ve tested the model. It works.
Sysadmin: What did you test?
Engineer: We ran it on the validation set. 94% accuracy.
Sysadmin: What about edge cases?
Engineer: What edge cases?
Sysadmin: The ones that will break it in production.
Engineer: We&rsquo;ll handle those when we see them.
And that&rsquo;s how you end up debugging at 3am.
"Testing that your model loads and returns predictions is not testing. That&#39;s checking if Python still works. Testing is finding out all the ways your model breaks before your users do." Part 4 of the series. This one&rsquo;s about testing AI systems before deployment. Not the &ldquo;it runs on my laptop&rdquo; kind of testing. The &ldquo;I&rsquo;ve actively tried to break this and couldn&rsquo;t&rdquo; kind of testing.
'><meta property="og:url" content="https://gazsecops.github.io/posts/ai-systems-responsibility-part-4/"><meta property="og:site_name" content="Gareth's Engineering Blog"><meta property="og:image" content="https://gazsecops.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2026-01-02 10:00:00 +0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>gareth@blog:~$</div></a></div><div class=header__search><input type=text id=header-search placeholder="Quick search..." style="padding:5px 10px;background:#222;color:#eee;border:1px solid #444;border-radius:3px;font-size:14px;width:200px"></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></nav></header><script>(function(){const e=document.getElementById("header-search");if(!e)return;e.addEventListener("keydown",function(e){if(e.key==="Enter"){const e=this.value.trim();e&&(window.location.href="/search?q="+encodeURIComponent(e))}}),document.addEventListener("keydown",function(t){t.key==="/"&&!["INPUT","TEXTAREA"].includes(t.target.tagName)&&(t.preventDefault(),e.focus()),t.key==="Escape"&&t.target===e&&e.blur()})})()</script><style>.header__inner{display:flex;align-items:center;gap:20px}.header__search{flex:none}#header-search::placeholder{color:#666;font-style:italic}#header-search:focus{outline:none;border-color:#888}@media(max-width:684px){.header__search{display:none}}</style><div class=content><article class=post><h1 class=post-title><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-4/>AI Systems Responsibility: Part 4 - Testing Before You Ship (Or: Stop Discovering Bugs in Production)</a></h1><div class=post-meta><time class=post-date>2026-01-02</time><span class=post-author>Gareth</span><span class=post-reading-time>9 min read (1858 words)</span></div><span class=post-tags>#<a href=https://gazsecops.github.io/tags/ai/>ai</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/testing/>testing</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/operations/>operations</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/series/>series</a>&nbsp;</span><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#the-standard-testing-which-is-worthless>The Standard Testing (Which Is Worthless)</a></li><li><a href=#what-actually-needs-testing>What Actually Needs Testing</a></li><li><a href=#the-testing-nobody-does-but-should>The Testing Nobody Does (But Should)</a></li><li><a href=#the-testing-thats-impossible-but-you-have-to-try-anyway>The Testing That&rsquo;s Impossible (But You Have To Try Anyway)</a></li><li><a href=#what-good-testing-looks-like>What Good Testing Looks Like</a></li><li><a href=#what-actually-happens>What Actually Happens</a></li><li><a href=#how-to-make-testing-happen>How To Make Testing Happen</a></li><li><a href=#part-5-preview>Part 5 Preview</a></li></ul></nav></div><div class=post-content><div><div class=dialogue><p><span class="speaker speaker-engineer">Engineer:</span> We&rsquo;ve tested the model. It works.</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> What did you test?</p><p><span class="speaker speaker-engineer">Engineer:</span> We ran it on the validation set. 94% accuracy.</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> What about edge cases?</p><p><span class="speaker speaker-engineer">Engineer:</span> What edge cases?</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> The ones that will break it in production.</p><p><span class="speaker speaker-engineer">Engineer:</span> We&rsquo;ll handle those when we see them.</p></div><p>And that&rsquo;s how you end up debugging at 3am.</p><aside class=pullquote>"Testing that your model loads and returns predictions is not testing. That's checking if Python still works. Testing is finding out all the ways your model breaks before your users do."</aside><p>Part 4 of the series. This one&rsquo;s about testing AI systems before deployment. Not the &ldquo;it runs on my laptop&rdquo; kind of testing. The &ldquo;I&rsquo;ve actively tried to break this and couldn&rsquo;t&rdquo; kind of testing.</p><h2 id=the-standard-testing-which-is-worthless>The Standard Testing (Which Is Worthless)<a href=#the-standard-testing-which-is-worthless class=hanchor arialabel=Anchor>#</a></h2><p>Most teams test AI like this:</p><ol><li>Split data into training/validation/test sets</li><li>Train on training set</li><li>Measure accuracy on validation set</li><li>Adjust hyperparameters</li><li>Measure accuracy on test set</li><li>If test accuracy is good, ship it</li></ol><p>This tells you one thing: your model performs well on data that looks like your training data.</p><p>It tells you nothing about:</p><ul><li>How it performs on data that doesn&rsquo;t look like training data</li><li>How it handles edge cases</li><li>How it fails when inputs are malformed</li><li>How it behaves under load</li><li>How it integrates with the rest of your system</li></ul><p>Congratulations, you&rsquo;ve confirmed that supervised learning works. Now do actual testing.</p><h2 id=what-actually-needs-testing>What Actually Needs Testing<a href=#what-actually-needs-testing class=hanchor arialabel=Anchor>#</a></h2><p><strong>1. Edge cases and boundary conditions</strong></p><p>Your model was trained on normal data. What happens with abnormal data?</p><ul><li>Empty inputs</li><li>Null values</li><li>Extremely long inputs</li><li>Extremely short inputs</li><li>Special characters</li><li>Unicode (emojis, non-Latin scripts)</li><li>Inputs outside the expected range</li><li>Inputs in different formats than expected</li></ul><p>Take your production model. Feed it garbage. See what happens. If it crashes, that&rsquo;s actually good - at least you get an error. Worse is when it confidently returns nonsense.</p><p><strong>Quick edge case test with pytest:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> pytest
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> model <span style=color:#f92672>import</span> predict
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TestEdgeCases</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_empty_input</span>(self):
</span></span><span style=display:flex><span>        result <span style=color:#f92672>=</span> predict(<span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> result <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_null_input</span>(self):
</span></span><span style=display:flex><span>        result <span style=color:#f92672>=</span> predict(<span style=color:#66d9ef>None</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> result <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_extremely_long_input</span>(self):
</span></span><span style=display:flex><span>        result <span style=color:#f92672>=</span> predict(<span style=color:#e6db74>&#34;x&#34;</span> <span style=color:#f92672>*</span> <span style=color:#ae81ff>100000</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> result <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_unicode_emoji</span>(self):
</span></span><span style=display:flex><span>        result <span style=color:#f92672>=</span> predict(<span style=color:#e6db74>&#34;hello ðŸŽ‰ world ðŸš€ test&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> result <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_non_latin</span>(self):
</span></span><span style=display:flex><span>        result <span style=color:#f92672>=</span> predict(<span style=color:#e6db74>&#34;ä½ å¥½ä¸–ç•Œ Ù…Ø±Ø­Ø¨Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù„Ù…&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>assert</span> result <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>
</span></span></code></pre></div><p>Run with: <code>pytest test_edge_cases.py -v</code></p><p><strong>2. Adversarial inputs</strong></p><p>Someone will try to game your model. Test for it.</p><p>Spam filter? Try inputs designed to look like legitimate email. Fraud detection? Try transactions designed to look legitimate. Content moderation? Try content that&rsquo;s technically within the rules but clearly shouldn&rsquo;t be.</p><p>If you&rsquo;re not actively trying to fool your model, someone else will. Better you find the vulnerabilities first.</p><p><strong>3. Data distribution shift</strong></p><p>Your model trained on data from Q1 2024. What happens when you run it on Q4 2024 data? Or 2025 data?</p><p>Take old models. Run them on recent data. See how accuracy degrades over time. This tells you how often you need to retrain.</p><p>If your model from six months ago is useless on today&rsquo;s data, you&rsquo;re going to be retraining constantly. Plan accordingly.</p><p><strong>4. Minority classes and underrepresented groups</strong></p><p>Your model has 95% accuracy overall. Great. What&rsquo;s the accuracy for:</p><ul><li>The smallest class in your dataset</li><li>Data from underrepresented demographics</li><li>Rare but important edge cases</li></ul><p>A model that&rsquo;s 98% accurate on common cases and 40% accurate on rare cases is a liability. Because the rare cases are often the important ones.</p><p>Medical diagnosis model? The rare diseases are often the serious ones. Financial fraud? The unusual transactions are often the fraudulent ones.</p><p>Test these specifically. Don&rsquo;t hide behind overall accuracy.</p><p><strong>5. Performance under load</strong></p><p>Your model handles 10 requests per second in testing. What about 1000 requests per second?</p><p>Load test it. See what happens when:</p><ul><li>GPU memory fills up</li><li>Request queue backs up</li><li>Latency increases</li><li>Timeouts start firing</li></ul><p>Does it degrade gracefully? Or does it fall over completely?</p><p>Also test: what happens when multiple models are running simultaneously? Because in production, you&rsquo;ll have old model running while new model spins up. Can your infrastructure handle both?</p><p><strong>Quick load test with Locust:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> locust <span style=color:#f92672>import</span> HttpUser, task, between
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ModelUser</span>(HttpUser):
</span></span><span style=display:flex><span>    wait_time <span style=color:#f92672>=</span> between(<span style=color:#ae81ff>0.1</span>, <span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>@task</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>client<span style=color:#f92672>.</span>post(<span style=color:#e6db74>&#34;/predict&#34;</span>, json<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;text&#34;</span>: <span style=color:#e6db74>&#34;sample input for testing&#34;</span>
</span></span><span style=display:flex><span>        })
</span></span></code></pre></div><p>Run with: <code>locust -f locustfile.py --host http://model-service:8080</code></p><p><strong>Or with k6:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-javascript data-lang=javascript><span style=display:flex><span><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>http</span> <span style=color:#a6e22e>from</span> <span style=color:#e6db74>&#39;k6/http&#39;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> { <span style=color:#a6e22e>check</span>, <span style=color:#a6e22e>sleep</span> } <span style=color:#a6e22e>from</span> <span style=color:#e6db74>&#39;k6&#39;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>export</span> <span style=color:#66d9ef>let</span> <span style=color:#a6e22e>options</span> <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>stages</span><span style=color:#f92672>:</span> [
</span></span><span style=display:flex><span>    { <span style=color:#a6e22e>duration</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;30s&#39;</span>, <span style=color:#a6e22e>target</span><span style=color:#f92672>:</span> <span style=color:#ae81ff>20</span> },
</span></span><span style=display:flex><span>    { <span style=color:#a6e22e>duration</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;1m&#39;</span>, <span style=color:#a6e22e>target</span><span style=color:#f92672>:</span> <span style=color:#ae81ff>100</span> },
</span></span><span style=display:flex><span>    { <span style=color:#a6e22e>duration</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;30s&#39;</span>, <span style=color:#a6e22e>target</span><span style=color:#f92672>:</span> <span style=color:#ae81ff>200</span> },
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>export</span> <span style=color:#66d9ef>default</span> <span style=color:#66d9ef>function</span> () {
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#a6e22e>payload</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>JSON</span>.<span style=color:#a6e22e>stringify</span>({ <span style=color:#a6e22e>text</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;sample input&#39;</span> });
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>let</span> <span style=color:#a6e22e>res</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>http</span>.<span style=color:#a6e22e>post</span>(<span style=color:#e6db74>&#39;http://model-service:8080/predict&#39;</span>, <span style=color:#a6e22e>payload</span>);
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>check</span>(<span style=color:#a6e22e>res</span>, { <span style=color:#e6db74>&#39;status was 200&#39;</span><span style=color:#f92672>:</span> (<span style=color:#a6e22e>r</span>) =&gt; <span style=color:#a6e22e>r</span>.<span style=color:#a6e22e>status</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>200</span> });
</span></span><span style=display:flex><span>  <span style=color:#a6e22e>sleep</span>(<span style=color:#ae81ff>0.1</span>);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Run with: <code>k6 run loadtest.js</code></p><p><strong>6. Integration testing</strong></p><p>Your model works in isolation. Does it work when integrated with the rest of your system?</p><ul><li>Can upstream services provide the inputs it needs?</li><li>Can downstream services handle its outputs?</li><li>What happens when upstream is slow?</li><li>What happens when downstream is unavailable?</li><li>Does error handling work end-to-end?</li></ul><p>I&rsquo;ve seen models that work perfectly in isolation break completely when integrated because nobody tested the full pipeline.</p><p><strong>Validate input data with Great Expectations:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> great_expectations <span style=color:#66d9ef>as</span> gx
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> pandas <span style=color:#66d9ef>as</span> pd
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_json(<span style=color:#e6db74>&#34;inputs.json&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>expectation_suite <span style=color:#f92672>=</span> gx<span style=color:#f92672>.</span>ExpectationSuite(name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model_inputs&#34;</span>)
</span></span><span style=display:flex><span>expectation_suite<span style=color:#f92672>.</span>add_expectation(
</span></span><span style=display:flex><span>    gx<span style=color:#f92672>.</span>expectations<span style=color:#f92672>.</span>ExpectColumnValuesToNotBeNull(column<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;user_id&#34;</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>expectation_suite<span style=color:#f92672>.</span>add_expectation(
</span></span><span style=display:flex><span>    gx<span style=color:#f92672>.</span>expectations<span style=color:#f92672>.</span>ExpectColumnValuesToBeBetween(
</span></span><span style=display:flex><span>        column<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;age&#34;</span>, min_value<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, max_value<span style=color:#f92672>=</span><span style=color:#ae81ff>120</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>expectation_suite<span style=color:#f92672>.</span>add_expectation(
</span></span><span style=display:flex><span>    gx<span style=color:#f92672>.</span>expectations<span style=color:#f92672>.</span>ExpectColumnValuesToBeInSet(
</span></span><span style=display:flex><span>        column<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;country&#34;</span>, value_set<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;US&#34;</span>, <span style=color:#e6db74>&#34;UK&#34;</span>, <span style=color:#e6db74>&#34;DE&#34;</span>, <span style=color:#e6db74>&#34;FR&#34;</span>]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>results <span style=color:#f92672>=</span> gx<span style=color:#f92672>.</span>validate(df, expectation_suite)
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> results<span style=color:#f92672>.</span>success:
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;Input validation failed!&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> result <span style=color:#f92672>in</span> results<span style=color:#f92672>.</span>results:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> result<span style=color:#f92672>.</span>success:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;  </span><span style=color:#e6db74>{</span>result<span style=color:#f92672>.</span>expectation_config<span style=color:#f92672>.</span>column<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>result<span style=color:#f92672>.</span>result<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p><strong>7. Rollback testing</strong></p><p>Can you actually roll back? Don&rsquo;t assume. Test it.</p><p>Deploy new model. Roll back to old model. Verify old model works. Do this in staging before you do it in production at 4am while management is screaming.</p><p>Also test: can you roll back quickly? If rollback takes 30 minutes, you&rsquo;re going to have 30 minutes of broken service.</p><p><strong>Sample rollback test script:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span>set -e
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;=== Rollback Test ===&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Current version:&#34;</span>
</span></span><span style=display:flex><span>curl -s http://model-service:8080/version
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Deploying new version...&#34;</span>
</span></span><span style=display:flex><span>./deploy.sh v2.0.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Checking health...&#34;</span>
</span></span><span style=display:flex><span>curl -s http://model-service:8080/health
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Rolling back...&#34;</span>
</span></span><span style=display:flex><span>./rollback.sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Checking health after rollback...&#34;</span>
</span></span><span style=display:flex><span>curl -s http://model-service:8080/health
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;&#34;</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;Current version after rollback:&#34;</span>
</span></span><span style=display:flex><span>curl -s http://model-service:8080/version
</span></span></code></pre></div><p>Time it. If it&rsquo;s more than 5 minutes, your rollback is too slow.</p><h2 id=the-testing-nobody-does-but-should>The Testing Nobody Does (But Should)<a href=#the-testing-nobody-does-but-should class=hanchor arialabel=Anchor>#</a></h2><p><strong>Chaos testing for AI</strong></p><p>Kill the model mid-request. What happens? Does the system fail gracefully? Or does everything explode?</p><p>Corrupt the input data. Feed malformed JSON. Send requests with missing required fields. See what breaks.</p><p>Make the model slow. Artificially add latency. See if timeouts work. See if fallback logic kicks in.</p><p>This is painful. This will find bugs. This is why nobody does it. But you should.</p><p><strong>Regression testing</strong></p><p>New model deployed. Is it better than the old model on everything? Or did you improve accuracy on common cases while making rare cases worse?</p><p>Test new model against old model on the same datasets. Compare predictions. Look for regressions.</p><p>I&rsquo;ve seen &ldquo;improvements&rdquo; that increased overall accuracy by 2% while making critical edge cases 20% worse. Nobody noticed until production.</p><p><strong>Bias and fairness testing</strong></p><p>Does your model treat all groups equally? Or does it perform better for some demographics than others?</p><p>This isn&rsquo;t just ethics (though it&rsquo;s that too). It&rsquo;s practical. If your model is biased, you&rsquo;re going to get sued. Or you&rsquo;re going to lose customers. Or you&rsquo;re going to make the news for the wrong reasons.</p><p>Test for disparate impact. Test for equal accuracy across groups. Test for fairness by whatever metric matters for your use case.</p><p>Will management care about this? Probably not until it becomes a PR problem.</p><p><strong>Monitoring in staging</strong></p><p>You have monitoring in production (Part 2). Do you have monitoring in staging?</p><p>Deploy to staging. Let it run for a week. Monitor accuracy. Monitor drift. Monitor latency. See if anything weird happens.</p><p>Catch problems in staging, not production.</p><h2 id=the-testing-thats-impossible-but-you-have-to-try-anyway>The Testing That&rsquo;s Impossible (But You Have To Try Anyway)<a href=#the-testing-thats-impossible-but-you-have-to-try-anyway class=hanchor arialabel=Anchor>#</a></h2><p><strong>Testing model explanations</strong></p><p>Your model makes a prediction. Can it explain why? Is the explanation correct?</p><p>This is incredibly hard to test. But if you&rsquo;re deploying models where explanations matter (medical, financial, legal), you need to try.</p><p>Take predictions. Look at explanations. Do they make sense? Are they consistent? Do subject matter experts agree with them?</p><p>Manual process. Doesn&rsquo;t scale. Do it anyway.</p><p><strong>Testing for unknown unknowns</strong></p><p>How do you test for things you don&rsquo;t know about?</p><p>You can&rsquo;t. But you can test for the category of &ldquo;things that don&rsquo;t look like training data.&rdquo;</p><p>Build an anomaly detector for your inputs. When something weird comes in, flag it. In testing, verify that weird inputs get flagged and handled appropriately.</p><p>Won&rsquo;t catch everything. Better than nothing.</p><h2 id=what-good-testing-looks-like>What Good Testing Looks Like<a href=#what-good-testing-looks-like class=hanchor arialabel=Anchor>#</a></h2><p><strong>Before deployment:</strong></p><ol><li>Model performs well on validation/test sets (baseline sanity check)</li><li>Model performs acceptably on edge cases (won&rsquo;t crash on weird inputs)</li><li>Model has known failure modes documented (we know where it breaks)</li><li>Model performance degrades gracefully (doesn&rsquo;t fall off a cliff)</li><li>Model integrates correctly with existing systems (end-to-end works)</li><li>Model rollback procedure tested and works (we can undo this)</li><li>Monitoring in place to catch problems (we&rsquo;ll know if it breaks)</li></ol><p><strong>After deployment (in staging):</strong></p><ol><li>Let it run for a week</li><li>Monitor everything</li><li>Compare to production model</li><li>Look for any anomalies</li><li>If it looks good, ship to production</li><li>If not, back to testing</li></ol><p><strong>After deployment (in production):</strong></p><ol><li>Canary deployment (small percentage of traffic)</li><li>Monitor like crazy</li><li>Compare to old model</li><li>Gradually increase traffic</li><li>Be ready to roll back at the first sign of trouble</li></ol><p>This is slow. This is boring. This catches bugs before users do.</p><h2 id=what-actually-happens>What Actually Happens<a href=#what-actually-happens class=hanchor arialabel=Anchor>#</a></h2><p>Theory: Thorough testing before deployment. Catch all issues in staging.</p><p>Reality: &ldquo;We need to ship this Friday.&rdquo; Testing gets cut. Deploy to production. Hope for the best.</p><p>Then: Production breaks. 3am incident. Rollback. Post-mortem.</p><p>Then: &ldquo;We need better testing.&rdquo; Everyone agrees.</p><p>Then: &ldquo;We need to ship this Friday.&rdquo; Testing gets cut again.</p><p>Cycle repeats.</p><h2 id=how-to-make-testing-happen>How To Make Testing Happen<a href=#how-to-make-testing-happen class=hanchor arialabel=Anchor>#</a></h2><p><strong>1. Make it fast</strong></p><p>If testing takes three weeks, it won&rsquo;t happen. Automate everything. Make it push-button. Make it fast enough that skipping it is harder than running it.</p><p><strong>2. Make it required</strong></p><p>Don&rsquo;t make testing optional. Make it a gate. Model doesn&rsquo;t deploy without test results. Management will complain. Hold firm.</p><p><strong>3. Make failures visible</strong></p><p>When testing finds problems, make sure everyone knows. Don&rsquo;t hide failures. Make them loud. Make them embarrassing.</p><p><strong>4. Make it part of the culture</strong></p><p>Testing isn&rsquo;t something the QA team does. Testing is something everyone does. Developers test. Data scientists test. Ops tests.</p><p>If your culture is &ldquo;ship fast and fix bugs in production,&rdquo; testing won&rsquo;t happen. Change the culture or accept the 3am incidents.</p><h2 id=part-5-preview>Part 5 Preview<a href=#part-5-preview class=hanchor arialabel=Anchor>#</a></h2><p>Next time: When to say no. How to push back when management wants to deploy something that shouldn&rsquo;t be deployed. How to make it stick without getting fired.</p><p>Spoiler: You won&rsquo;t always win. But you need to try.</p><hr><p><em>Part 4 of N in a series on running AI systems. <a href=/posts/ai-systems-responsibility-part-1/>Part 1: Who Carries the Can?</a> | <a href=/posts/ai-systems-responsibility-part-2/>Part 2: Monitoring</a> | <a href=/posts/ai-systems-responsibility-part-3/>Part 3: Incident Response</a></em></p></div></div><div class=related-posts style="margin-top:3rem;padding-top:2rem;border-top:1px solid #333"><h3>Related Posts</h3><ul style=list-style:none;padding:0><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/>AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode)</a>
<span style=color:#999;font-size:.9rem>- 30 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#tools</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/>AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions</a>
<span style=color:#999;font-size:.9rem>- 23 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#management</span>
<span style=margin-right:.5rem>#culture</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/>AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong)</a>
<span style=color:#999;font-size:.9rem>- 16 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#case-studies</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-5/>AI Systems Responsibility: Part 5 - When to Say No (And How to Make It Stick)</a>
<span style=color:#999;font-size:.9rem>- 9 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#management</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/>AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why</a>
<span style=color:#999;font-size:.9rem>- 19 Dec 2025</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#incident-response</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li></ul></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-5/ class="button inline prev">&lt; [<span class=button__text>AI Systems Responsibility: Part 5 - When to Say No (And How to Make It Stick)</span>]
</a>::
<a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/ class="button inline next">[<span class=button__text>AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>Â© 2026</span></div><div class=build-info><span class=build-commit>e733073</span><span class=build-date>2026-02-12 12:33 UTC</span></div></div></footer></div></body></html>