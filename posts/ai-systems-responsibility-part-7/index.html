<!doctype html><html lang=en><head><title>AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions :: Gareth's Engineering Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='Six parts of this series explaining what goes wrong with AI systems. Testing that doesn&rsquo;t happen. Monitoring that doesn&rsquo;t exist. Models shipped before they&rsquo;re ready. Incidents that could have been prevented.
You&rsquo;d think after reading all that, teams would learn. They don&rsquo;t.
The same mistakes happen over and over. Not because people are stupid. Because the organizational incentives guarantee failure.
"You can have thorough testing or you can ship by Friday. You can&#39;t have both. Management always picks Friday." Part 7. The last of the core series. Why organizations are structurally incapable of running AI systems properly, and what you can do about it (spoiler: not much).
'><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/><link rel=stylesheet href=https://gazsecops.github.io/css/extended.min.c658c723e006469d82f697e19c5338967fad12c57650bdd915bacf9cfbe2cc38.css><link rel=stylesheet href=https://gazsecops.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://gazsecops.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://gazsecops.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://gazsecops.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://gazsecops.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://gazsecops.github.io/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://gazsecops.github.io/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://gazsecops.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://gazsecops.github.io/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://gazsecops.github.io/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://gazsecops.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://gazsecops.github.io/favicon.png><link rel=apple-touch-icon href=https://gazsecops.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions"><meta property="og:description" content='Six parts of this series explaining what goes wrong with AI systems. Testing that doesn&rsquo;t happen. Monitoring that doesn&rsquo;t exist. Models shipped before they&rsquo;re ready. Incidents that could have been prevented.
You&rsquo;d think after reading all that, teams would learn. They don&rsquo;t.
The same mistakes happen over and over. Not because people are stupid. Because the organizational incentives guarantee failure.
"You can have thorough testing or you can ship by Friday. You can&#39;t have both. Management always picks Friday." Part 7. The last of the core series. Why organizations are structurally incapable of running AI systems properly, and what you can do about it (spoiler: not much).
'><meta property="og:url" content="https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/"><meta property="og:site_name" content="Gareth's Engineering Blog"><meta property="og:image" content="https://gazsecops.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2026-01-23 10:00:00 +0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>gareth@blog:~$</div></a></div><div class=header__search><input type=text id=header-search placeholder="Quick search..." style="padding:5px 10px;background:#222;color:#eee;border:1px solid #444;border-radius:3px;font-size:14px;width:200px"></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></nav></header><script>(function(){const e=document.getElementById("header-search");if(!e)return;e.addEventListener("keydown",function(e){if(e.key==="Enter"){const e=this.value.trim();e&&(window.location.href="/search?q="+encodeURIComponent(e))}}),document.addEventListener("keydown",function(t){t.key==="/"&&!["INPUT","TEXTAREA"].includes(t.target.tagName)&&(t.preventDefault(),e.focus()),t.key==="Escape"&&t.target===e&&e.blur()})})()</script><style>.header__inner{display:flex;align-items:center;gap:20px}.header__search{flex:none}#header-search::placeholder{color:#666;font-style:italic}#header-search:focus{outline:none;border-color:#888}@media(max-width:684px){.header__search{display:none}}</style><div class=content><article class=post><h1 class=post-title><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/>AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions</a></h1><div class=post-meta><time class=post-date>2026-01-23</time><span class=post-author>Gareth</span><span class=post-reading-time>9 min read (1853 words)</span></div><span class=post-tags>#<a href=https://gazsecops.github.io/tags/ai/>ai</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/management/>management</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/culture/>culture</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/operations/>operations</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/series/>series</a>&nbsp;</span><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#the-incentive-problem>The Incentive Problem</a></li><li><a href=#the-knowledge-problem>The Knowledge Problem</a></li><li><a href=#the-metrics-problem>The Metrics Problem</a></li><li><a href=#the-responsibility-problem>The Responsibility Problem</a></li><li><a href=#the-time-horizon-problem>The Time Horizon Problem</a></li><li><a href=#the-expertise-problem>The Expertise Problem</a></li><li><a href=#the-culture-problem>The Culture Problem</a></li><li><a href=#the-technical-debt-problem>The Technical Debt Problem</a></li><li><a href=#what-actually-works-rarely>What Actually Works (Rarely)</a></li><li><a href=#what-you-can-actually-do>What You Can Actually Do</a></li><li><a href=#the-uncomfortable-truth>The Uncomfortable Truth</a></li><li><a href=#the-end>The End</a></li></ul></nav></div><div class=post-content><div><p>Six parts of this series explaining what goes wrong with AI systems. Testing that doesn&rsquo;t happen. Monitoring that doesn&rsquo;t exist. Models shipped before they&rsquo;re ready. Incidents that could have been prevented.</p><p>You&rsquo;d think after reading all that, teams would learn. They don&rsquo;t.</p><p>The same mistakes happen over and over. Not because people are stupid. Because the organizational incentives guarantee failure.</p><aside class=pullquote>"You can have thorough testing or you can ship by Friday. You can't have both. Management always picks Friday."</aside><p>Part 7. The last of the core series. Why organizations are structurally incapable of running AI systems properly, and what you can do about it (spoiler: not much).</p><h2 id=the-incentive-problem>The Incentive Problem<a href=#the-incentive-problem class=hanchor arialabel=Anchor>#</a></h2><p><strong>What gets rewarded:</strong> Shipping fast. Meeting deadlines. Saying yes. Being a team player. Delivering features.</p><p><strong>What gets punished:</strong> Saying no. Raising concerns. Delaying deployments. Being &ldquo;negative.&rdquo; Blocking progress.</p><p>You&rsquo;re a sysadmin. You see a model that isn&rsquo;t ready. You raise concerns. What happens?</p><p>Best case: You&rsquo;re overruled. Model ships anyway. When it breaks, you get blamed for not testing it properly.</p><p>Worst case: You&rsquo;re labeled &ldquo;not a team player.&rdquo; Passed over for promotion. Eventually managed out.</p><p>Either way, the model ships.</p><p>So next time, you raise concerns less loudly. Or you don&rsquo;t raise them at all. Model ships. Breaks. Cycle repeats.</p><p>The incentives guarantee bad outcomes.</p><h2 id=the-knowledge-problem>The Knowledge Problem<a href=#the-knowledge-problem class=hanchor arialabel=Anchor>#</a></h2><p><strong>Who knows the model is broken:</strong> Sysadmins. Data scientists. Engineers. People actually testing it.</p><p><strong>Who decides whether to ship:</strong> Management. Product managers. Sales. People who haven&rsquo;t seen the test results.</p><p><strong>Who bridges the gap:</strong> Nobody, effectively.</p><p>Conversation goes like this:</p><div class=dialogue><p><span class="speaker speaker-engineer">Engineer:</span> The model fails on edge case X.</p><p><span class="speaker speaker-management">Management:</span> How often does edge case X occur?</p><p><span class="speaker speaker-engineer">Engineer:</span> 5% of inputs.</p><p><span class="speaker speaker-management">Management:</span> So it works 95% of the time?</p><p><span class="speaker speaker-engineer">Engineer:</span> Well, yes, but-</p><p><span class="speaker speaker-management">Management:</span> Good enough. Ship it.</p></div><p>Engineer tried to communicate risk. Management heard &ldquo;95% success rate.&rdquo; Shipped broken model.</p><p>This isn&rsquo;t management being stupid. This is a communication failure. Technical people speak in technical terms. Management hears business terms. Risk gets lost in translation.</p><h2 id=the-metrics-problem>The Metrics Problem<a href=#the-metrics-problem class=hanchor arialabel=Anchor>#</a></h2><p><strong>What gets measured:</strong> Deployment velocity. Feature delivery. Time to market.</p><p><strong>What doesn&rsquo;t get measured:</strong> Quality. Reliability. Maintainability. Long-term sustainability.</p><p>You can&rsquo;t manage what you don&rsquo;t measure. If you&rsquo;re measuring velocity but not quality, guess which one gets optimized?</p><p>Teams that ship fast get rewarded. Teams that ship reliable software get ignored. So teams optimize for shipping fast.</p><p>Model&rsquo;s ready? Ship it. Model&rsquo;s not ready? Ship it anyway. We&rsquo;ll fix it in production.</p><p>Then production breaks. Everyone&rsquo;s surprised. &ldquo;How did we not catch this in testing?&rdquo;</p><p>You didn&rsquo;t catch it in testing because testing takes time and you measured velocity.</p><h2 id=the-responsibility-problem>The Responsibility Problem<a href=#the-responsibility-problem class=hanchor arialabel=Anchor>#</a></h2><p><strong>Who&rsquo;s responsible for the model working:</strong> Everyone, theoretically. Nobody, practically.</p><p>Data scientists build it. Engineers deploy it. Ops runs it. Product defines requirements. Management approves it.</p><p>When it breaks, whose fault is it?</p><p>Data scientists: &ldquo;We built what product asked for.&rdquo;</p><p>Product: &ldquo;We specified the requirements. Engineering should have tested it.&rdquo;</p><p>Engineering: &ldquo;We deployed what data science built. Ops should have monitored it.&rdquo;</p><p>Ops: &ldquo;We monitored uptime. Model accuracy isn&rsquo;t our responsibility.&rdquo;</p><p>Everybody&rsquo;s fault. Nobody&rsquo;s fault. No accountability.</p><p>So the model breaks again. And again. And nobody fixes the underlying problem because nobody owns it.</p><h2 id=the-time-horizon-problem>The Time Horizon Problem<a href=#the-time-horizon-problem class=hanchor arialabel=Anchor>#</a></h2><p><strong>What management cares about:</strong> This quarter&rsquo;s results. This year&rsquo;s targets.</p><p><strong>What AI systems need:</strong> Long-term planning. Sustained investment. Continuous maintenance.</p><p>Management wants results now. Ship the model. Hit the deadline. Show ROI this quarter.</p><p>AI systems degrade over time. Data drifts. Accuracy drops. Models need retraining. This takes ongoing effort and investment.</p><p>But ongoing investment doesn&rsquo;t show immediate ROI. So it doesn&rsquo;t happen. Models get deployed and abandoned. Accuracy degrades. Nobody notices until it&rsquo;s critical.</p><p>Then: emergency retraining. Crisis mode. &ldquo;Why didn&rsquo;t we see this coming?&rdquo;</p><p>You did see it coming. You chose not to invest in preventing it.</p><h2 id=the-expertise-problem>The Expertise Problem<a href=#the-expertise-problem class=hanchor arialabel=Anchor>#</a></h2><p><strong>Who has AI expertise:</strong> Data scientists. ML engineers. A handful of specialists.</p><p><strong>Who&rsquo;s running AI systems:</strong> Regular engineers and sysadmins who&rsquo;ve never dealt with this before.</p><p>Traditional software: Most engineers understand it. Standard patterns. Known failure modes. Well-documented debugging techniques.</p><p>AI systems: Specialized knowledge required. Most engineers don&rsquo;t have it. Learning curve is steep. Documentation is sparse.</p><p>Result: Systems running in production that nobody fully understands. When they break, nobody knows how to fix them. Escalation to the few people who do understand. Bottleneck. Delays. Incidents.</p><p>Worse: The people who built the models move on. Take the knowledge with them. New team inherits systems they don&rsquo;t understand. Can&rsquo;t maintain properly. Can&rsquo;t debug effectively.</p><h2 id=the-culture-problem>The Culture Problem<a href=#the-culture-problem class=hanchor arialabel=Anchor>#</a></h2><p><strong>What the culture says:</strong> Move fast. Break things. Iterate. Fail fast. Ship and learn.</p><p><strong>What AI needs:</strong> Move carefully. Test thoroughly. Plan for failure. Understand before shipping.</p><p>These are fundamentally incompatible.</p><p>Move-fast culture works for web apps where you can deploy fixes in minutes. Doesn&rsquo;t work for AI where you can&rsquo;t quickly fix a broken model at 3am.</p><p>But nobody wants to change the culture. Culture made the company successful. AI is just another technology. Use the same process. Ship fast. Fix in production.</p><p>Then production breaks. And you can&rsquo;t fix it quickly. And customers are affected. And management asks why we didn&rsquo;t test it properly.</p><p>Because your culture doesn&rsquo;t allow time for testing properly.</p><h2 id=the-technical-debt-problem>The Technical Debt Problem<a href=#the-technical-debt-problem class=hanchor arialabel=Anchor>#</a></h2><p><strong>What happens when you move fast:</strong> Technical debt accumulates. Shortcuts. Workarounds. &ldquo;We&rsquo;ll fix it later.&rdquo;</p><p><strong>What happens to technical debt:</strong> It doesn&rsquo;t get fixed. It accumulates. It compounds. It eventually breaks everything.</p><p>AI systems compound this. Every shortcut in data pipelines. Every workaround in feature engineering. Every &ldquo;good enough&rdquo; model shipped without proper testing.</p><p>It all accumulates.</p><p>Then: Someone needs to debug why the model broke. Discovers three layers of workarounds. Nobody remembers why. Nobody documented it. Can&rsquo;t fix without breaking something else.</p><p>Technical debt is insidious. Doesn&rsquo;t show up in sprint velocity. Doesn&rsquo;t affect this quarter&rsquo;s metrics. Just slowly makes everything harder until the system is unmaintainable.</p><h2 id=what-actually-works-rarely>What Actually Works (Rarely)<a href=#what-actually-works-rarely class=hanchor arialabel=Anchor>#</a></h2><p>Some organizations get this right. Not many. Here&rsquo;s what they do differently:</p><p><strong>1. They measure quality, not just velocity</strong></p><p>Track defects. Track incidents. Track time to resolve. Track customer satisfaction. Reward teams that ship reliable software, not just fast software.</p><p><strong>2. They enforce gates</strong></p><p>Can&rsquo;t deploy without test results. Can&rsquo;t deploy without monitoring. Can&rsquo;t deploy without rollback plan. No exceptions. No &ldquo;we&rsquo;ll do it in v2.&rdquo;</p><p><strong>3. They staff for maintenance, not just features</strong></p><p>Budget for ongoing maintenance. Budget for retraining. Budget for monitoring. Don&rsquo;t treat AI as &ldquo;ship it and forget it.&rdquo;</p><p><strong>4. They have actual ownership</strong></p><p>One team owns each model end-to-end. Data science, engineering, ops. All one team. No handing off. No &ldquo;not my problem.&rdquo;</p><p><strong>5. They invest in expertise</strong></p><p>Train engineers on AI systems. Hire people with ML ops experience. Don&rsquo;t expect regular sysadmins to magically understand how to run models in production.</p><p><strong>6. They actually say no</strong></p><p>When a model isn&rsquo;t ready, they don&rsquo;t ship it. Deadlines slip. Management gets upset. They ship it when it&rsquo;s ready.</p><p>Most organizations don&rsquo;t do any of this. Too hard. Too slow. Not enough ROI.</p><p>So they keep making the same mistakes.</p><h2 id=what-you-can-actually-do>What You Can Actually Do<a href=#what-you-can-actually-do class=hanchor arialabel=Anchor>#</a></h2><p>You&rsquo;re a sysadmin. You&rsquo;re responsible for keeping AI systems running. You don&rsquo;t control the culture. You don&rsquo;t set the incentives. You don&rsquo;t make the decisions.</p><p>What can you do?</p><p><strong>1. Document everything</strong></p><p>When you raise concerns and get overruled, document it. Email. Tickets. Slack messages. Contemporaneous record. Not to cover your arse (though it does). To have evidence when things break.</p><p><strong>What to track in a &ldquo;known issues&rdquo; log:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-markdown data-lang=markdown><span style=display:flex><span>| Date | Model/System | Issue Raised | Evidence | Decision | Outcome |
</span></span><span style=display:flex><span>|------|---------------|--------------|----------|----------|---------|
</span></span><span style=display:flex><span>| 2026-01-15 | Fraud v2 | 30% false positive on new card types | Test results, p-value &lt; 0.01 | Ship anyway, fix in v2.1 | [Pending] |
</span></span><span style=display:flex><span>| 2026-01-20 | Chatbot | No input validation on prompts | Adversarial test, 3/5 exploits worked | Add rate limiting, ship | [Pending] |
</span></span></code></pre></div><p>Fill in the &ldquo;Outcome&rdquo; column when it breaks (or doesn&rsquo;t). This is your evidence base. When management asks &ldquo;why do we keep having these incidents?&rdquo;, you have data.</p><p><strong>2. Build visibility</strong></p><p>Make problems visible. Dashboards showing accuracy degradation. Reports showing technical debt. Metrics showing incident frequency. Can&rsquo;t ignore what you can see.</p><p><strong>Metrics that force attention:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span>AI system health <span style=color:#f92672>(</span>weekly report<span style=color:#f92672>)</span><span style=color:#960050;background-color:#1e0010>:</span>
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> Average model accuracy<span style=color:#960050;background-color:#1e0010>:</span> [<span style=color:#960050;background-color:#1e0010>X</span>]<span style=color:#f92672>%</span>
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> Accuracy trend <span style=color:#f92672>(</span><span style=color:#ae81ff>4</span> weeks<span style=color:#f92672>)</span><span style=color:#960050;background-color:#1e0010>:</span> [<span style=color:#960050;background-color:#1e0010>improving/stable/degrading</span>]
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> Incidents this <span style=color:#66d9ef>month</span><span style=color:#960050;background-color:#1e0010>:</span> [<span style=color:#960050;background-color:#1e0010>X</span>]
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> Incidents caused <span style=color:#66d9ef>by</span> known issues<span style=color:#960050;background-color:#1e0010>:</span> [<span style=color:#960050;background-color:#1e0010>X</span>] <span style=color:#f92672>(</span>what we shipped anyway<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> Mean <span style=color:#66d9ef>time</span> to detect<span style=color:#960050;background-color:#1e0010>:</span> [<span style=color:#960050;background-color:#1e0010>X</span>] minutes
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> Mean <span style=color:#66d9ef>time</span> to rollback<span style=color:#960050;background-color:#1e0010>:</span> [<span style=color:#960050;background-color:#1e0010>X</span>] minutes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Technical debt<span style=color:#960050;background-color:#1e0010>:</span>
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> Models <span style=color:#66d9ef>without</span> monitoring<span style=color:#960050;background-color:#1e0010>:</span> [<span style=color:#960050;background-color:#1e0010>X</span>]
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> Models with no documented failure modes<span style=color:#960050;background-color:#1e0010>:</span> [<span style=color:#960050;background-color:#1e0010>X</span>]
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> Models where <span style=color:#f92672>or</span>iginal author left<span style=color:#960050;background-color:#1e0010>:</span> [<span style=color:#960050;background-color:#1e0010>X</span>]
</span></span><span style=display:flex><span><span style=color:#f92672>-</span> Rollback procedures tested in last <span style=color:#ae81ff>90</span> days<span style=color:#960050;background-color:#1e0010>:</span> [<span style=color:#960050;background-color:#1e0010>X/Y</span>]
</span></span></code></pre></div><p>When you put these numbers in front of management every month, patterns become obvious. &ldquo;We&rsquo;ve had 4 incidents this quarter from models we knew were broken&rdquo; is harder to ignore than vague complaints.</p><p><strong>3. Build allies</strong></p><p>Find people who care about reliability. Engineers who are tired of 3am incidents. Managers who&rsquo;ve been burned before. Product people who&rsquo;ve lost customers to bugs. Build coalition.</p><p><strong>4. Pick your battles</strong></p><p>You can&rsquo;t fight everything. Choose what matters. Critical systems. High-risk deployments. Known failure modes. Fight those. Let the small stuff go.</p><p><strong>5. Demonstrate value</strong></p><p>When you prevent an incident by catching a bug, make it known. When you roll back quickly and save the day, make it known. Build reputation for being right.</p><p><strong>6. Have a backup plan</strong></p><p>If it all goes to hell, what&rsquo;s your plan? Different team? Different company? Different career? Don&rsquo;t get stuck in an organization that doesn&rsquo;t value what you do.</p><h2 id=the-uncomfortable-truth>The Uncomfortable Truth<a href=#the-uncomfortable-truth class=hanchor arialabel=Anchor>#</a></h2><p>Most organizations are structurally incapable of running AI systems properly. The incentives are wrong. The culture is wrong. The processes are wrong.</p><p>You can fight it. You can push back. You can document. You can build allies.</p><p>Sometimes you&rsquo;ll win. More often you&rsquo;ll lose.</p><p>The system is designed to produce bad outcomes. Your job is to minimize the damage. Not prevent it entirely. Minimize it.</p><p>Keep the incidents small. Keep the blast radius contained. Keep the rollback plan ready.</p><p>Because the 3am phone call is coming. The model will break. Production will fail.</p><p>And when it does, you&rsquo;re the one who has to fix it.</p><h2 id=the-end>The End<a href=#the-end class=hanchor arialabel=Anchor>#</a></h2><p>That&rsquo;s the core series. Seven parts on running AI systems responsibly. There&rsquo;s a bonus Part 8 with practical tools.</p><p>What you should do:</p><ul><li>Test properly (Part 4)</li><li>Monitor everything (Part 2)</li><li>Have rollback plans (Part 1)</li><li>Prepare for incidents (Part 3)</li><li>Say no when it matters (Part 5)</li><li>Learn from failures (Part 6)</li></ul><p>What will actually happen:</p><ul><li>Deadlines will slip</li><li>Testing will get cut</li><li>Models will ship broken</li><li>Production will break</li><li>You&rsquo;ll fix it at 3am</li><li>Cycle repeats</li></ul><p>Welcome to running AI systems in the real world.</p><p>Good luck. You&rsquo;ll need it.</p><hr><p><em>Part 7 of a series on running AI systems. <a href=/posts/ai-systems-responsibility-part-1/>Part 1: Who Carries the Can?</a> | <a href=/posts/ai-systems-responsibility-part-2/>Part 2: Monitoring</a> | <a href=/posts/ai-systems-responsibility-part-3/>Part 3: Incident Response</a> | <a href=/posts/ai-systems-responsibility-part-4/>Part 4: Testing</a> | <a href=/posts/ai-systems-responsibility-part-5/>Part 5: When to Say No</a> | <a href=/posts/ai-systems-responsibility-part-6/>Part 6: Case Studies</a> | <a href=/posts/ai-systems-responsibility-part-8/>Part 8: Practical Tools</a></em></p></div></div><div class=related-posts style="margin-top:3rem;padding-top:2rem;border-top:1px solid #333"><h3>Related Posts</h3><ul style=list-style:none;padding:0><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-5/>AI Systems Responsibility: Part 5 - When to Say No (And How to Make It Stick)</a>
<span style=color:#999;font-size:.9rem>- 9 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#management</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/>AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode)</a>
<span style=color:#999;font-size:.9rem>- 30 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#tools</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/>AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong)</a>
<span style=color:#999;font-size:.9rem>- 16 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#case-studies</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-4/>AI Systems Responsibility: Part 4 - Testing Before You Ship (Or: Stop Discovering Bugs in Production)</a>
<span style=color:#999;font-size:.9rem>- 2 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#testing</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/>AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why</a>
<span style=color:#999;font-size:.9rem>- 19 Dec 2025</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#incident-response</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li></ul></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/ class="button inline prev">&lt; [<span class=button__text>AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode)</span>]
</a>::
<a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/ class="button inline next">[<span class=button__text>AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong)</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2026</span></div><div class=build-info><span class=build-commit>e733073</span><span class=build-date>2026-02-12 12:33 UTC</span></div></div></footer></div></body></html>