<!doctype html><html lang=en><head><title>AI Systems Responsibility: Part 2 - Monitoring That Actually Works :: Gareth's Engineering Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content=" Engineer: The model&rsquo;s working fine. Look, uptime is 99.9%!
Sysadmin: And the accuracy?
Engineer: What?
Sysadmin: The accuracy. What percentage of predictions are correct?
Engineer: We don&rsquo;t track that.
Sysadmin: Then you don&rsquo;t know if it&rsquo;s working.
&#34;Your monitoring dashboard shows five green lights and one red one. The red one is 'model accuracy: unknown'. That's not a monitoring problem. That's a career problem.&#34; Part 2 of the series on running AI systems. This one&rsquo;s about monitoring. Not the useless kind vendors sell you. The kind that actually tells you when things are broken before management finds out from Twitter.
"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-2/><link rel=stylesheet href=https://gazsecops.github.io/css/extended.min.c658c723e006469d82f697e19c5338967fad12c57650bdd915bacf9cfbe2cc38.css><link rel=stylesheet href=https://gazsecops.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://gazsecops.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://gazsecops.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://gazsecops.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://gazsecops.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://gazsecops.github.io/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://gazsecops.github.io/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://gazsecops.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://gazsecops.github.io/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://gazsecops.github.io/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://gazsecops.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://gazsecops.github.io/favicon.png><link rel=apple-touch-icon href=https://gazsecops.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="AI Systems Responsibility: Part 2 - Monitoring That Actually Works"><meta property="og:description" content=" Engineer: The model&rsquo;s working fine. Look, uptime is 99.9%!
Sysadmin: And the accuracy?
Engineer: What?
Sysadmin: The accuracy. What percentage of predictions are correct?
Engineer: We don&rsquo;t track that.
Sysadmin: Then you don&rsquo;t know if it&rsquo;s working.
&#34;Your monitoring dashboard shows five green lights and one red one. The red one is 'model accuracy: unknown'. That's not a monitoring problem. That's a career problem.&#34; Part 2 of the series on running AI systems. This one&rsquo;s about monitoring. Not the useless kind vendors sell you. The kind that actually tells you when things are broken before management finds out from Twitter.
"><meta property="og:url" content="https://gazsecops.github.io/posts/ai-systems-responsibility-part-2/"><meta property="og:site_name" content="Gareth's Engineering Blog"><meta property="og:image" content="https://gazsecops.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2025-12-12 10:00:00 +0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>gareth@blog:~$</div></a></div><div class=header__search><input type=text id=header-search placeholder="Quick search..." style="padding:5px 10px;background:#222;color:#eee;border:1px solid #444;border-radius:3px;font-size:14px;width:200px"></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></nav></header><script>(function(){const e=document.getElementById("header-search");if(!e)return;e.addEventListener("keydown",function(e){if(e.key==="Enter"){const e=this.value.trim();e&&(window.location.href="/search?q="+encodeURIComponent(e))}}),document.addEventListener("keydown",function(t){t.key==="/"&&!["INPUT","TEXTAREA"].includes(t.target.tagName)&&(t.preventDefault(),e.focus()),t.key==="Escape"&&t.target===e&&e.blur()})})()</script><style>.header__inner{display:flex;align-items:center;gap:20px}.header__search{flex:none}#header-search::placeholder{color:#666;font-style:italic}#header-search:focus{outline:none;border-color:#888}@media(max-width:684px){.header__search{display:none}}</style><div class=content><article class=post><h1 class=post-title><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-2/>AI Systems Responsibility: Part 2 - Monitoring That Actually Works</a></h1><div class=post-meta><time class=post-date>2025-12-12</time><span class=post-author>Gareth</span><span class=post-reading-time>7 min read (1419 words)</span></div><span class=post-tags>#<a href=https://gazsecops.github.io/tags/ai/>ai</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/monitoring/>monitoring</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/operations/>operations</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/series/>series</a>&nbsp;</span><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#the-usual-monitoring-is-useless>The Usual Monitoring Is Useless</a></li><li><a href=#what-you-actually-need-to-monitor>What You Actually Need to Monitor</a></li><li><a href=#the-hard-part-getting-ground-truth>The Hard Part: Getting Ground Truth</a></li><li><a href=#alerting-that-doesnt-suck>Alerting That Doesn&rsquo;t Suck</a></li><li><a href=#tools-that-dont-hate-you>Tools That Don&rsquo;t Hate You</a><ul><li><a href=#make-it-real-a-minimal-metrics-set>Make It Real: A Minimal Metrics Set</a></li><li><a href=#promql-you-can-actually-use>PromQL You Can Actually Use</a></li></ul></li><li><a href=#what-actually-happens>What Actually Happens</a></li><li><a href=#part-3-preview>Part 3 Preview</a></li></ul></nav></div><div class=post-content><div><div class=dialogue><p><span class="speaker speaker-engineer">Engineer:</span> The model&rsquo;s working fine. Look, uptime is 99.9%!</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> And the accuracy?</p><p><span class="speaker speaker-engineer">Engineer:</span> What?</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> The accuracy. What percentage of predictions are correct?</p><p><span class="speaker speaker-engineer">Engineer:</span> We don&rsquo;t track that.</p><p><span class="speaker speaker-sysadmin">Sysadmin:</span> Then you don&rsquo;t know if it&rsquo;s working.</p></div><aside class=pullquote>"Your monitoring dashboard shows five green lights and one red one. The red one is 'model accuracy: unknown'. That's not a monitoring problem. That's a career problem."</aside><p>Part 2 of the series on running AI systems. This one&rsquo;s about monitoring. Not the useless kind vendors sell you. The kind that actually tells you when things are broken before management finds out from Twitter.</p><h2 id=the-usual-monitoring-is-useless>The Usual Monitoring Is Useless<a href=#the-usual-monitoring-is-useless class=hanchor arialabel=Anchor>#</a></h2><p>Traditional monitoring for traditional systems: Is it up? Is it responding? What&rsquo;s the latency? Memory usage? CPU? Disk I/O?</p><p>All useful. All necessary. All completely insufficient for AI systems.</p><p>Your model can be up, responding in 50ms, using 30% CPU, and still be returning complete garbage. No alerts. No errors. Just confidently wrong answers.</p><p>I&rsquo;ve seen this. Model&rsquo;s &ldquo;working fine&rdquo; according to all the standard metrics. Uptime dashboards showing green. SLA being met. Then someone checks the actual predictions and discovers it&rsquo;s been wrong for three days. Accuracy dropped from 95% to 60%. Nobody noticed because nobody was looking.</p><p>&ldquo;But we have logging!&rdquo;</p><p>Logging what? That requests came in and responses went out? That tells me nothing about whether the responses were correct.</p><h2 id=what-you-actually-need-to-monitor>What You Actually Need to Monitor<a href=#what-you-actually-need-to-monitor class=hanchor arialabel=Anchor>#</a></h2><p><strong>1. Prediction accuracy</strong></p><p>How often is your model right? Not &ldquo;right according to the training set.&rdquo; Right in production. On real data. With real users.</p><p>This is harder than it sounds because you don&rsquo;t always get ground truth immediately. User submits a support ticket, model categorizes it, ticket gets resolved three days later - that&rsquo;s when you know if the categorization was correct. But you need to track it.</p><p>If accuracy drops 5%, I want to know. If it drops 10%, I want to be paged. If it drops 20%, I want the model rolled back automatically.</p><p><strong>2. Input distribution drift</strong></p><p>Your model was trained on data from 2023. It&rsquo;s 2025 now. Is the input data still the same shape?</p><p>Plot your input distributions. Compare them to your training data distributions. If they start diverging, that&rsquo;s a problem. Model&rsquo;s making predictions on data it&rsquo;s never seen before.</p><p>Example: Fraud detection model trained on pre-pandemic transaction patterns. Pandemic happens. Everyone starts buying things online. Transaction patterns change completely. Model&rsquo;s still using old patterns. Accuracy tanks. You don&rsquo;t notice because you&rsquo;re only monitoring uptime.</p><p><strong>3. Output distribution drift</strong></p><p>What&rsquo;s your model predicting? Is it still predicting the same distribution of outputs?</p><p>If your model normally predicts &ldquo;class A&rdquo; 70% of the time and &ldquo;class B&rdquo; 30% of the time, and suddenly it&rsquo;s predicting &ldquo;class A&rdquo; 95% of the time, something changed. Either the input data changed or the model&rsquo;s broken.</p><p>Track this. Alert on it. Investigate when it changes.</p><p><strong>4. Prediction confidence</strong></p><p>Most models return a confidence score. &ldquo;I&rsquo;m 95% sure this is spam.&rdquo; &ldquo;I&rsquo;m 60% sure this transaction is fraudulent.&rdquo;</p><p>Track the distribution of confidence scores. If your model suddenly starts returning low-confidence predictions for everything, that&rsquo;s a sign it&rsquo;s seeing data it doesn&rsquo;t know how to handle.</p><p>Worse: if it&rsquo;s returning high-confidence predictions that are wrong, your model&rsquo;s not just broken, it&rsquo;s confidently broken.</p><p><strong>5. Feature importance drift</strong></p><p>Which features is your model using to make decisions? Is that changing over time?</p><p>If your fraud detection model suddenly starts weighting &ldquo;time of day&rdquo; more heavily than &ldquo;transaction amount,&rdquo; something&rsquo;s wrong. Either the data changed or the model retrained on bad data.</p><p><strong>6. Latency percentiles (not just averages)</strong></p><p>Average latency is 100ms? Great. P99 latency is 10 seconds? Not great.</p><p>AI models have unpredictable latency. Some inputs are fast, some are slow. If your P99 latency starts climbing, you&rsquo;ve got a problem. Maybe certain inputs are causing slowdowns. Maybe your GPU&rsquo;s throttling. Maybe your model&rsquo;s just slow on certain edge cases.</p><p>Track P50, P95, P99. Alert when they drift outside acceptable ranges.</p><h2 id=the-hard-part-getting-ground-truth>The Hard Part: Getting Ground Truth<a href=#the-hard-part-getting-ground-truth class=hanchor arialabel=Anchor>#</a></h2><p>Monitoring accuracy requires knowing what the right answer is. For some systems, you get this easily:</p><ul><li>Spam filter: User marks email as spam or not spam</li><li>Fraud detection: Transaction gets investigated, confirmed fraud or not</li><li>Search ranking: User clicks on result or doesn&rsquo;t</li></ul><p>For other systems, you never get ground truth:</p><ul><li>Credit scoring: You approved the loan. Did they pay it back? You won&rsquo;t know for years.</li><li>Medical diagnosis: Model suggests a diagnosis. Doctor overrides it. Was the model right and the doctor wrong? Or vice versa?</li></ul><p>When you can&rsquo;t get ground truth, you monitor proxy metrics:</p><ul><li>User satisfaction scores</li><li>Human override rates (if humans are overriding the model 50% of the time, something&rsquo;s wrong)</li><li>Complaint rates</li><li>Business metrics (revenue, conversion rates, churn)</li></ul><p>Not perfect, but better than nothing.</p><h2 id=alerting-that-doesnt-suck>Alerting That Doesn&rsquo;t Suck<a href=#alerting-that-doesnt-suck class=hanchor arialabel=Anchor>#</a></h2><p>Standard monitoring alerts: &ldquo;Service is down.&rdquo; &ldquo;Latency above threshold.&rdquo; &ldquo;Memory usage critical.&rdquo;</p><p>AI monitoring alerts need context:</p><p><strong>Bad alert:</strong> &ldquo;Model accuracy below 90%&rdquo;</p><p><strong>Good alert:</strong> &ldquo;Model accuracy dropped from 94% to 87% in the last hour. Input distribution shifted. Rollback recommended.&rdquo;</p><p>Include trend data. Include comparison to baseline. Include actionable information.</p><p>And for the love of all that&rsquo;s holy, don&rsquo;t alert on every tiny fluctuation. Set thresholds that matter. If accuracy dropping from 95.2% to 95.1% doesn&rsquo;t require action, don&rsquo;t page me about it.</p><h2 id=tools-that-dont-hate-you>Tools That Don&rsquo;t Hate You<a href=#tools-that-dont-hate-you class=hanchor arialabel=Anchor>#</a></h2><p>You don&rsquo;t need fancy AI-specific monitoring platforms. You need:</p><p><strong>Prometheus/Grafana (or equivalent):</strong> Track metrics over time. Plot distributions. Alert on thresholds.</p><p><strong>Data pipeline that logs predictions and outcomes:</strong> Store your predictions. Store your ground truth when you get it. Join them later for accuracy calculations.</p><p><strong>Dashboards that show trends:</strong> Not just &ldquo;current accuracy: 92%.&rdquo; Show me a graph of accuracy over the last 30 days. Show me input distribution drift. Show me feature importance.</p><p><strong>Automated reports:</strong> Daily/weekly summary of model performance. Accuracy trends, distribution drift, latency percentiles. Something I can skim in 2 minutes to confirm nothing&rsquo;s on fire.</p><h3 id=make-it-real-a-minimal-metrics-set>Make It Real: A Minimal Metrics Set<a href=#make-it-real-a-minimal-metrics-set class=hanchor arialabel=Anchor>#</a></h3><p>You can&rsquo;t alert on &ldquo;model drift&rdquo; if you never emit any model metrics.</p><p>At minimum, your inference service should expose:</p><ul><li><code>model_requests_total{model, endpoint, status}</code></li><li><code>model_latency_seconds_bucket{model, endpoint}</code> (histogram)</li><li><code>model_predictions_total{model, label}</code> (counter)</li><li><code>model_confidence_bucket{model}</code> (histogram, 0-1)</li></ul><p>If you have delayed ground truth, add:</p><ul><li><code>model_labels_total{model, label}</code> (counter - the truth when it arrives)</li></ul><p>Then you can compute an accuracy proxy over a window when the labels land.</p><h3 id=promql-you-can-actually-use>PromQL You Can Actually Use<a href=#promql-you-can-actually-use class=hanchor arialabel=Anchor>#</a></h3><p>Latency (p95):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span><span style=color:#66d9ef>histogram_quantile</span><span style=color:#f92672>(</span>
</span></span><span style=display:flex><span>  <span style=color:#ae81ff>0.95</span>,
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>sum</span> <span style=color:#66d9ef>by</span> <span style=color:#f92672>(</span>le, model<span style=color:#f92672>)</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>model_latency_seconds_bucket[<span style=color:#e6db74>10m</span>]<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Output distribution shift (label share changes):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span><span style=color:#66d9ef>sum</span> <span style=color:#66d9ef>by</span> <span style=color:#f92672>(</span>label<span style=color:#f92672>)</span> <span style=color:#f92672>(</span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>model_predictions_total{model<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>fraud-v3</span>&#34;}[<span style=color:#e6db74>15m</span>]<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>/</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>sum</span><span style=color:#f92672>(</span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>model_predictions_total{model<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>fraud-v3</span>&#34;}[<span style=color:#e6db74>15m</span>]<span style=color:#f92672>))</span>
</span></span></code></pre></div><p>Confidence collapse (too many low-confidence predictions):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span><span style=color:#66d9ef>sum</span><span style=color:#f92672>(</span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>model_confidence_bucket{model<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>fraud-v3</span>&#34;,le<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>0.3</span>&#34;}[<span style=color:#e6db74>15m</span>]<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>/</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>sum</span><span style=color:#f92672>(</span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>model_confidence_bucket{model<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>fraud-v3</span>&#34;,le<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>1</span>&#34;}[<span style=color:#e6db74>15m</span>]<span style=color:#f92672>))</span>
</span></span></code></pre></div><p>If you can join predictions to ground truth and emit both counters, you can do a rough &ldquo;eventual accuracy&rdquo; for class labels:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span><span style=color:#66d9ef>sum</span><span style=color:#f92672>(</span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>model_correct_total{model<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>fraud-v3</span>&#34;}[<span style=color:#e6db74>1h</span>]<span style=color:#f92672>))</span>
</span></span><span style=display:flex><span><span style=color:#f92672>/</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>sum</span><span style=color:#f92672>(</span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>model_labelled_total{model<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>fraud-v3</span>&#34;}[<span style=color:#e6db74>1h</span>]<span style=color:#f92672>))</span>
</span></span></code></pre></div><p>That last one requires your pipeline to increment <code>model_labelled_total</code> when you receive ground truth, and <code>model_correct_total</code> when it matches the original prediction.</p><p>If you&rsquo;re not doing that, your &ldquo;accuracy&rdquo; graph is a lie.</p><h2 id=what-actually-happens>What Actually Happens<a href=#what-actually-happens class=hanchor arialabel=Anchor>#</a></h2><p>Theory: You set up thorough monitoring. You track accuracy, drift, latency. You get alerts when things degrade. You catch problems early.</p><p>Reality: You set up monitoring. Management complains it&rsquo;s too expensive to log all predictions. You compromise, log 10% sample. Drift happens in the other 90%. You don&rsquo;t catch it. Model&rsquo;s broken for a week before someone notices.</p><p>Or: You set up alerts. They fire constantly because the thresholds are wrong. You tune the thresholds. Now they never fire. Model breaks. No alert. You find out from an angry customer email.</p><p>Or: You track everything. Build beautiful dashboards. Nobody looks at them. Model&rsquo;s accuracy drops 15%. Dashboard shows it clearly. Nobody notices until revenue drops and management demands to know why.</p><p>This is why you need:</p><ol><li><p><strong>Automated checks</strong>: If accuracy drops below threshold, automatically roll back. Don&rsquo;t wait for a human to notice.</p></li><li><p><strong>Regular reviews</strong>: Weekly review of model metrics. Bake it into your ops routine. Don&rsquo;t rely on dashboards that nobody looks at.</p></li><li><p><strong>Business metric correlation</strong>: Tie your model metrics to business metrics. When model accuracy drops, does revenue drop? Does customer satisfaction drop? Make the business case for fixing it.</p></li></ol><h2 id=part-3-preview>Part 3 Preview<a href=#part-3-preview class=hanchor arialabel=Anchor>#</a></h2><p>Next up: incident response for AI systems. What to do when your model goes sideways at 3am and management wants answers you don&rsquo;t have.</p><p>Spoiler: It&rsquo;s worse than normal ops because &ldquo;I don&rsquo;t know why it&rsquo;s wrong&rdquo; is not an acceptable answer, but it&rsquo;s often the true answer.</p><hr><p><em>Part 2 of N in a series on running AI systems. <a href=/posts/ai-systems-responsibility-part-1/>Read Part 1: Who Carries the Can?</a></em></p></div></div><div class=related-posts style="margin-top:3rem;padding-top:2rem;border-top:1px solid #333"><h3>Related Posts</h3><ul style=list-style:none;padding:0><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/>AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why</a>
<span style=color:#999;font-size:.9rem>- 19 Dec 2025</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#incident-response</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-1/>AI Systems Responsibility: Part 1 - Who Carries the Can?</a>
<span style=color:#999;font-size:.9rem>- 5 Dec 2025</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#sysadmin</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/>AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode)</a>
<span style=color:#999;font-size:.9rem>- 30 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#tools</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/>AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions</a>
<span style=color:#999;font-size:.9rem>- 23 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#management</span>
<span style=margin-right:.5rem>#culture</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/>AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong)</a>
<span style=color:#999;font-size:.9rem>- 16 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#case-studies</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li></ul></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/ class="button inline prev">&lt; [<span class=button__text>AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why</span>]
</a>::
<a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-1/ class="button inline next">[<span class=button__text>AI Systems Responsibility: Part 1 - Who Carries the Can?</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2026</span></div><div class=build-info><span class=build-commit>e733073</span><span class=build-date>2026-02-12 12:33 UTC</span></div></div></footer></div></body></html>