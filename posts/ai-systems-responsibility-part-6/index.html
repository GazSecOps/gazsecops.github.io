<!doctype html><html lang=en><head><title>AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong) :: Gareth's Engineering Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='This is where I tell you about actual AI deployments that went wrong. Real incidents. Real failures. Real consequences.
Names changed. Details changed enough that you can&rsquo;t identify the companies. But the failures? Those are real.
"Every AI disaster follows the same pattern: someone knew it was broken, someone decided to ship it anyway, and someone else carried the can when it broke in production." Part 6 of the series. Case studies. Learn from other people&rsquo;s mistakes so you don&rsquo;t repeat them.
'><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/><link rel=stylesheet href=https://gazsecops.github.io/css/extended.min.c658c723e006469d82f697e19c5338967fad12c57650bdd915bacf9cfbe2cc38.css><link rel=stylesheet href=https://gazsecops.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://gazsecops.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://gazsecops.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://gazsecops.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://gazsecops.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://gazsecops.github.io/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://gazsecops.github.io/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://gazsecops.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://gazsecops.github.io/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://gazsecops.github.io/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://gazsecops.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://gazsecops.github.io/favicon.png><link rel=apple-touch-icon href=https://gazsecops.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong)"><meta property="og:description" content='This is where I tell you about actual AI deployments that went wrong. Real incidents. Real failures. Real consequences.
Names changed. Details changed enough that you can&rsquo;t identify the companies. But the failures? Those are real.
"Every AI disaster follows the same pattern: someone knew it was broken, someone decided to ship it anyway, and someone else carried the can when it broke in production." Part 6 of the series. Case studies. Learn from other people&rsquo;s mistakes so you don&rsquo;t repeat them.
'><meta property="og:url" content="https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/"><meta property="og:site_name" content="Gareth's Engineering Blog"><meta property="og:image" content="https://gazsecops.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2026-01-16 10:00:00 +0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>gareth@blog:~$</div></a></div><div class=header__search><input type=text id=header-search placeholder="Quick search..." style="padding:5px 10px;background:#222;color:#eee;border:1px solid #444;border-radius:3px;font-size:14px;width:200px"></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></nav></header><script>(function(){const e=document.getElementById("header-search");if(!e)return;e.addEventListener("keydown",function(e){if(e.key==="Enter"){const e=this.value.trim();e&&(window.location.href="/search?q="+encodeURIComponent(e))}}),document.addEventListener("keydown",function(t){t.key==="/"&&!["INPUT","TEXTAREA"].includes(t.target.tagName)&&(t.preventDefault(),e.focus()),t.key==="Escape"&&t.target===e&&e.blur()})})()</script><style>.header__inner{display:flex;align-items:center;gap:20px}.header__search{flex:none}#header-search::placeholder{color:#666;font-style:italic}#header-search:focus{outline:none;border-color:#888}@media(max-width:684px){.header__search{display:none}}</style><div class=content><article class=post><h1 class=post-title><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/>AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong)</a></h1><div class=post-meta><time class=post-date>2026-01-16</time><span class=post-author>Gareth</span><span class=post-reading-time>9 min read (1776 words)</span></div><span class=post-tags>#<a href=https://gazsecops.github.io/tags/ai/>ai</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/case-studies/>case-studies</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/operations/>operations</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/series/>series</a>&nbsp;</span><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#case-study-1-the-medical-triage-system-that-wasnt>Case Study 1: The Medical Triage System That Wasn&rsquo;t</a></li><li><a href=#case-study-2-the-fraud-detector-that-cried-wolf>Case Study 2: The Fraud Detector That Cried Wolf</a></li><li><a href=#case-study-3-the-chatbot-that-went-rogue>Case Study 3: The Chatbot That Went Rogue</a></li><li><a href=#case-study-4-the-model-that-worked-too-well>Case Study 4: The Model That Worked Too Well</a></li><li><a href=#case-study-5-the-model-nobody-could-debug>Case Study 5: The Model Nobody Could Debug</a></li><li><a href=#case-study-6-the-infrastructure-that-couldnt-scale>Case Study 6: The Infrastructure That Couldn&rsquo;t Scale</a></li><li><a href=#common-patterns>Common Patterns</a></li><li><a href=#what-you-can-learn>What You Can Learn</a></li><li><a href=#the-pattern-youll-see>The Pattern You&rsquo;ll See</a></li><li><a href=#part-7-preview>Part 7 Preview</a></li></ul></nav></div><div class=post-content><div><p>This is where I tell you about actual AI deployments that went wrong. Real incidents. Real failures. Real consequences.</p><p>Names changed. Details changed enough that you can&rsquo;t identify the companies. But the failures? Those are real.</p><aside class=pullquote>"Every AI disaster follows the same pattern: someone knew it was broken, someone decided to ship it anyway, and someone else carried the can when it broke in production."</aside><p>Part 6 of the series. Case studies. Learn from other people&rsquo;s mistakes so you don&rsquo;t repeat them.</p><h2 id=case-study-1-the-medical-triage-system-that-wasnt>Case Study 1: The Medical Triage System That Wasn&rsquo;t<a href=#case-study-1-the-medical-triage-system-that-wasnt class=hanchor arialabel=Anchor>#</a></h2><p><strong>What they built:</strong> AI system to triage emergency department patients. Predict severity, recommend priority.</p><p><strong>What they tested:</strong> Accuracy on historical data. 92% accurate. Looked great.</p><p><strong>What they didn&rsquo;t test:</strong> Real-time performance under load. Integration with existing hospital systems. Edge cases.</p><p><strong>What broke:</strong></p><p>Week one in production: System slow. Taking 30 seconds to triage each patient. Emergency department can&rsquo;t wait 30 seconds. Staff start bypassing it.</p><p>Week two: System starts classifying everything as &ldquo;low priority.&rdquo; Turns out input format from hospital&rsquo;s patient management system changed slightly. Parser broke silently. Model getting garbage inputs, returning garbage outputs.</p><p>Week three: Someone notices a pattern. System consistently rates chest pain in women as lower priority than chest pain in men. Training data was biased. Model learned the bias.</p><p><strong>The aftermath:</strong></p><p>System pulled after three weeks. Back to manual triage. Hospital spent six months rebuilding trust with staff who now think &ldquo;AI doesn&rsquo;t work.&rdquo;</p><p><strong>What they should have done:</strong></p><ul><li>Load testing before deployment</li><li>Integration testing with actual hospital systems</li><li>Bias testing across demographics</li><li>Monitoring for accuracy in production</li><li>Human oversight for all predictions</li></ul><p><strong>What they actually did:</strong></p><p>Tested on clean historical data. Shipped when it looked good. Hoped for the best.</p><p><strong>The monitoring that would have caught this:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span>model_prediction_latency_seconds{quantile<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>0.95</span>&#34;} <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>5</span>
</span></span><span style=display:flex><span>model_accuracy_ratio <span style=color:#66d9ef>by</span> <span style=color:#f92672>(</span>gender<span style=color:#f92672>)</span> <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.85</span>
</span></span></code></pre></div><p>Alert on latency spikes and accuracy broken down by demographics.</p><h2 id=case-study-2-the-fraud-detector-that-cried-wolf>Case Study 2: The Fraud Detector That Cried Wolf<a href=#case-study-2-the-fraud-detector-that-cried-wolf class=hanchor arialabel=Anchor>#</a></h2><p><strong>What they built:</strong> Real-time fraud detection for credit card transactions. Flag suspicious transactions, decline automatically.</p><p><strong>What they tested:</strong> Accuracy on historical fraud data. 97% accuracy. False positive rate: 2%.</p><p><strong>What they didn&rsquo;t test:</strong> What happens when fraud patterns change. What happens when false positives hit innocent customers.</p><p><strong>What broke:</strong></p><p>First month: Working fine. Catching fraud. Happy management.</p><p>Second month: New fraud pattern emerges. Fraudsters figured out the model. Start bypassing it. Fraud rate increases but model doesn&rsquo;t catch it.</p><p>Same time: False positives hitting loyal customers. Legitimate transactions declined. Customers angry. Call center overwhelmed.</p><p>Third month: Someone runs the numbers. Model&rsquo;s catching 60% of fraud (down from 95%). False positive rate up to 8% (up from 2%). But nobody noticed because they were only monitoring &ldquo;system uptime.&rdquo;</p><p><strong>The aftermath:</strong></p><p>Model retrained. But damage done. Lost customers. Lost revenue. Call center still dealing with angry customers months later.</p><p><strong>What they should have done:</strong></p><ul><li>Monitor actual fraud catch rate in production, not just uptime</li><li>Monitor false positive rate</li><li>Alert when fraud patterns change</li><li>Have human review for high-value transactions</li><li>A/B test before full rollout</li></ul><p><strong>What they actually did:</strong></p><p>Deployed based on historical accuracy. Assumed it would stay accurate. Didn&rsquo;t monitor what mattered.</p><p><strong>The monitoring that would have caught this:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span>fraud_detection_true_positive_rate <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.80</span>
</span></span><span style=display:flex><span>fraud_detection_false_positive_rate <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.05</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>fraud_detection_total{decision<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>block</span>&#34;}[<span style=color:#e6db74>1h</span>]<span style=color:#f92672>)</span> <span style=color:#f92672>/</span> <span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>fraud_detection_total[<span style=color:#e6db74>1h</span>]<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Track actual fraud catch rate (when ground truth becomes available) and false positive rate.</p><h2 id=case-study-3-the-chatbot-that-went-rogue>Case Study 3: The Chatbot That Went Rogue<a href=#case-study-3-the-chatbot-that-went-rogue class=hanchor arialabel=Anchor>#</a></h2><p><strong>What they built:</strong> Customer service chatbot. Handle common queries, escalate complex ones to humans.</p><p><strong>What they tested:</strong> Response quality on test queries. Looked reasonable.</p><p><strong>What they didn&rsquo;t test:</strong> Adversarial inputs. Edge cases. What happens when someone tries to break it.</p><p><strong>What broke:</strong></p><p>Week one: Working fine for most queries.</p><p>Week two: Someone discovers the chatbot will answer questions about anything if you phrase them correctly. Not just company products. Anything.</p><p>Week three: Chatbot starts giving medical advice. Legal advice. Financial advice. All completely wrong but confidently stated.</p><p>Week four: Chatbot tells a customer to &ldquo;sue the company&rdquo; when asked about a complaint. Screenshot goes viral on Twitter.</p><p><strong>The aftermath:</strong></p><p>Chatbot pulled immediately. PR nightmare. &ldquo;Company&rsquo;s AI tells customers to sue them&rdquo; in the news.</p><p><strong>What they should have done:</strong></p><ul><li>Strict input validation</li><li>Response filtering</li><li>Testing with adversarial inputs</li><li>Clear boundaries on what topics the bot can discuss</li><li>Human review of escalated conversations</li></ul><p><strong>What they actually did:</strong></p><p>Deployed based on happy-path testing. Didn&rsquo;t think about what could go wrong.</p><p><strong>The monitoring that would have caught this:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span>chatbot_response_contains_blocked_topic_total <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>chatbot_input_length_bytes <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>chatbot_conversation_total{escalated<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>true</span>&#34;}[<span style=color:#e6db74>1h</span>]<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Log and alert on blocked topics, unusually long inputs, and escalation rates.</p><h2 id=case-study-4-the-model-that-worked-too-well>Case Study 4: The Model That Worked Too Well<a href=#case-study-4-the-model-that-worked-too-well class=hanchor arialabel=Anchor>#</a></h2><p><strong>What they built:</strong> Content recommendation system. Suggest articles users might like based on reading history.</p><p><strong>What they tested:</strong> Click-through rate. Users clicking on recommendations. Metric going up. Success!</p><p><strong>What they didn&rsquo;t test:</strong> Long-term effects. What content was being recommended. Whether optimization for clicks was good for users.</p><p><strong>What broke:</strong></p><p>First six months: Click-through rate up 40%. Management delighted. Bonuses all round.</p><p>Next six months: User complaints increasing. &ldquo;Why am I only seeing political content?&rdquo; &ldquo;Why is everything so negative?&rdquo; &ldquo;Why do I keep seeing the same topics?&rdquo;</p><p>Investigation reveals: Model optimized for clicks. Controversial content gets clicks. Negative content gets clicks. Model started recommending increasingly extreme content to maximize engagement.</p><p>User satisfaction down. Time on site down. Subscription cancellations up.</p><p><strong>The aftermath:</strong></p><p>Model retrained with different objectives. But echo chamber already created. Users locked into content bubbles. Hard to undo.</p><p><strong>What they should have done:</strong></p><ul><li>Monitor user satisfaction, not just clicks</li><li>Monitor content diversity in recommendations</li><li>Test for filter bubble effects</li><li>Human oversight of recommendation patterns</li><li>Think about second-order effects</li></ul><p><strong>What they actually did:</strong></p><p>Optimized for one metric. Ignored everything else.</p><p><strong>The monitoring that would have caught this:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span><span style=color:#66d9ef>avg</span><span style=color:#f92672>(</span>user_satisfaction_score<span style=color:#f92672>)</span> <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>3.5</span>
</span></span><span style=display:flex><span>recommendation_content_diversity_ratio <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.3</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>user_churn_total[<span style=color:#e6db74>7d</span>]<span style=color:#f92672>)</span> <span style=color:#f92672>/</span> <span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>user_signup_total[<span style=color:#e6db74>7d</span>]<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Track user satisfaction, content diversity, and churn alongside engagement.</p><h2 id=case-study-5-the-model-nobody-could-debug>Case Study 5: The Model Nobody Could Debug<a href=#case-study-5-the-model-nobody-could-debug class=hanchor arialabel=Anchor>#</a></h2><p><strong>What they built:</strong> Loan approval model. Predict likelihood of repayment, approve or deny automatically.</p><p><strong>What they tested:</strong> Accuracy on historical data. 89% accurate. Better than manual process.</p><p><strong>What they didn&rsquo;t test:</strong> Explainability. Fairness. What happens when someone asks &ldquo;why was I denied?&rdquo;</p><p><strong>What broke:</strong></p><p>First few months: Working fine. Approving loans. Denying loans. No problems.</p><p>Then: Customer denied. Asks why. &ldquo;The algorithm decided.&rdquo; Customer complains. Escalates. Threatens legal action.</p><p>Team tries to figure out why customer was denied. Model is a neural network. 47 layers. Nobody can explain the decision. Not even the data scientists.</p><p>Regulator gets involved. &ldquo;You need to explain your lending decisions.&rdquo; Team can&rsquo;t. Model is a black box.</p><p><strong>The aftermath:</strong></p><p>Model pulled. Back to manual review for sensitive cases. Spent a year rebuilding with explainable models.</p><p><strong>What they should have done:</strong></p><ul><li>Use explainable models for high-stakes decisions</li><li>Document decision factors</li><li>Test explanations with humans</li><li>Keep humans in the loop for decisions that need explanation</li></ul><p><strong>What they actually did:</strong></p><p>Used the most accurate model without thinking about explainability.</p><p><strong>The monitoring that would have caught this:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span>loan_decision_explainability_score <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>loan_decision_time_seconds{requires_explanation<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>true</span>&#34;} <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>30</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>loan_complaints_total{reason<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>unexplained_decision</span>&#34;}[<span style=color:#e6db74>7d</span>]<span style=color:#f92672>)</span>
</span></span></code></pre></div><p>Track when explanations fail and correlate with complaints.</p><h2 id=case-study-6-the-infrastructure-that-couldnt-scale>Case Study 6: The Infrastructure That Couldn&rsquo;t Scale<a href=#case-study-6-the-infrastructure-that-couldnt-scale class=hanchor arialabel=Anchor>#</a></h2><p><strong>What they built:</strong> Image recognition system. Classify product images automatically.</p><p><strong>What they tested:</strong> Accuracy on test images. Speed on single-image processing.</p><p><strong>What they didn&rsquo;t test:</strong> Performance at scale. What happens when thousands of images hit simultaneously.</p><p><strong>What broke:</strong></p><p>Launch day: Marketing campaign goes live. Traffic spikes. Image uploads flood in.</p><p>Model server: 8 requests per second. Image queue: 2000 images waiting. Processing time: 4 hours to clear queue.</p><p>Users: Uploading images. Waiting. Nothing happening. Assuming it&rsquo;s broken. Uploading again. Making it worse.</p><p>System: Falling over completely. GPU out of memory. Requests timing out. Everything broken.</p><p><strong>The aftermath:</strong></p><p>Emergency scale-up. More servers. More GPUs. Cost 10x what was budgeted. System eventually stable but expensive.</p><p><strong>What they should have done:</strong></p><ul><li>Load testing before launch</li><li>Auto-scaling infrastructure</li><li>Queue management and backpressure</li><li>User communication when system is slow</li></ul><p><strong>What they actually did:</strong></p><p>Tested single-image performance. Assumed it would scale. It didn&rsquo;t.</p><p><strong>The monitoring that would have caught this:</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-promql data-lang=promql><span style=display:flex><span>image_queue_depth <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>image_processing_latency_seconds{quantile<span style=color:#f92672>=</span>&#34;<span style=color:#e6db74>0.99</span>&#34;} <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>image_processing_errors_total[<span style=color:#e6db74>5m</span>]<span style=color:#f92672>)</span> <span style=color:#f92672>/</span> <span style=color:#66d9ef>rate</span><span style=color:#f92672>(</span>image_processing_total[<span style=color:#e6db74>5m</span>]<span style=color:#f92672>)</span> <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.05</span>
</span></span><span style=display:flex><span>gpu_memory_used_bytes <span style=color:#f92672>/</span> gpu_memory_total_bytes <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.9</span>
</span></span></code></pre></div><p>Track queue depth, latency at P99, error rates, and GPU memory before launch.</p><h2 id=common-patterns>Common Patterns<a href=#common-patterns class=hanchor arialabel=Anchor>#</a></h2><p>Every case study has the same themes:</p><p><strong>1. Testing what&rsquo;s easy, not what matters</strong></p><p>Accuracy on test data is easy to measure. Production performance is hard. Guess which one gets tested.</p><p><strong>2. Monitoring the wrong things</strong></p><p>Uptime is easy to monitor. Accuracy, bias, user satisfaction are hard. Guess which one gets monitored.</p><p><strong>3. Optimizing for the wrong metrics</strong></p><p>Clicks are easy to measure. User satisfaction is hard. Guess which one gets optimized.</p><p><strong>4. Shipping before it&rsquo;s ready</strong></p><p>Deadline is Friday. Model isn&rsquo;t ready. Ship anyway. Deal with consequences later.</p><p><strong>5. Ignoring known problems</strong></p><p>&ldquo;We&rsquo;ll fix that in v2.&rdquo; Narrator: They didn&rsquo;t.</p><p><strong>6. No rollback plan</strong></p><p>&ldquo;We&rsquo;ll figure it out if we need to.&rdquo; Narrator: At 3am, under pressure, they couldn&rsquo;t.</p><h2 id=what-you-can-learn>What You Can Learn<a href=#what-you-can-learn class=hanchor arialabel=Anchor>#</a></h2><p><strong>From Case Study 1 (Medical Triage):</strong>
Test integration. Test under load. Test for bias. Don&rsquo;t assume clean test data represents production.</p><p><strong>From Case Study 2 (Fraud Detection):</strong>
Monitor what matters, not what&rsquo;s easy. Accuracy degrades over time. Alert on it.</p><p><strong>From Case Study 3 (Chatbot):</strong>
Test adversarial inputs. Set boundaries. Filter outputs. Don&rsquo;t assume users will play nice.</p><p><strong>From Case Study 4 (Recommendations):</strong>
Optimizing one metric breaks everything else. Monitor second-order effects. Think long-term.</p><p><strong>From Case Study 5 (Loan Approval):</strong>
High-stakes decisions need explainability. Black boxes are legal liability. Use interpretable models.</p><p><strong>From Case Study 6 (Image Recognition):</strong>
Test at scale. Plan for scale. Budget for scale. Single-image performance doesn&rsquo;t predict production performance.</p><h2 id=the-pattern-youll-see>The Pattern You&rsquo;ll See<a href=#the-pattern-youll-see class=hanchor arialabel=Anchor>#</a></h2><p>Someone builds a model. Tests it on clean data. Gets good accuracy. Ships it.</p><p>Production is messy. Data is messy. Users are messy. Model breaks.</p><p>Incident happens. Rollback happens. Post-mortem happens. &ldquo;We need better testing.&rdquo;</p><p>Everyone agrees. Then the next deadline hits and the cycle repeats.</p><p>The only way to break the cycle: Actually do the testing. Actually set up monitoring. Actually have a rollback plan. Actually say no when it&rsquo;s not ready.</p><p>Will that slow you down? Yes.</p><p>Will that save you from 3am incidents? Also yes.</p><h2 id=part-7-preview>Part 7 Preview<a href=#part-7-preview class=hanchor arialabel=Anchor>#</a></h2><p>That&rsquo;s the end of the core series. But there&rsquo;s one more topic: the organizational side. Why do smart people keep making the same mistakes? Why does testing get skipped? Why do known problems get shipped?</p><p>Next time: The organizational dysfunction that makes AI incidents inevitable.</p><hr><p><em>Part 6 of N in a series on running AI systems. <a href=/posts/ai-systems-responsibility-part-1/>Part 1: Who Carries the Can?</a> | <a href=/posts/ai-systems-responsibility-part-2/>Part 2: Monitoring</a> | <a href=/posts/ai-systems-responsibility-part-3/>Part 3: Incident Response</a> | <a href=/posts/ai-systems-responsibility-part-4/>Part 4: Testing</a> | <a href=/posts/ai-systems-responsibility-part-5/>Part 5: When to Say No</a></em></p></div></div><div class=related-posts style="margin-top:3rem;padding-top:2rem;border-top:1px solid #333"><h3>Related Posts</h3><ul style=list-style:none;padding:0><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/>AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode)</a>
<span style=color:#999;font-size:.9rem>- 30 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#tools</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/>AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions</a>
<span style=color:#999;font-size:.9rem>- 23 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#management</span>
<span style=margin-right:.5rem>#culture</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-5/>AI Systems Responsibility: Part 5 - When to Say No (And How to Make It Stick)</a>
<span style=color:#999;font-size:.9rem>- 9 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#management</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-4/>AI Systems Responsibility: Part 4 - Testing Before You Ship (Or: Stop Discovering Bugs in Production)</a>
<span style=color:#999;font-size:.9rem>- 2 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#testing</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/>AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why</a>
<span style=color:#999;font-size:.9rem>- 19 Dec 2025</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#incident-response</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li></ul></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/ class="button inline prev">&lt; [<span class=button__text>AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions</span>]
</a>::
<a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-5/ class="button inline next">[<span class=button__text>AI Systems Responsibility: Part 5 - When to Say No (And How to Make It Stick)</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2026</span></div><div class=build-info><span class=build-commit>e733073</span><span class=build-date>2026-02-12 12:33 UTC</span></div></div></footer></div></body></html>