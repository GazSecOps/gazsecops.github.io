<!doctype html><html lang=en><head><title>AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode) :: Gareth's Engineering Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='Part 7 was meant to be the end. &ldquo;Good luck, you&rsquo;ll need it.&rdquo; Fin.
Then someone asked: &ldquo;Alright, but what tools actually work? Not vendor pitches. Not &lsquo;AI observability platforms&rsquo; that cost more than my salary. Actual tools that help run AI systems without making everything worse.&rdquo;
Fair question.
"The best AI ops tools are the boring ones. Prometheus. Grafana. Python scripts. Git. The same tools you&#39;d use for normal systems, just pointed at different metrics." Bonus episode. Practical tools and techniques that actually help when you&rsquo;re running AI systems. Things I&rsquo;ve used. Things that work.
'><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/><link rel=stylesheet href=https://gazsecops.github.io/css/extended.min.c658c723e006469d82f697e19c5338967fad12c57650bdd915bacf9cfbe2cc38.css><link rel=stylesheet href=https://gazsecops.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://gazsecops.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://gazsecops.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://gazsecops.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://gazsecops.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://gazsecops.github.io/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://gazsecops.github.io/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://gazsecops.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://gazsecops.github.io/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://gazsecops.github.io/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://gazsecops.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://gazsecops.github.io/favicon.png><link rel=apple-touch-icon href=https://gazsecops.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode)"><meta property="og:description" content='Part 7 was meant to be the end. &ldquo;Good luck, you&rsquo;ll need it.&rdquo; Fin.
Then someone asked: &ldquo;Alright, but what tools actually work? Not vendor pitches. Not &lsquo;AI observability platforms&rsquo; that cost more than my salary. Actual tools that help run AI systems without making everything worse.&rdquo;
Fair question.
"The best AI ops tools are the boring ones. Prometheus. Grafana. Python scripts. Git. The same tools you&#39;d use for normal systems, just pointed at different metrics." Bonus episode. Practical tools and techniques that actually help when you&rsquo;re running AI systems. Things I&rsquo;ve used. Things that work.
'><meta property="og:url" content="https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/"><meta property="og:site_name" content="Gareth's Engineering Blog"><meta property="og:image" content="https://gazsecops.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2026-01-30 10:00:00 +0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>gareth@blog:~$</div></a></div><div class=header__search><input type=text id=header-search placeholder="Quick search..." style="padding:5px 10px;background:#222;color:#eee;border:1px solid #444;border-radius:3px;font-size:14px;width:200px"></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;â–¾</li><li><ul class=menu__dropdown><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></nav></header><script>(function(){const e=document.getElementById("header-search");if(!e)return;e.addEventListener("keydown",function(e){if(e.key==="Enter"){const e=this.value.trim();e&&(window.location.href="/search?q="+encodeURIComponent(e))}}),document.addEventListener("keydown",function(t){t.key==="/"&&!["INPUT","TEXTAREA"].includes(t.target.tagName)&&(t.preventDefault(),e.focus()),t.key==="Escape"&&t.target===e&&e.blur()})})()</script><style>.header__inner{display:flex;align-items:center;gap:20px}.header__search{flex:none}#header-search::placeholder{color:#666;font-style:italic}#header-search:focus{outline:none;border-color:#888}@media(max-width:684px){.header__search{display:none}}</style><div class=content><article class=post><h1 class=post-title><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/>AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode)</a></h1><div class=post-meta><time class=post-date>2026-01-30</time><span class=post-author>Gareth</span><span class=post-reading-time>9 min read (1815 words)</span></div><span class=post-tags>#<a href=https://gazsecops.github.io/tags/ai/>ai</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/tools/>tools</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/operations/>operations</a>&nbsp;
#<a href=https://gazsecops.github.io/tags/series/>series</a>&nbsp;</span><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#the-monitoring-stack>The Monitoring Stack</a></li><li><a href=#the-logging-stack>The Logging Stack</a></li><li><a href=#the-data-pipeline>The Data Pipeline</a></li><li><a href=#the-experiment-tracking>The Experiment Tracking</a></li><li><a href=#the-testing-framework>The Testing Framework</a></li><li><a href=#the-deployment-tools>The Deployment Tools</a></li><li><a href=#the-incident-response-tools>The Incident Response Tools</a></li><li><a href=#the-things-you-dont-need>The Things You Don&rsquo;t Need</a></li><li><a href=#the-actual-stack-id-use>The Actual Stack I&rsquo;d Use</a></li><li><a href=#the-pattern>The Pattern</a></li><li><a href=#what-about-the-fancy-stuff>What About The Fancy Stuff?</a></li><li><a href=#the-real-tool-that-helps>The Real Tool That Helps</a></li><li><a href=#part-9>Part 9?</a></li></ul></nav></div><div class=post-content><div><p>Part 7 was meant to be the end. &ldquo;Good luck, you&rsquo;ll need it.&rdquo; Fin.</p><p>Then someone asked: &ldquo;Alright, but what tools actually work? Not vendor pitches. Not &lsquo;AI observability platforms&rsquo; that cost more than my salary. Actual tools that help run AI systems without making everything worse.&rdquo;</p><p>Fair question.</p><aside class=pullquote>"The best AI ops tools are the boring ones. Prometheus. Grafana. Python scripts. Git. The same tools you'd use for normal systems, just pointed at different metrics."</aside><p>Bonus episode. Practical tools and techniques that actually help when you&rsquo;re running AI systems. Things I&rsquo;ve used. Things that work.</p><h2 id=the-monitoring-stack>The Monitoring Stack<a href=#the-monitoring-stack class=hanchor arialabel=Anchor>#</a></h2><p><strong>What you need:</strong> Track accuracy, latency, input/output distributions, drift.</p><p><strong>What actually works:</strong></p><p><strong>Prometheus + Grafana</strong></p><p>Not exciting. Not AI-specific. Works.</p><p>Export metrics from your model service:</p><ul><li>Prediction count (by class, by confidence bucket)</li><li>Latency (P50, P95, P99)</li><li>Input feature distributions</li><li>Accuracy (when you have ground truth)</li><li>Model version being served</li></ul><p>Scrape with Prometheus. Visualize in Grafana. Alert on thresholds.</p><p>Cost: Free. Learning curve: Moderate. Reliability: High.</p><p><strong>Why it works:</strong> Because it&rsquo;s the same stack you use for everything else. You already know it. Your team already knows it. No new tools to learn. No vendor lock-in.</p><p><strong>InfluxDB + Telegraf</strong> (if you prefer time-series databases)</p><p>Same idea. Different implementation. Also works fine.</p><p><strong>CloudWatch/Azure Monitor/GCP Monitoring</strong> (if you&rsquo;re cloud-native)</p><p>Costs money. Vendor lock-in. But if you&rsquo;re already using AWS/Azure/GCP, might as well use their monitoring. Less to manage.</p><h2 id=the-logging-stack>The Logging Stack<a href=#the-logging-stack class=hanchor arialabel=Anchor>#</a></h2><p><strong>What you need:</strong> Log predictions, inputs, outcomes. Join them later for accuracy calculations.</p><p><strong>What actually works:</strong></p><p><strong>ELK Stack (Elasticsearch, Logstash, Kibana)</strong></p><p>Old. Boring. Works.</p><p>Log every prediction:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;timestamp&#34;</span>: <span style=color:#e6db74>&#34;2026-01-30T10:15:23Z&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;model_version&#34;</span>: <span style=color:#e6db74>&#34;v2.3.1&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;prediction&#34;</span>: <span style=color:#e6db74>&#34;spam&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;confidence&#34;</span>: <span style=color:#ae81ff>0.87</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;input_hash&#34;</span>: <span style=color:#e6db74>&#34;abc123&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;request_id&#34;</span>: <span style=color:#e6db74>&#34;req_xyz&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Log ground truth when you get it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;timestamp&#34;</span>: <span style=color:#e6db74>&#34;2026-01-30T10:20:15Z&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;request_id&#34;</span>: <span style=color:#e6db74>&#34;req_xyz&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;actual&#34;</span>: <span style=color:#e6db74>&#34;not_spam&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Join on <code>request_id</code>. Calculate accuracy. Alert when it drops.</p><p>Cost: Free (self-hosted) or moderate (managed). Learning curve: Moderate. Reliability: High.</p><p><strong>Loki + Grafana</strong> (if you want something lighter)</p><p>Less heavy than Elasticsearch. Integrates with Grafana. Good enough for most use cases.</p><p><strong>Just write to S3/Cloud Storage and process later</strong></p><p>Cheapest option. Log everything to cloud storage. Run batch jobs to calculate metrics. Works if you don&rsquo;t need real-time.</p><h2 id=the-data-pipeline>The Data Pipeline<a href=#the-data-pipeline class=hanchor arialabel=Anchor>#</a></h2><p><strong>What you need:</strong> Get data from models, transform it, store it, analyze it.</p><p><strong>What actually works:</strong></p><p><strong>Python scripts + cron</strong></p><p>I&rsquo;m serious. For most use cases, you don&rsquo;t need Airflow or Kafka or Spark. You need a Python script that runs every hour.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># check_model_accuracy.py</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run every hour via cron</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>predictions <span style=color:#f92672>=</span> load_predictions_from_last_hour()
</span></span><span style=display:flex><span>ground_truth <span style=color:#f92672>=</span> load_ground_truth_from_last_hour()
</span></span><span style=display:flex><span>accuracy <span style=color:#f92672>=</span> calculate_accuracy(predictions, ground_truth)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> accuracy <span style=color:#f92672>&lt;</span> THRESHOLD:
</span></span><span style=display:flex><span>    alert_on_call()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>log_to_prometheus(accuracy)
</span></span></code></pre></div><p>Cost: Free. Learning curve: Low. Reliability: Good enough.</p><p><strong>Airflow</strong> (if you need orchestration)</p><p>When your cron jobs get too complex, use Airflow. Not before.</p><p><strong>dbt</strong> (for data transformation)</p><p>If you&rsquo;re doing SQL-heavy transformations, dbt is good. Version control for your queries. Tests for data quality.</p><h2 id=the-experiment-tracking>The Experiment Tracking<a href=#the-experiment-tracking class=hanchor arialabel=Anchor>#</a></h2><p><strong>What you need:</strong> Track which model version is in production. Which performed best. What changed between versions.</p><p><strong>What actually works:</strong></p><p><strong>MLflow</strong></p><p>Open source. Does experiment tracking, model registry, deployment. Not perfect, but adequate.</p><p>Log experiments during training. Register models. Deploy to production. Track which version is where.</p><p>Here&rsquo;s the smallest MLflow example that is still useful:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> mlflow
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> mlflow.sklearn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.ensemble <span style=color:#f92672>import</span> RandomForestClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X_train, y_train <span style=color:#f92672>=</span> load_training_data()
</span></span><span style=display:flex><span>X_val, y_val <span style=color:#f92672>=</span> load_validation_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>with</span> mlflow<span style=color:#f92672>.</span>start_run():
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> RandomForestClassifier(
</span></span><span style=display:flex><span>        n_estimators<span style=color:#f92672>=</span><span style=color:#ae81ff>200</span>,
</span></span><span style=display:flex><span>        random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>,
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>fit(X_train, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    val_accuracy <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>score(X_val, y_val)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    mlflow<span style=color:#f92672>.</span>log_param(<span style=color:#e6db74>&#34;n_estimators&#34;</span>, <span style=color:#ae81ff>200</span>)
</span></span><span style=display:flex><span>    mlflow<span style=color:#f92672>.</span>log_metric(<span style=color:#e6db74>&#34;val_accuracy&#34;</span>, val_accuracy)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    mlflow<span style=color:#f92672>.</span>sklearn<span style=color:#f92672>.</span>log_model(
</span></span><span style=display:flex><span>        sk_model<span style=color:#f92672>=</span>model,
</span></span><span style=display:flex><span>        artifact_path<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;model&#34;</span>,
</span></span><span style=display:flex><span>        registered_model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;fraud-model&#34;</span>,
</span></span><span style=display:flex><span>    )
</span></span></code></pre></div><p>Then you have a registry entry called <code>fraud-model</code> with versions. That&rsquo;s what ops needs.</p><p>If you&rsquo;re deploying via the registry, you can serve a specific version:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>mlflow models serve -m <span style=color:#e6db74>&#34;models:/fraud-model/12&#34;</span> -p <span style=color:#ae81ff>5000</span>
</span></span></code></pre></div><p>The useful operational habit is this: every model you deploy has a versioned identity you can point at during an incident.</p><p>Cost: Free. Learning curve: Moderate. Reliability: Decent.</p><p><strong>Git tags + artifact storage</strong></p><p>Simpler approach: Tag your git commits. Store model artifacts in S3/GCS/Azure Blob with version tags. Track deployments in a spreadsheet or database.</p><p>Not fancy. Works.</p><h2 id=the-testing-framework>The Testing Framework<a href=#the-testing-framework class=hanchor arialabel=Anchor>#</a></h2><p><strong>What you need:</strong> Test models before deployment. Test edge cases. Test integration.</p><p><strong>What actually works:</strong></p><p><strong>pytest + your own test data</strong></p><p>Write tests like normal software:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_model_handles_empty_input</span>():
</span></span><span style=display:flex><span>    prediction <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(<span style=color:#e6db74>&#34;&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> prediction <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_model_handles_unicode</span>():
</span></span><span style=display:flex><span>    prediction <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(<span style=color:#e6db74>&#34;Hello ðŸ‘‹ Ã©moji&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> prediction <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>test_model_accuracy_on_test_set</span>():
</span></span><span style=display:flex><span>    accuracy <span style=color:#f92672>=</span> evaluate_model(test_data)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> accuracy <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.90</span>
</span></span></code></pre></div><p>Run before deployment. Fail deployment if tests fail.</p><p>Cost: Free. Learning curve: Low. Reliability: High.</p><p><strong>Great Expectations</strong> (for data validation)</p><p>Test your input data quality. Catches issues before they hit the model.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>expect_column_values_to_be_between(<span style=color:#e6db74>&#34;age&#34;</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>120</span>)
</span></span><span style=display:flex><span>expect_column_values_to_not_be_null(<span style=color:#e6db74>&#34;user_id&#34;</span>)
</span></span></code></pre></div><p>Useful for catching upstream data quality issues.</p><h2 id=the-deployment-tools>The Deployment Tools<a href=#the-deployment-tools class=hanchor arialabel=Anchor>#</a></h2><p><strong>What you need:</strong> Deploy models. Roll back quickly. Route traffic gradually.</p><p><strong>What actually works:</strong></p><p><strong>Docker + Kubernetes</strong></p><p>Everyone uses it. Documentation exists. People know how to debug it. Use it.</p><p>Package model in Docker container. Deploy to Kubernetes. Use rolling deployments. Use health checks. Use readiness probes.</p><p>Cost: Moderate (infrastructure). Learning curve: High. Reliability: High (when configured correctly).</p><p><strong>Blue-green deployments</strong></p><p>Run old version and new version simultaneously. Route a small percentage of traffic to new version. Monitor. Gradually increase. Roll back if it breaks.</p><p>There are two ways people do this.</p><p><strong>The boring way (works):</strong> run two Deployments and switch the Service selector.</p><ul><li><code>model-service-blue</code> (current)</li><li><code>model-service-green</code> (new)</li></ul><p>Service points at one of them via a label:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>model-service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>model-service</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>colour</span>: <span style=color:#ae81ff>blue</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>8080</span>
</span></span></code></pre></div><p>Switching is a single patch:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl patch svc model-service -p <span style=color:#e6db74>&#39;{&#34;spec&#34;:{&#34;selector&#34;:{&#34;app&#34;:&#34;model-service&#34;,&#34;colour&#34;:&#34;green&#34;}}}&#39;</span>
</span></span></code></pre></div><p>If you want traffic weighting and step-by-step rollout, use something that does it on purpose (Argo Rollouts, service mesh, cloud load balancer with weights). The point isn&rsquo;t which tool. The point is: deploy and rollback are both fast and boring.</p><p><strong>Feature flags</strong></p><p>Deploy new model but don&rsquo;t activate it yet. Use feature flag to control which model serves traffic. Can toggle instantly without redeployment.</p><p>LaunchDarkly, Split.io, or roll your own with a config file.</p><p>Rolling your own is fine if you keep it simple.</p><p>Example: a single config value that controls the active model version:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;active_model&#34;</span>: <span style=color:#e6db74>&#34;v2.3.1&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Your service reads it on a timer and switches:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-python data-lang=python><span style=display:flex><span>active <span style=color:#f92672>=</span> load_config()[<span style=color:#e6db74>&#34;active_model&#34;</span>]
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> active <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;v2.3.1&#34;</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> predict_v231(req)
</span></span><span style=display:flex><span><span style=color:#66d9ef>return</span> predict_v230(req)
</span></span></code></pre></div><p>The rule for feature flags in model serving is the same as everywhere else:</p><ul><li>If you can&rsquo;t flip it back instantly, it&rsquo;s not a feature flag. It&rsquo;s a deployment.</li></ul><h2 id=the-incident-response-tools>The Incident Response Tools<a href=#the-incident-response-tools class=hanchor arialabel=Anchor>#</a></h2><p><strong>What you need:</strong> When things break at 3am, what helps you fix it quickly?</p><p><strong>What actually works:</strong></p><p><strong>Runbooks</strong></p><p>Literal documents that say &ldquo;When X happens, do Y.&rdquo;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-fallback data-lang=fallback><span style=display:flex><span>Model accuracy dropped below 85%:
</span></span><span style=display:flex><span>1. Check Grafana dashboard for accuracy trend
</span></span><span style=display:flex><span>2. Check input distribution for drift
</span></span><span style=display:flex><span>3. If drift detected, investigate upstream data pipeline
</span></span><span style=display:flex><span>4. If no drift, check model version deployed
</span></span><span style=display:flex><span>5. If wrong version, rollback using script: ./rollback.sh
</span></span><span style=display:flex><span>6. If correct version, escalate to data science team
</span></span></code></pre></div><p>Store in wiki. Update after every incident. Reference during incidents.</p><p>Cost: Free. Usefulness: High.</p><p><strong>ChatOps</strong></p><p>Run commands from Slack. View dashboards from Slack. Get alerts in Slack.</p><p>When incident happens, everyone&rsquo;s in the same Slack channel. Can see the same data. Can run the same commands.</p><p>PagerDuty, OpsGenie, or roll your own with Slack bots.</p><p><strong>Rollback scripts</strong></p><p>One command to roll back. Tested. Documented. Ready to run at 3am.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#75715e># rollback.sh - Roll back to previous model version</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubectl set image deployment/model-service model<span style=color:#f92672>=</span>model:v2.3.0
</span></span><span style=display:flex><span>kubectl rollout status deployment/model-service
</span></span></code></pre></div><p>Store in git. Test regularly. Don&rsquo;t wait until 3am to discover it doesn&rsquo;t work.</p><h2 id=the-things-you-dont-need>The Things You Don&rsquo;t Need<a href=#the-things-you-dont-need class=hanchor arialabel=Anchor>#</a></h2><p><strong>AI Observability Platforms</strong></p><p>Vendors will sell you &ldquo;AI-specific observability platforms.&rdquo; Cost: Thousands per month.</p><p>What do they do? Track metrics. Log predictions. Alert on drift.</p><p>What can you do instead? Use Prometheus and ELK. Cost: Infrastructure only.</p><p>Do you need AI-specific platforms? Maybe, if you&rsquo;re running hundreds of models at scale. Probably not, if you&rsquo;re running a handful.</p><p><strong>AutoML Platforms</strong></p><p>Vendors will sell you platforms that &ldquo;automatically build and deploy ML models.&rdquo;</p><p>Do they work? Sometimes. Are they worth the cost? Rarely.</p><p>Most of the time, you need custom logic, custom preprocessing, custom evaluation. AutoML doesn&rsquo;t handle that.</p><p><strong>Model Performance Management Tools</strong></p><p>Vendors will sell you tools to &ldquo;manage model performance in production.&rdquo;</p><p>What do they do? Track accuracy. Alert on drift. Suggest retraining.</p><p>What can you do instead? Write Python scripts. Calculate accuracy yourself. Alert yourself.</p><p>Exception: If you&rsquo;re managing dozens of models, a dedicated tool might make sense. For a few models? Overkill.</p><h2 id=the-actual-stack-id-use>The Actual Stack I&rsquo;d Use<a href=#the-actual-stack-id-use class=hanchor arialabel=Anchor>#</a></h2><p>If I were setting up AI ops from scratch, here&rsquo;s what I&rsquo;d use:</p><p><strong>Monitoring:</strong> Prometheus + Grafana
<strong>Logging:</strong> Loki + Grafana (or ELK if I need search)
<strong>Data Pipeline:</strong> Python scripts + cron (upgrade to Airflow when needed)
<strong>Experiment Tracking:</strong> MLflow (or git tags if simple)
<strong>Testing:</strong> pytest + Great Expectations
<strong>Deployment:</strong> Docker + Kubernetes
<strong>Incident Response:</strong> Runbooks in wiki + Slack + rollback scripts</p><p>Total cost: Infrastructure only. No vendor subscriptions. No lock-in. Everything&rsquo;s open source or DIY.</p><p>Learning curve: Moderate. But you&rsquo;d learn these tools anyway for normal ops.</p><h2 id=the-pattern>The Pattern<a href=#the-pattern class=hanchor arialabel=Anchor>#</a></h2><p>Notice the pattern? The tools that work are boring tools you&rsquo;d use for any system. Just pointed at different metrics.</p><p>You don&rsquo;t need AI-specific tools. You need good operational practices applied to AI systems.</p><p>Monitor the right things. Log the right things. Test properly. Deploy carefully. Respond quickly.</p><p>Same as any other system. Just with different failure modes.</p><h2 id=what-about-the-fancy-stuff>What About The Fancy Stuff?<a href=#what-about-the-fancy-stuff class=hanchor arialabel=Anchor>#</a></h2><p>&ldquo;But what about [insert trendy AI ops tool here]?&rdquo;</p><p>Maybe it&rsquo;s good. Maybe it&rsquo;s useful. Maybe it solves real problems.</p><p>But start with the boring tools first. Prometheus. Grafana. Python scripts. Git.</p><p>Get those working. Get good at using them. Then, if you hit a problem the boring tools can&rsquo;t solve, look at the fancy tools.</p><p>Most teams never hit that point. Most teams would benefit more from using Prometheus correctly than from buying an AI observability platform.</p><h2 id=the-real-tool-that-helps>The Real Tool That Helps<a href=#the-real-tool-that-helps class=hanchor arialabel=Anchor>#</a></h2><p>The actual tool that helps the most? Not software.</p><p>It&rsquo;s discipline.</p><p>Discipline to write tests even when deadlines loom. Discipline to monitor even when everything&rsquo;s working. Discipline to document even when you&rsquo;re tired. Discipline to say no even when management pushes.</p><p>No tool will save you if you don&rsquo;t have discipline.</p><p>Boring tools + discipline > Fancy tools + chaos.</p><h2 id=part-9>Part 9?<a href=#part-9 class=hanchor arialabel=Anchor>#</a></h2><p>There won&rsquo;t be a Part 9. This was the bonus episode. The series is done.</p><p>Seven parts on what goes wrong. One bonus part on what actually helps.</p><p>If you&rsquo;ve read all eight parts and you&rsquo;re still running AI systems, you&rsquo;re either brave or foolish. Probably both.</p><p>Good luck. You&rsquo;ll still need it.</p><hr><p><em>Part 8 (Bonus) of a series on running AI systems. <a href=/posts/ai-systems-responsibility-part-1/>Part 1: Who Carries the Can?</a> | <a href=/posts/ai-systems-responsibility-part-2/>Part 2: Monitoring</a> | <a href=/posts/ai-systems-responsibility-part-3/>Part 3: Incident Response</a> | <a href=/posts/ai-systems-responsibility-part-4/>Part 4: Testing</a> | <a href=/posts/ai-systems-responsibility-part-5/>Part 5: When to Say No</a> | <a href=/posts/ai-systems-responsibility-part-6/>Part 6: Case Studies</a> | <a href=/posts/ai-systems-responsibility-part-7/>Part 7: Organizational Dysfunction</a></em></p></div></div><div class=related-posts style="margin-top:3rem;padding-top:2rem;border-top:1px solid #333"><h3>Related Posts</h3><ul style=list-style:none;padding:0><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/>AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions</a>
<span style=color:#999;font-size:.9rem>- 23 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#management</span>
<span style=margin-right:.5rem>#culture</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/>AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong)</a>
<span style=color:#999;font-size:.9rem>- 16 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#case-studies</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-5/>AI Systems Responsibility: Part 5 - When to Say No (And How to Make It Stick)</a>
<span style=color:#999;font-size:.9rem>- 9 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#management</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-4/>AI Systems Responsibility: Part 4 - Testing Before You Ship (Or: Stop Discovering Bugs in Production)</a>
<span style=color:#999;font-size:.9rem>- 2 Jan 2026</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#testing</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li><li style=margin-bottom:1rem><a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/>AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why</a>
<span style=color:#999;font-size:.9rem>- 19 Dec 2025</span><div style=font-size:.85rem;color:#666;margin-top:.25rem><span style=margin-right:.5rem>#ai</span>
<span style=margin-right:.5rem>#incident-response</span>
<span style=margin-right:.5rem>#operations</span>
<span style=margin-right:.5rem>#series</span></div></li></ul></div><div class=pagination><div class=pagination__title><span class=pagination__title-h></span><hr></div><div class=pagination__buttons><a href=https://gazsecops.github.io/posts/oauth2-obo-flow-complete-guide/ class="button inline prev">&lt; [<span class=button__text>OAuth2 On-Behalf-Of Flow: A Complete Guide for Microservices</span>]
</a>::
<a href=https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/ class="button inline next">[<span class=button__text>AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions</span>] ></a></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>Â© 2026</span></div><div class=build-info><span class=build-commit>e733073</span><span class=build-date>2026-02-12 12:33 UTC</span></div></div></footer></div></body></html>