<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Series on Gareth's Engineering Blog</title><link>https://gazsecops.github.io/series/</link><description>Recent content in Series on Gareth's Engineering Blog</description><generator>Hugo</generator><language>en-gb</language><lastBuildDate>Fri, 06 Feb 2026 14:17:49 +0000</lastBuildDate><atom:link href="https://gazsecops.github.io/series/index.xml" rel="self" type="application/rss+xml"/><item><title>AI Systems Responsibility: An 8-Part Series</title><link>https://gazsecops.github.io/series/ai-systems-responsibility/</link><pubDate>Mon, 15 Dec 2025 00:00:00 +0000</pubDate><guid>https://gazsecops.github.io/series/ai-systems-responsibility/</guid><description>&lt;p&gt;When your AI system breaks at 3am, the sysadmin gets the page. Not the data scientist. Not the ML engineer. The person who has to fix it.&lt;/p&gt;
&lt;p&gt;This 8-part series covers what happens when AI systems meet production reality. What breaks. Who gets blamed. What actually works.&lt;/p&gt;
&lt;h2 id="the-series"&gt;The Series&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://gazsecops.github.io/posts/ai-systems-responsibility-part-1/"&gt;Part 1: Silent Failures&lt;/a&gt; - Why AI fails differently than traditional software&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gazsecops.github.io/posts/ai-systems-responsibility-part-2/"&gt;Part 2: Monitoring What Matters&lt;/a&gt; - Beyond uptime checks&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gazsecops.github.io/posts/ai-systems-responsibility-part-3/"&gt;Part 3: Incident Response&lt;/a&gt; - The 4am conversation nobody wants&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gazsecops.github.io/posts/ai-systems-responsibility-part-4/"&gt;Part 4: Testing Requirements&lt;/a&gt; - When standard testing isn&amp;rsquo;t enough&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gazsecops.github.io/posts/ai-systems-responsibility-part-5/"&gt;Part 5: When to Say No&lt;/a&gt; - Building political capital&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gazsecops.github.io/posts/ai-systems-responsibility-part-6/"&gt;Part 6: Case Studies&lt;/a&gt; - Real failures, real consequences&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gazsecops.github.io/posts/ai-systems-responsibility-part-7/"&gt;Part 7: Organizational Dysfunction&lt;/a&gt; - Why smart people make dumb decisions&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gazsecops.github.io/posts/ai-systems-responsibility-part-8/"&gt;Part 8: Practical Tools&lt;/a&gt; - Boring tools that actually work&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="who-this-is-for"&gt;Who This Is For&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Sysadmins who got handed AI systems&lt;/li&gt;
&lt;li&gt;SREs suddenly responsible for ML infrastructure&lt;/li&gt;
&lt;li&gt;Engineers who have to keep the thing running&lt;/li&gt;
&lt;li&gt;Anyone who gets paged when models break&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="what-youll-learn"&gt;What You&amp;rsquo;ll Learn&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;How to monitor AI systems properly&lt;/li&gt;
&lt;li&gt;What to test before shipping&lt;/li&gt;
&lt;li&gt;When to push back (and how)&lt;/li&gt;
&lt;li&gt;Incident response when you can&amp;rsquo;t explain why it broke&lt;/li&gt;
&lt;li&gt;Tools that work (Prometheus, not expensive platforms)&lt;/li&gt;
&lt;li&gt;Political survival in dysfunctional organizations&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="the-core-philosophy"&gt;The Core Philosophy&lt;/h2&gt;
&lt;p&gt;The system is designed to produce bad outcomes. Your job is to minimize the damage. Not prevent it entirely. Minimize it.&lt;/p&gt;</description></item></channel></rss>