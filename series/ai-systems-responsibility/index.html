<!doctype html><html lang=en><head><title>AI Systems Responsibility: An 8-Part Series :: Gareth's Engineering Blog</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="When your AI system breaks at 3am, the sysadmin gets the page. Not the data scientist. Not the ML engineer. The person who has to fix it.
This 8-part series covers what happens when AI systems meet production reality. What breaks. Who gets blamed. What actually works.
The Series Part 1: Silent Failures - Why AI fails differently than traditional software Part 2: Monitoring What Matters - Beyond uptime checks Part 3: Incident Response - The 4am conversation nobody wants Part 4: Testing Requirements - When standard testing isn&rsquo;t enough Part 5: When to Say No - Building political capital Part 6: Case Studies - Real failures, real consequences Part 7: Organizational Dysfunction - Why smart people make dumb decisions Part 8: Practical Tools - Boring tools that actually work Who This Is For Sysadmins who got handed AI systems SREs suddenly responsible for ML infrastructure Engineers who have to keep the thing running Anyone who gets paged when models break What You&rsquo;ll Learn How to monitor AI systems properly What to test before shipping When to push back (and how) Incident response when you can&rsquo;t explain why it broke Tools that work (Prometheus, not expensive platforms) Political survival in dysfunctional organizations The Core Philosophy The system is designed to produce bad outcomes. Your job is to minimize the damage. Not prevent it entirely. Minimize it.
"><meta name=keywords content><meta name=robots content="noodp"><link rel=canonical href=https://gazsecops.github.io/series/ai-systems-responsibility/><link rel=stylesheet href=https://gazsecops.github.io/css/extended.min.c658c723e006469d82f697e19c5338967fad12c57650bdd915bacf9cfbe2cc38.css><link rel=stylesheet href=https://gazsecops.github.io/css/buttons.min.86f6b4c106b6c6eb690ae5203d36b442c1f66f718ff4e8164fa86cf6c61ad641.css><link rel=stylesheet href=https://gazsecops.github.io/css/code.min.d529ea4b2fb8d34328d7d31afc5466d5f7bc2f0bc9abdd98b69385335d7baee4.css><link rel=stylesheet href=https://gazsecops.github.io/css/fonts.min.5bb7ed13e1d00d8ff39ea84af26737007eb5051b157b86fc24487c94f3dc8bbe.css><link rel=stylesheet href=https://gazsecops.github.io/css/footer.min.eb8dfc2c6a7eafa36cd3ba92d63e69e849e2200e0002a228d137f236b09ecd75.css><link rel=stylesheet href=https://gazsecops.github.io/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/header.min.75c7eb0e2872d95ff48109c6647d0223a38db52e2561dd87966eb5fc7c6bdac6.css><link rel=stylesheet href=https://gazsecops.github.io/css/main.min.36833afd348409fc6c3d09d0897c5833d9d5bf1ff31f5e60ea3ee42ce2b1268c.css><link rel=stylesheet href=https://gazsecops.github.io/css/menu.min.3c17467ebeb3d38663dce68f71f519901124fa5cbb4519b2fb0667a21e9aca39.css><link rel=stylesheet href=https://gazsecops.github.io/css/pagination.min.bbb986dbce00a5ce5aca0504b7925fc1c581992a4bf57f163e5d69cc1db7d836.css><link rel=stylesheet href=https://gazsecops.github.io/css/post.min.e6dddd258e64c83e05cec0cd49c05216742d42fc8ecbfbe6b67083412b609bd3.css><link rel=stylesheet href=https://gazsecops.github.io/css/syntax.min.a0773cce9310cb6d8ed23e50f005448facf29a53001b57e038828daa466b25c0.css><link rel=stylesheet href=https://gazsecops.github.io/css/terminal.min.e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855.css><link rel=stylesheet href=https://gazsecops.github.io/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css><link rel="shortcut icon" href=https://gazsecops.github.io/favicon.png><link rel=apple-touch-icon href=https://gazsecops.github.io/apple-touch-icon.png><meta name=twitter:card content="summary"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="og:title" content="AI Systems Responsibility: An 8-Part Series"><meta property="og:description" content="When your AI system breaks at 3am, the sysadmin gets the page. Not the data scientist. Not the ML engineer. The person who has to fix it.
This 8-part series covers what happens when AI systems meet production reality. What breaks. Who gets blamed. What actually works.
The Series Part 1: Silent Failures - Why AI fails differently than traditional software Part 2: Monitoring What Matters - Beyond uptime checks Part 3: Incident Response - The 4am conversation nobody wants Part 4: Testing Requirements - When standard testing isn&rsquo;t enough Part 5: When to Say No - Building political capital Part 6: Case Studies - Real failures, real consequences Part 7: Organizational Dysfunction - Why smart people make dumb decisions Part 8: Practical Tools - Boring tools that actually work Who This Is For Sysadmins who got handed AI systems SREs suddenly responsible for ML infrastructure Engineers who have to keep the thing running Anyone who gets paged when models break What You&rsquo;ll Learn How to monitor AI systems properly What to test before shipping When to push back (and how) Incident response when you can&rsquo;t explain why it broke Tools that work (Prometheus, not expensive platforms) Political survival in dysfunctional organizations The Core Philosophy The system is designed to produce bad outcomes. Your job is to minimize the damage. Not prevent it entirely. Minimize it.
"><meta property="og:url" content="https://gazsecops.github.io/series/ai-systems-responsibility/"><meta property="og:site_name" content="Gareth's Engineering Blog"><meta property="og:image" content="https://gazsecops.github.io/og-image.png"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="627"><meta property="article:published_time" content="2025-12-15 00:00:00 +0000 UTC"></head><body><div class="container center"><header class=header><div class=header__inner><div class=header__logo><a href=/><div class=logo>gareth@blog:~$</div></a></div><div class=header__search><input type=text id=header-search placeholder="Quick search..." style="padding:5px 10px;background:#222;color:#eee;border:1px solid #444;border-radius:3px;font-size:14px;width:200px"></div><ul class="menu menu--mobile"><li class=menu__trigger>Menu&nbsp;▾</li><li><ul class=menu__dropdown><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></li></ul></div><nav class=navigation-menu><ul class="navigation-menu__inner menu--desktop"><li><a href=/topics/>Topics</a></li><li><a href=/posts/>All</a></li><li><a href=/series/>Series</a></li><li><a href=/tags/>Tags</a></li><li><a href=/archive/>Archive</a></li><li><a href=/search/>Search</a></li><li><a href=/about/>About</a></li></ul></nav></header><script>(function(){const e=document.getElementById("header-search");if(!e)return;e.addEventListener("keydown",function(e){if(e.key==="Enter"){const e=this.value.trim();e&&(window.location.href="/search?q="+encodeURIComponent(e))}}),document.addEventListener("keydown",function(t){t.key==="/"&&!["INPUT","TEXTAREA"].includes(t.target.tagName)&&(t.preventDefault(),e.focus()),t.key==="Escape"&&t.target===e&&e.blur()})})()</script><style>.header__inner{display:flex;align-items:center;gap:20px}.header__search{flex:none}#header-search::placeholder{color:#666;font-style:italic}#header-search:focus{outline:none;border-color:#888}@media(max-width:684px){.header__search{display:none}}</style><div class=content><article class=post><h1 class=post-title><a href=https://gazsecops.github.io/series/ai-systems-responsibility/>AI Systems Responsibility: An 8-Part Series</a></h1><div class=post-meta><time class=post-date>2025-12-15</time><span class=post-reading-time>2 min read (226 words)</span></div><div class=table-of-contents><h2>Table of Contents</h2><nav id=TableOfContents><ul><li><a href=#the-series>The Series</a></li><li><a href=#who-this-is-for>Who This Is For</a></li><li><a href=#what-youll-learn>What You&rsquo;ll Learn</a></li><li><a href=#the-core-philosophy>The Core Philosophy</a></li></ul></nav></div><div class=post-content><div><p>When your AI system breaks at 3am, the sysadmin gets the page. Not the data scientist. Not the ML engineer. The person who has to fix it.</p><p>This 8-part series covers what happens when AI systems meet production reality. What breaks. Who gets blamed. What actually works.</p><h2 id=the-series>The Series<a href=#the-series class=hanchor arialabel=Anchor>#</a></h2><ol><li><a href=/posts/ai-systems-responsibility-part-1/>Part 1: Silent Failures</a> - Why AI fails differently than traditional software</li><li><a href=/posts/ai-systems-responsibility-part-2/>Part 2: Monitoring What Matters</a> - Beyond uptime checks</li><li><a href=/posts/ai-systems-responsibility-part-3/>Part 3: Incident Response</a> - The 4am conversation nobody wants</li><li><a href=/posts/ai-systems-responsibility-part-4/>Part 4: Testing Requirements</a> - When standard testing isn&rsquo;t enough</li><li><a href=/posts/ai-systems-responsibility-part-5/>Part 5: When to Say No</a> - Building political capital</li><li><a href=/posts/ai-systems-responsibility-part-6/>Part 6: Case Studies</a> - Real failures, real consequences</li><li><a href=/posts/ai-systems-responsibility-part-7/>Part 7: Organizational Dysfunction</a> - Why smart people make dumb decisions</li><li><a href=/posts/ai-systems-responsibility-part-8/>Part 8: Practical Tools</a> - Boring tools that actually work</li></ol><h2 id=who-this-is-for>Who This Is For<a href=#who-this-is-for class=hanchor arialabel=Anchor>#</a></h2><ul><li>Sysadmins who got handed AI systems</li><li>SREs suddenly responsible for ML infrastructure</li><li>Engineers who have to keep the thing running</li><li>Anyone who gets paged when models break</li></ul><h2 id=what-youll-learn>What You&rsquo;ll Learn<a href=#what-youll-learn class=hanchor arialabel=Anchor>#</a></h2><ul><li>How to monitor AI systems properly</li><li>What to test before shipping</li><li>When to push back (and how)</li><li>Incident response when you can&rsquo;t explain why it broke</li><li>Tools that work (Prometheus, not expensive platforms)</li><li>Political survival in dysfunctional organizations</li></ul><h2 id=the-core-philosophy>The Core Philosophy<a href=#the-core-philosophy class=hanchor arialabel=Anchor>#</a></h2><p>The system is designed to produce bad outcomes. Your job is to minimize the damage. Not prevent it entirely. Minimize it.</p></div></div></article></div><footer class=footer><div class=footer__inner><div class=copyright><span>© 2026</span></div><div class=build-info><span class=build-commit>e733073</span><span class=build-date>2026-02-12 12:33 UTC</span></div></div></footer></div></body></html>